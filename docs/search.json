[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "The materials provided in this repository are FREE to use.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement.\n\nStarts 25 November 2024\nFree online course: ùóßùóøùóÆùó∂ùóª ùòÅùóµùó≤ ùóßùóøùóÆùó∂ùóªùó≤ùóø: ùóóùó≤ùòÄùó∂ùó¥ùóª ùóöùó≤ùóªùóºùó∫ùó∂ùó∞ùòÄ ùóÆùóªùó± ùóïùó∂ùóºùó∂ùóªùó≥ùóºùóøùó∫ùóÆùòÅùó∂ùó∞ùòÄ ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ https://www.futurelearn.com/courses/train-the-trainer-design-genomics-training Interactive exercises delivered via FutureLearn."
  },
  {
    "objectID": "module6/module6.html",
    "href": "module6/module6.html",
    "title": "Module 6",
    "section": "",
    "text": "ChatGPT for bioinformatics\nThis will work with a Pro subscription.\nThe purpose of this demo is to show how to merge CSV files produced by ARIBA Resfinder and Pathogenwatch, using ChatGPT.\nThe input files can be found here: https://github.com/monicaiabrudan/bacterial-genomics/tree/main/module6/input\nAnd the output files can be found here: https://github.com/monicaiabrudan/bacterial-genomics/tree/main/module6/output\nUpload the CSV files\n\nPrompt ChatGPT to merge the three files.\n\nThe three files have different number of rows!\n\nSelect only isolates that are present in all files, and are resistance to Ciprofloxacin.\n\nThe selection of Ciprofloxacin resistant isolates is based on the column ‚ÄúCiprofloxacin‚Äù in the AMR profiles files.\nChatGPT attempts to explain the genes that are responsible to Ciprofloxacin resistance, however, these are all wrong"
  },
  {
    "objectID": "module7/module7.html",
    "href": "module7/module7.html",
    "title": "Module 7",
    "section": "",
    "text": "What is MLST?\n\nProposed by Maiden et al.¬†in 1998.\nPopular genotypic method in taxonomy classification.\nIdentifies allelic variants to characterize, subtype, and classify the members of a haploid bacterial population.\n\n\n\nHow does MLST work?\n\nConsist of sequencing several different housekeeping genes (generally 7) from an organism and comparing their sequences with the sequences of the same genes from different strains of the same organism.\nFor each gene, an approximately 45‚Äì500 bp sequence is amplified using PCR and then sequenced by Sanger‚Äôs method.\nEach nucleotide along the sequence is compared, and differences are noted.\nEach difference or sequence variant is called an allele and is assigned a number. The strain being studied is then assigned a series of numbers as its allelic profile or multilocus sequence types.\nStrains with identical sequences for a given gene have the same allele for that gene, and two strains with identical sequences for all the genes have the same allelic profile.\nThe relationship between each allelic profile is expressed in a dendrogram of linkage distance that varies.\nCombination of variants = Sequence Type (ST)\nNumber of differences is ignored; a new identifier is given for every possible variant\nWith 30 alleles per locus: ~20 billion possibilities\nDatabase with observed combinations\n\n\n\n\n\n\nUsing MLST to Study Bacterial Variation: Prospects in the Genomic Era, 2014, Future Microbiology\n\n\nMore general information on MLST can be found here\n\n\nMLST with ARIBA\nThe manual can be found here: https://github.com/sanger-pathogens/ariba/wiki/MLST-calling-with-ARIBA\nARIBA can be used for MLST using the typing schemes from PubMLST. A list of available species can be obtained by running\nariba pubmlstspecies\nDownload the data (in this example, Staphylococcus aureus) using pubmlstget:\nariba pubmlstget \"Staphylococcus aureus\" get_mlst\nHow to run ARIBA for one sample\nariba run get_mlst/ref_db /home/ubuntu/Data/all_fastqs/ERR017200_1.fastq.gz /home/ubuntu/Data/all_fastqs/ERR017200_2.fastq.gz MLST\n\nWrite a script to run ARIBA MLST on all samples."
  },
  {
    "objectID": "Day1/Assembly.html",
    "href": "Day1/Assembly.html",
    "title": "Assembly",
    "section": "",
    "text": "SPAdes Assembly (Single K-mer)\n\nBasic assembly with automatic k-mer selection\nspades.py-1 sample1_1.clean.fastq.gz-2 sample1_2.clean.fastq.gz-o /home/ubuntu/analysis/assembly/sample1_spades_auto--careful-t 4-m 8\n\n\nView assembly statistics\ncat /home/ubuntu/analysis/assembly/sample1_spades_auto/scaffolds.fasta\n\n\n\nSPAdes Assembly (Specific K-mer)\n\n\n\n\n\n\n\n\n\nCommon Flags\nMeaning\nExample\nDescription\n\n\n\n\n-t\nNumber of CPU threads\n-t 4\nTells SPAdes to use 4 CPU cores for parallel processing. Use more threads if your machine has more cores (e.g., -t 8 or -t 16).\n\n\n-m\nMaximum memory (in GB)\n-m 8\nLimits SPAdes to use at most 8 GB of RAM. This prevents it from using too much system memory.\n\n\n\n\nAssembly with specific k-mer value\n~ Divide in groups and run SPAdes on one sample, with different k-mer sizes (21,33,55 or 77)\nReplace K-MER in the command below with one of the values 21,33,55 or 77\nGeneral command:\nspades.py-1 sample1_1.clean.fastq.gz-2 sample1_2.clean.fastq.gz-k K-MER-o /home/ubuntu/output--careful-t 4-m 8\nEg (this will take around 20 minutes to run!)\ncd /home/ubuntu/Data/G18252308/illumina\nspades -1 ERR4635696_1.fastq.gz -2 ERR4635696_2.fastq.gz -k 21 -o ../../Assembly --careful -t 4 -m 8\nK-mer selection guidelines:\n\nSmaller k-mers (21-33): better for low coverage, shorter contigs\nLarger k-mers (55-99): better for high coverage, more contiguous\nMultiple k-mers: SPAdes uses all and picks best\n\n\n\n\nUnicycler Hybrid Assembly\n(We will not run this in the classroom)\n\nHybrid assembly (short + long reads)\nunicycler-1 sample1_1.clean.fastq.gz-2 sample1_2.clean.fastq.gz-l sample1_nanopore.fastq.gz-o /home/ubuntu/analysis/assembly/sample1_unicycler-t 4\n\n\nShort-read only mode (for comparison)\nunicycler-1 sample1_1.clean.fastq.gz-2 sample1_2.clean.fastq.gz-o /home/ubuntu/analysis/assembly/sample1_unicycler_short-t 4\n\n\n\nAssembly Quality Assessment\n\nQUAST - comprehensive quality metrics\nquast.py/home/ubuntu/analysis/assembly/sample1_spades_auto/scaffolds.fasta/home/ubuntu/analysis/assembly/sample1_spades_k77/scaffolds.fasta/home/ubuntu/analysis/assembly/sample1_unicycler/assembly.fasta-o /home/ubuntu/analysis/assembly/quast_comparison-r /home/ubuntu/Reference/Saureus_reference.fasta-g /home/ubuntu/Reference/Saureus_reference.gff--threads 4\n\n\nView report\nKey QUAST metrics:\nNumber of contigs: Lower is better (fewer breaks)\nLargest contig: Should be ~2-3 Mb for chromosome\nTotal length: Should be ~2.8 Mb for S. aureus\nN50: Higher is better (longer contigs)\nL50: Lower is better (fewer contigs contain 50% of assembly)\n\n\nQuick Assembly Statistics\n\n\nSimple assembly stats script\ncd /home/ubuntu/Data/Assembly/\ncat assemblystats.sh\nTry running this:\nbash assemblystats.sh kmer21/contigs.fasta\nbash assemblystats.sh ENA_assembly/SAMEA5818231.fa\nCompare your data with your colleagues that chose a different K-mer size\n\n\nTroubleshooting\nIssue: SPAdes fails with memory error: Reduce memory usage by using ‚Äìmemory flag\nspades.py -1 R1.fastq.gz -2 R2.fastq.gz -o output --careful -t 2 -m 4\nIssue: Too many contigs (&gt;200)\nCoverage might be too low; Try larger k-mer values; Check for contamination Issue:\nAssembly too small/large\nCheck reference genome size\nVerify species identification with Mash Look for contamination"
  },
  {
    "objectID": "Compare_AMR_files.html",
    "href": "Compare_AMR_files.html",
    "title": "Bioinformatic methods for bacterial genomics, Cluj-Napoca, October 2025",
    "section": "",
    "text": "import pandas as pd\nfrom google.colab import files\nimport io\n\n# Upload files\nresfinder_file = files.upload()\n\n# Load the files\n# Check the uploaded file name\nprint(\"Uploaded files:\", resfinder_file.keys())\n\n# Use the correct file name from the printed keys\nfilename = list(resfinder_file.keys())[0]  # Automatically use the first uploaded file\ndf_res = pd.read_csv(io.StringIO(resfinder_file[filename].decode('utf-8')))\n\n# Display the first few rows\nprint(df_res.head())\n\n\npw_amr_file = files.upload()\n\n# Load the files\n# Check the uploaded file name\nprint(\"Uploaded files:\", pw_amr_file.keys())\n\n# Use the correct file name from the printed keys\nfilename = list(pw_amr_file.keys())[0]  # Automatically use the first uploaded file\ndf_pw = pd.read_csv(io.StringIO(pw_amr_file[filename].decode('utf-8')))\n\n# Display the first few rows\nprint(df_pw.head())\n\n\n# Clean Resfinder gene names\nresfinder_genes = {gene.replace('.match', '') for gene in df_res.columns}\nresfinder_genes2 = [w.replace('_', '') for w in resfinder_genes]\nprint (resfinder_genes2)\n# Get PW AMR gene names\npw_amr_genes = set(df_pw .columns)\n\n# Find substrings\nsubstr_in_resfinder = {gene for gene in pw_amr_genes if any(gene in res_gene for res_gene in resfinder_genes2)}\n\nprint(\"PW AMR genes that are substrings of Resfinder genes:\", substr_in_resfinder)\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving ARIBA_resfinder.csv to ARIBA_resfinder (6).csv\nUploaded files: dict_keys(['ARIBA_resfinder (6).csv'])\n        name aadD.match aph_2____Ia+.match blaZ_1.match bleO.match  \\\n0  ERR017200         no                 no          yes         no   \n1  ERR030279         no                 no          yes         no   \n2  ERR031664         no                 no          yes         no   \n3  ERR033465         no                 no          yes         no   \n4  ERR033475         no                 no          yes         no   \n\n  cat_pC221_.match erm_C_.match mecA-.match  \n0               no           no         yes  \n1               no          yes         yes  \n2               no          yes         yes  \n3               no           no         yes  \n4               no          yes         yes  \n\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving PW_AMR_genes.csv to PW_AMR_genes (5).csv\nUploaded files: dict_keys(['PW_AMR_genes (5).csv'])\n         NAME  aadD  aacA-aphD  mecA  blaZ  ermC  ermC_LP  dfrA\n0  SRR7653488     0          0     1     1     1        1     1\n1  SRR7653497     0          1     1     1     0        0     0\n2  ERR4635593     1          1     1     1     1        1     1\n3  ERR4783604     0          1     1     1     0        0     1\n4   ERR033465     0          0     1     1     0        0     0\n['ermC', 'name', 'mecA-', 'bleO', 'catpC221', 'aadD', 'blaZ1', 'aph2Ia+']\nPW AMR genes that are substrings of Resfinder genes: {'blaZ', 'aadD', 'ermC', 'mecA'}"
  },
  {
    "objectID": "agenda.html",
    "href": "agenda.html",
    "title": "Agenda",
    "section": "",
    "text": "Prerequisites for following this course.\n\nYou are expected to be comfortable with bash and also with a text editor such as vi or nano. If you are not, try to practice a few commands before you attempt to follow the steps below.\nThere are numerous online introductory tutorials to the UNIX/Linux operating system and command line, including:\nhttp://www.ee.surrey.ac.uk/Teaching/Unix\nhttp://swcarpentry.github.io/shell-novice/\nPlease make sure you have received from your course instructor the Public IPv4 of your Amazon EC2 instance, the name of the machine you are going to work on and also a public key to access the machine via ssh."
  },
  {
    "objectID": "data-flo/data-flo.html",
    "href": "data-flo/data-flo.html",
    "title": "Data-flo tutorial",
    "section": "",
    "text": "Data-flo (https://data-flo.io/) is a system for customised integration and manipulation of diverse data via a simple drag and drop interface.\nFor Data-flo documentation, go to https://docs.data-flo.io/introduction/readme\nData-flo can easily combine epidemiological data, genomic data, laboratory data, and various metadata from disparate sources (i.e., different data systems) and formats.\nData-flo provides a visual method to design a reusable pipeline to integrate, clean, and manipulate data in a multitude of ways, eliminating the need for continuous manual intervention (e.g., coding, formatting, spreadsheet formulas, manual copy-pasting).\nData-flo pipelines are combinations of ready-to-use data adaptors that can be tailored, modularised and shared for reuse and reproducibility. Once a Data-flo pipeline has been created, it can be run anytime, by anyone with access, to enable push-button data extraction and transformation. This saves significant time by removing the bulk of the manual repetitive workflows that require multiple sequential or tedious steps, enabling practitioners to focus on analysis and interpretation.\n\n\n‚ÄúSteps‚Äù in Data-flo are called ADAPTORS. There are three main types of adaptors, which serve different functions.\n\nImporting data\nManipulating & transforming data\nExporting data\n\n\n\n\nData-flo editing view. Examples of Data-flo adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo that imports data from Google spreadsheets, through ‚ÄúGoogle spreadsheet‚Äù adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo where two datatables are merged using a ‚ÄúJoin datatables‚Äù adaptor.\n\n\n\n\n\nData-flo editing view. Using various adaptors, columns in a datatable can be removed or renamed, specific strings or blank values can be replaced, dates can be reformatted, distinct lists can be generated.\n\n\n\n\n\nData-flo editing view. In this example, a Data-flo pushes updated data to a Microreact project and supplies the URL for that project using the ‚ÄúUpdate Microreact project‚Äù adaptor.\n\n\n\n\n\n\n\n\nHow to run a Data-flo, Step 1: Go to Data-flo transformations: data-flo.io/transformations/dataflows\n\n\n\n\n\nHow to run a Data-flo, Step 2: Select your favourite Data-flo. The Data-flo shown in this example is ‚ÄúLab to bioinformatics‚Äù\n\n\n\n\n\nHow to run a Data-flo, Step 3: Click on the RUN tab to get to the RUN page.\n\n\n\n\n\nHow to run a Data-flo, Step 4: Hit the RUN button.\n\n\n\n\n\nHow to run a Data-flo, Step 5: Check out the output. In this example, the output is a Microreact link. You can choose to RUN AGAIN the Data-flo, and this will update your results (in case the input had been changed, of course).\n\n\n\n\n\nCreate a Data-flo from scratch, Step 1: Go to the + sign on the bottom right of your screen.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 2: Select ‚ÄúNEW DATAFLOW‚Äù.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 3: Select your preferred ACCESS CONTROL.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 4: Add adaptors to your Data-flo. On the left-hand side of Data-flo, you can find the list of available adaptors.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 5: Try to retrieve data from a spreadsheet. Click on ‚ÄúSpreadsheet file‚Äù on the list of adaptors. Hover over the ‚Äúi‚Äù button next to ‚ÄúSpreadsheet file‚Äù to see basic information about this adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 6: Clicking on ‚ÄúSpreadsheet file‚Äù on the list of adaptors, will add an adaptor (which looks like a box), with the title ‚ÄúSpreadsheet file‚Äù on your canvas. When you click on *file on the left side of the adaptor, the view on the right hand side of your screen will change, and you will be able to see a list of options under ‚ÄúBINDING TYPE‚Äù, such as ‚ÄúBind to a Dataflow input‚Äù and ‚ÄúBind to another transformation‚Äù.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 7: From the list of ‚ÄúBINDING TYPEs‚Äù, Select ‚ÄúBind to a Dataflow input‚Äù and press the ‚Äú+‚Äù next to INPUT ARGUMENT.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 8: A box with ‚Äúfile‚Äù will appear on the canvas, linked to the ‚ÄúSpreadsheet file‚Äù adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 9: To run a Data-flo, press the little ‚Äúbug icon‚Äù on top, on the right hand side of the Save button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 10: Pressing the ‚Äúbug icon‚Äù will trigger the Dataflow Debugger and you will be shown a ‚ÄúChoose file‚Äù button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 11: Press the ‚ÄúChoose file‚Äù button and select a spreadsheet from your local computer. Click the ‚Äúdata‚Äù on the right hand side of the adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 12: Clicking on the ‚Äúdata‚Äù on the right hand side of the adaptor, will retrieve a view of the file selected. In this case, the Dataflow Debugger is showing the first 3 rows of a file containing epidemiological data.\n\n\n\n\n\n\n\n\nHow to share your Data-flo, Step 1: Press the downward facing arrow shown here on the top right corner. This will trigger the export of a .json file, which can be sent and shared.\n\n\n\n\n\nHow to share your Data-flo, Step 2: If you have received a .json Data-flo file, press the + sign at the bottom right of your screen\n\n\n\n\n\nHow to share your Data-flo, Step 3: Press the IMPORT button and then select your .json file containing the Data-flo."
  },
  {
    "objectID": "data-flo/data-flo.html#a-short-introduction",
    "href": "data-flo/data-flo.html#a-short-introduction",
    "title": "Data-flo tutorial",
    "section": "",
    "text": "Data-flo (https://data-flo.io/) is a system for customised integration and manipulation of diverse data via a simple drag and drop interface.\nFor Data-flo documentation, go to https://docs.data-flo.io/introduction/readme\nData-flo can easily combine epidemiological data, genomic data, laboratory data, and various metadata from disparate sources (i.e., different data systems) and formats.\nData-flo provides a visual method to design a reusable pipeline to integrate, clean, and manipulate data in a multitude of ways, eliminating the need for continuous manual intervention (e.g., coding, formatting, spreadsheet formulas, manual copy-pasting).\nData-flo pipelines are combinations of ready-to-use data adaptors that can be tailored, modularised and shared for reuse and reproducibility. Once a Data-flo pipeline has been created, it can be run anytime, by anyone with access, to enable push-button data extraction and transformation. This saves significant time by removing the bulk of the manual repetitive workflows that require multiple sequential or tedious steps, enabling practitioners to focus on analysis and interpretation.\n\n\n‚ÄúSteps‚Äù in Data-flo are called ADAPTORS. There are three main types of adaptors, which serve different functions.\n\nImporting data\nManipulating & transforming data\nExporting data\n\n\n\n\nData-flo editing view. Examples of Data-flo adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo that imports data from Google spreadsheets, through ‚ÄúGoogle spreadsheet‚Äù adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo where two datatables are merged using a ‚ÄúJoin datatables‚Äù adaptor.\n\n\n\n\n\nData-flo editing view. Using various adaptors, columns in a datatable can be removed or renamed, specific strings or blank values can be replaced, dates can be reformatted, distinct lists can be generated.\n\n\n\n\n\nData-flo editing view. In this example, a Data-flo pushes updated data to a Microreact project and supplies the URL for that project using the ‚ÄúUpdate Microreact project‚Äù adaptor.\n\n\n\n\n\n\n\n\nHow to run a Data-flo, Step 1: Go to Data-flo transformations: data-flo.io/transformations/dataflows\n\n\n\n\n\nHow to run a Data-flo, Step 2: Select your favourite Data-flo. The Data-flo shown in this example is ‚ÄúLab to bioinformatics‚Äù\n\n\n\n\n\nHow to run a Data-flo, Step 3: Click on the RUN tab to get to the RUN page.\n\n\n\n\n\nHow to run a Data-flo, Step 4: Hit the RUN button.\n\n\n\n\n\nHow to run a Data-flo, Step 5: Check out the output. In this example, the output is a Microreact link. You can choose to RUN AGAIN the Data-flo, and this will update your results (in case the input had been changed, of course).\n\n\n\n\n\nCreate a Data-flo from scratch, Step 1: Go to the + sign on the bottom right of your screen.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 2: Select ‚ÄúNEW DATAFLOW‚Äù.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 3: Select your preferred ACCESS CONTROL.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 4: Add adaptors to your Data-flo. On the left-hand side of Data-flo, you can find the list of available adaptors.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 5: Try to retrieve data from a spreadsheet. Click on ‚ÄúSpreadsheet file‚Äù on the list of adaptors. Hover over the ‚Äúi‚Äù button next to ‚ÄúSpreadsheet file‚Äù to see basic information about this adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 6: Clicking on ‚ÄúSpreadsheet file‚Äù on the list of adaptors, will add an adaptor (which looks like a box), with the title ‚ÄúSpreadsheet file‚Äù on your canvas. When you click on *file on the left side of the adaptor, the view on the right hand side of your screen will change, and you will be able to see a list of options under ‚ÄúBINDING TYPE‚Äù, such as ‚ÄúBind to a Dataflow input‚Äù and ‚ÄúBind to another transformation‚Äù.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 7: From the list of ‚ÄúBINDING TYPEs‚Äù, Select ‚ÄúBind to a Dataflow input‚Äù and press the ‚Äú+‚Äù next to INPUT ARGUMENT.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 8: A box with ‚Äúfile‚Äù will appear on the canvas, linked to the ‚ÄúSpreadsheet file‚Äù adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 9: To run a Data-flo, press the little ‚Äúbug icon‚Äù on top, on the right hand side of the Save button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 10: Pressing the ‚Äúbug icon‚Äù will trigger the Dataflow Debugger and you will be shown a ‚ÄúChoose file‚Äù button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 11: Press the ‚ÄúChoose file‚Äù button and select a spreadsheet from your local computer. Click the ‚Äúdata‚Äù on the right hand side of the adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 12: Clicking on the ‚Äúdata‚Äù on the right hand side of the adaptor, will retrieve a view of the file selected. In this case, the Dataflow Debugger is showing the first 3 rows of a file containing epidemiological data.\n\n\n\n\n\n\n\n\nHow to share your Data-flo, Step 1: Press the downward facing arrow shown here on the top right corner. This will trigger the export of a .json file, which can be sent and shared.\n\n\n\n\n\nHow to share your Data-flo, Step 2: If you have received a .json Data-flo file, press the + sign at the bottom right of your screen\n\n\n\n\n\nHow to share your Data-flo, Step 3: Press the IMPORT button and then select your .json file containing the Data-flo."
  },
  {
    "objectID": "microreact/microreact.html",
    "href": "microreact/microreact.html",
    "title": "Microreact tutorial",
    "section": "",
    "text": "For Microreact documentation, go to https://docs.microreact.org/"
  },
  {
    "objectID": "microreact/microreact.html#task-1-create-an-editable-project.",
    "href": "microreact/microreact.html#task-1-create-an-editable-project.",
    "title": "Microreact tutorial",
    "section": "Task 1: Create an editable project.",
    "text": "Task 1: Create an editable project.\n\n\n\nTask 1: Create an editable project. Step 1: Notice the crossed out pen on right top corner of your screen. This indicates that you cannot edit the current Microreact project.\n\n\n\n\n\nTask 1: Create an editable project. Step 2: Click the crossed out ‚ÄúPen‚Äù symbol in the top right of the screen. A window appears asking you to ‚ÄúSIGN IN TO EDIT‚Äù.\n\n\n\n\n\nTask 1: Create an editable project. Step 3: Once you sign in, the message changes, and you are invited to ‚ÄúMAKE A COPY‚Äù of the current project. Make a copy, to proceed.\n\n\n\n\n\nTask 1: Create an editable project. Step 4: Notice that, once you made a copy of the project, the crossed out ‚Äúpen‚Äù symbol will change to a ‚Äúnormal pen‚Äù, and you will be able to edit and save the project."
  },
  {
    "objectID": "microreact/microreact.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "href": "microreact/microreact.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "title": "Microreact tutorial",
    "section": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.",
    "text": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 1: Select the ‚ÄúKpn Colombia‚Äù view. Click on the ‚ÄúPen‚Äù symbol on the top right menu. Click on the ‚ÄúCreate New Chart‚Äù\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 2: Drag the new chart to overlap with the tree.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 3: An empty panel for the new chart will be shown on top of the tree panel.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 4: In the Chart Type dropdown list select ‚ÄúBar Chart‚Äù.\n\n\n\n\n\nTask2: Present WGS QC statistics in a chart. Step 5: A new window appears. In the X Axis Column, select ‚ÄúWGS_QC_no_contigs‚Äù and for ‚ÄúMaximum number of bins‚Äù select 10.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 6: The bar chart will look like above. Observe that most genomes have less the 100 contigs."
  },
  {
    "objectID": "microreact/microreact.html#task-3-what-are-the-dominating-sequence-types-sts-in-colombia",
    "href": "microreact/microreact.html#task-3-what-are-the-dominating-sequence-types-sts-in-colombia",
    "title": "Microreact tutorial",
    "section": "Task 3: What are the dominating sequence types (STs) in Colombia?",
    "text": "Task 3: What are the dominating sequence types (STs) in Colombia?\n\n\n\nTask 3: What are the dominating STs in Colombia? Now that you‚Äôve created one chart, you can create another one! Step 1: Go to the ‚ÄúPen: symbol on the right hand side and click on the‚ÄùCreate New Chart‚Äù.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 2: The new chart can stay right on top of the previously created one.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 3: Notice a white canvas on top of the previously generated chart.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 4: Once again, from the Chart Type dropdown menu, select ‚ÄúBar Chart‚Äù, and when the new view shows up on the ‚ÄúX Axis Column‚Äù, select ST.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 5: A new chart will appear. The labels on the x-axis appear squished and they are hard to read. Drag the panel divider on the left hand side of the chart, to increase the width of the panel.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 6: The information on the x-axis should be readable now. The 3 most abundant STs are ST11, ST258 and ST512.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 7: Click the ‚ÄúViews‚Äù panel on the left hand side, hover over ‚ÄúKpn Colombia‚Äù, click on the three dots on the corner of the view and hit ‚ÄúUpdate View‚Äù\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 8: Go to the Save icon on the right corner, press the icon and choose ‚ÄúUpdate This Project‚Äù"
  },
  {
    "objectID": "microreact/microreact.html#task-4-plot-metadata-blocks-for-the-carbapenamase-genes-ctx-m-15-ndm-1-kpc-and-oxa.-what-are-the-prevalent-amr-mechanisms-detected",
    "href": "microreact/microreact.html#task-4-plot-metadata-blocks-for-the-carbapenamase-genes-ctx-m-15-ndm-1-kpc-and-oxa.-what-are-the-prevalent-amr-mechanisms-detected",
    "title": "Microreact tutorial",
    "section": "Task 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?",
    "text": "Task 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 1: Press the icon on the top right of the tree panel.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 2: In the Metadata blocks dropdown list, tick all boxes containing KPC, NDM, VIM, OXA and CTX-M-15 genes.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 3: The tree panel will show metadata columns. Yellow indicates presence of a certain gene and green indicates absence. In the panel shown here, it appears that the most common genes present are KPC-2, KPC-3, NDM-1 and CTX-M-15."
  },
  {
    "objectID": "microreact/microreact.html#task-5-which-sts-are-associated-with-the-presence-of-carbapenamase-genes",
    "href": "microreact/microreact.html#task-5-which-sts-are-associated-with-the-presence-of-carbapenamase-genes",
    "title": "Microreact tutorial",
    "section": "Task 5: Which STs are associated with the presence of carbapenamase genes?",
    "text": "Task 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 1: Go to the ‚ÄúMetadata blocks‚Äù and check the ST box.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 2: Observe the new metadata column next to the tree, with the header ‚ÄúST‚Äù.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? In the tree with added metadata blocks, we can observe a large brown block of isolates belonging to ST258. You will need to check the Legend on the very right of Microreact.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? On a close look, we can observe that ST258 is associated with the presence of the carbapenamase gene KPC-3."
  },
  {
    "objectID": "Assessment/assessment.html",
    "href": "Assessment/assessment.html",
    "title": "Assessment",
    "section": "",
    "text": "More details will be communicated to you via email."
  },
  {
    "objectID": "module2/module2.html",
    "href": "module2/module2.html",
    "title": "Module 2",
    "section": "",
    "text": "Read more here:\nhttps://genome.ucsc.edu/FAQ/FAQformat.html\n\n\n\nFollow the steps in Module 1 to connect to your remote virtual machine.\nThe next exercise is broadly based on the study https://journals.asm.org/doi/full/10.1128/msphere.00185-23 by Abrudan and Shamanna, 2023\n\n\n\nThe European Nucleotide Archive (ENA) provides a comprehensive record of the world‚Äôs nucleotide sequencing information, covering raw sequencing Data, sequence assembly information and functional annotation.\nAccess to ENA Data is provided through the browser, through search tools, through large scale file download and through the API."
  },
  {
    "objectID": "module2/module2.html#reads-qc",
    "href": "module2/module2.html#reads-qc",
    "title": "Module 2",
    "section": "Reads QC",
    "text": "Reads QC\nIn this part of the exercise, we will use a programme called FastQC.\nFastQC aims to provide a simple way to do some quality control checks on raw sequence Data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your Data has any problems of which you should be aware before doing any further analysis.\nThe main functions of FastQC are\n\nImport of Data from BAM, SAM or FastQ files (any variant)\nProviding a quick overview to tell you in which areas there may be problems\nSummary graphs and tables to quickly assess your Data\nExport of results to an HTML based permanent report\n\n\nRun FastQC programatically\nTo run non-interactively you simply have to specify a list of files to process on the command line.¬†\nfastqc somefile.txt someotherfile.txt\nfastqc /home/ubuntu/Data/*/*/*\n\n\nQC your fastq reads through the visual interface\nNavigate to /home/ubuntu/Software/FastQC and double click on the FastQC icon.\nIn the visual interface, open all files produced by FastQC and assess the results, eg ERR4635696_1_fastqc.html\nVisualize and interpret the FastQC results. Compare the QC results for Nanopore and Illumina sequence reads\nOptional: Unzip the files /home/ubuntu/Data/*/*/*_*_fastqc.zip and produce a script that extracts the information on reads lengths, GC contents etc; plot the results from all samples.\n\n\n\nFastQC Illumina reads\n\n\n\n\n\nFastQC Nanopore reads"
  },
  {
    "objectID": "module2/module2.html#speciation",
    "href": "module2/module2.html#speciation",
    "title": "Module 2",
    "section": "Speciation",
    "text": "Speciation\nBactinspector is a package to a) determine the most probable species based on sequence in fasta/fastq files using refseq and Mash (https://mash.readthedocs.io/en/latest/index.html) and b) determine the closest reference in refseq to a set of fasta/fastq files.\n\nGo to sample files /home/ubuntu/Data/ and run Bactinspector using the following command\n\nbactinspector closest_match -fq \"*_2.fastq.gz\"\n\n\nTidy up the output *.tsv file\n\ntail -n +2 *.tsv| column -t | less -S\ncat *.tsv | tr \"\\t\" \"~\" | cut -d\"~\" -f2\nEg of a result:\nASM289538v1\nGCF_002895385\n\n\nHow do you interpret the results? Look up your results in ENA.\nIn your browser, type\nhttps://www.ebi.ac.uk/ena/browser/text-search?query=ASM289538v1\n\n\nCheck fields such as:\n\nAssembly Level\nGenome Representation\nStrain\nCount Contig\nContig N50\nTotal Length\n\nDownload the reference genome from the visual reference\n\nOr from the command line.\nwget https://www.ebi.ac.uk/ena/browser/api/fasta/CP018629.1 -O - &gt;&gt; GCA_002895385-chromosomes.fasta"
  },
  {
    "objectID": "module2/module2.html#genome-assembly",
    "href": "module2/module2.html#genome-assembly",
    "title": "Module 2",
    "section": "Genome assembly",
    "text": "Genome assembly\n\n\n\nAn example of the N50 metric\n\n\nThe Velvet assembler\nVelvet is an algorithm package that has been designed to deal with de novo genome assembly and short read sequencing alignments. This is achieved through the manipulation of de Bruijn graphs for genomic sequence assembly via the removal of errors and the simplification of repeated regions.\nRun Velvet\nFind a value of k (between 21 and 99) to start with, and record your choice.\nvelveth G18252308 KMER -short -separate -fastq /home/ubuntu/Data/G18252308/illumina/ERR4635696_1.fastq.gz /home/ubuntu/Data/G18252308/illumina/ERR4635696_2.fastq.gz\nAfter velveth is finished, look in the new folder that has the name you chose. You should see the following files:\nLog\nRoadmaps\nSequences\nThe ‚ÄôLog‚Äò file has a useful reminder of what commands you typed to get this assembly result, for reproducing results later on. ‚ÄôSequences‚Äò contains the sequences we put in, and ‚ÄôRoadmaps‚Äò contains the index you just created.\nNow we will run the assembly with default parameters:\nvelvetg G18252308\n\nVelvet will end with a text like this:\nFinal graph has ... nodes and n50 of ..., max ..., total ..., using .../... reads\n\nThe number of nodes represents the number of nodes in the graph, which (more or less) is the number of contigs. Velvet reports its N50 (as well as everything else) in ‚Äòkmer‚Äô space. The conversion to ‚Äòbasespace‚Äô is as simple as adding k-1 to the reported length.\nLook again at the folder asm_name, you should see the following extra files:\ncontigs.faGraphLastGraphPreGraphstats.txt\nThe important files are:\ncontigs.fa - the assembly itselfGraph - a textual representation of the contig graphstats.txt - a file containing statistics on each contig\nQuestions\n\nWhat k-mer did you use?\nWhat is the N50 of the assembly?\nWhat is the size of the largest contig?\nHow many contigs are there in the contigs.fa file? Use grep -c NODE contigs.fa\n\nRun again velveth and velvetg with another KMER size.\n\nIn the classroom, decide you got the best N50 and the lowest number of contigs!\nVisualise the assembles using ACT and Artemis\nACT is a Java application for displaying pairwise comparisons between two or more DNA sequences.\nACT can be used to identify and analyse regions of similarity and difference between genomes and to explore conservation of synteny, in the context of the entire sequences and their annotation. It can read complete EMBL, GENBANK and GFF entries or sequences in FASTA or raw format.\nTip: ACT and Artemis are installed here: /usr/share/miniconda/pkgs/artemis-18.2.0-hdfd78af_0/share/artemis-18.2.0-0¬†\nGo the the visual interface of your virtual machine.\nIn the Terminal, type\nart &\nArtemis should open. Load your contigs.fa file.\n\n\n\n\n\nTry View-&gt;Overview in the Artemis menu\n\n\nFrom the Graph menu, open GC Deviation (G-C)/(G+C) by clicking on the button next to it.\nRescale the plot for to a more appropriate window size for this zoomed out view: Right click on the graph, and click Maximum Window Size, and select 20000. Then move the graph slider of the right hand side of the screen down to the bottom of the bar.\n\nFrom the graph you can see that plot generally varies about an upper level and a lower level across the assembly, with shifts occurring at contig boundaries."
  },
  {
    "objectID": "module2/module2.html#assembly-qc-with-quast",
    "href": "module2/module2.html#assembly-qc-with-quast",
    "title": "Module 2",
    "section": "Assembly QC with Quast",
    "text": "Assembly QC with Quast\nQUAST stands for QUality ASsessment Tool. It evaluates genome/metagenome assemblies by computing various metrics. The current QUAST toolkit includes the general QUAST tool for genome assemblies, MetaQUAST, the extension for metagenomic Datasets, QUAST-LG, the extension for large genomes (e.g., mammalians), and Icarus, the interactive visualizer for these tools.\n\nIn your virtual machine, go to the Chrome browser, and navigate to:\nfile:///home/ubuntu/Data/G18252308/illumina/G18252308_quast/report.html\nAlternatively, navigate to /home/ubuntu/Data/G18252308/illumina/G18252308_quast and double click on report.html\n\nWhat is the cumulative length of the assembly? Can you check if this expected for a S. aureus genome? Hint: go back to the reference genome you found previously! What was the length of that assembly?"
  },
  {
    "objectID": "module2/module2.html#assembly-methods-comparison",
    "href": "module2/module2.html#assembly-methods-comparison",
    "title": "Module 2",
    "section": "Assembly methods comparison",
    "text": "Assembly methods comparison\nFor assembly of short (Illumina) reads, we will use Unicycler. With short read data, Unicycler acts as wrapper script for the SPAdes assembler, and will produce an assembly with settings optimised for bacteria.\nIn the command below we:\n\nAllocate 4 CPUs to the assembler\n\n-t 4\n\nSpecify the Illumina forward and reverse reads to use\n\n-1 &lt;fastq&gt; and -2 &lt;fastq&gt;\n\nSpecify a directory to output files to\n\n-o &lt;dir&gt;\n\n\nunicycler -t 4 -1 /home/ubuntu/Data/G18252308/illumina/ERR4635696_1.fastq.gz -2 /home/ubuntu/Data/G18252308/illumina/ERR4635696_2.fastq.gz -o ERR4635696.short\nFor assembly of long reads, Unicycler switches to the long read assembler miniasm - this is a very rapid assembler, but is not particularly accurate, and can often introduce errors and mistakes into its outputs.\n\nIn the command below we:\n\nAllocate 4 CPUs to the assembler\n\n-t 4\n\nSpecify the ONT long reads to use\n\n-l &lt;fastq&gt;\n\nSpecify a directory to output files to\n\n-o &lt;dir&gt;\n\n\nunicycler -t 4 -l /home/ubuntu/Data/G18252308/nanopore/ERR8187245.fastq.gz -o S_aureus_JKD6159.s100.unicycler.long\nFor assembly of both long and short reads, Unicycler uses SPAdes for an initial ‚Äòshort read‚Äô assembly, and then uses the long reads to attempt to bridge the gaps between contigs from the initial SPAdes assembly. This can work well when the short read data is of high quality/depth but the ONT data is of low depth. However, the ‚Äòbridging step‚Äô can be time consuming, and if the short read assembly is poor, the final assembly may also be. If you have high quality and depth long reads, this may not be the best option.\nIn the command below we:\n\nAllocate 4 CPUs to the assembler\n\n-t 4\n\nSpecify the Illumina forward and reverse reads to use\n\n-1 &lt;fastq&gt; and -2 &lt;fastq&gt;\n\nSpecify the ONT long reads to use\n\n-l &lt;fastq&gt;\n\nSpecify a directory to output files to\n\n-o &lt;dir&gt;\n\n\nNote, this method may take over an hour to run on the VMs - check with an instructor before running.\nunicycler -t 4 -1/home/ubuntu/Data/G18252308/illumina/ERR4635696_1.fastq.gz -2 /home/ubuntu/Data/G18252308/illumina/ERR4635696_2.fastq.gz -l /home/ubuntu/Data/G18252308/nanopore/ERR8187245.fastq.gz -o ERR8187245.unicycler.hybrid"
  },
  {
    "objectID": "module2/module2.html#compare-velvet-and-spades-assemblers",
    "href": "module2/module2.html#compare-velvet-and-spades-assemblers",
    "title": "Module 2",
    "section": "Compare Velvet and SPADES assemblers",
    "text": "Compare Velvet and SPADES assemblers\nRun SPADES (with Unicycler)\nunicycler -t 4 -1 /home/ubuntu/Data/G18255819/illumina/ERR4784794_1.fastq.gz -2 /home/ubuntu/Data/G18255819/illumina/ERR4784794_2.fastq.gz -o ERR4784794.short\nView the SPADES assembly metrics in Quast\n\nHow do these metrics compare with the results you got from the Velvet assembler?"
  },
  {
    "objectID": "module2/module2.html#genome-annotation-with-prokka",
    "href": "module2/module2.html#genome-annotation-with-prokka",
    "title": "Module 2",
    "section": "Genome annotation with Prokka",
    "text": "Genome annotation with Prokka\n\nRun Prokka for one sample\ncd /home/ubuntu/Data/\ndocker run -v $(pwd):$(pwd) -w $(pwd) staphb/prokka:latest prokka --outdir /home/ubuntu/Data/annotations/G18252308 --prefix G18252308 /home/ubuntu/Data/G18252308/illumina/G18252308_kmer51/contigs.fa\nCheck if the mecA gene was found in the annotated genome.\n\nOnce prokka is done, open Artemis and load the .gff file and locate the mecA gene. How many nucleotides does it have?\n\n\n\nHow to run Prokka on all samples from one folder\nExample script: create the following bash file: /home/ubuntu/Data/PROKKA_run/run_prokka.sh\nAdd the following lines of code:\nfor sample in `ls /home/ubuntu/Data/all_fastas/*.fasta | sed ‚Äòs/\\.fasta//‚Äô`\ndo\n¬†echo $sample\n¬†output=$(echo $sample | sed -E ‚Äòs#.*/([^/]+)$#\\1#‚Äô)\n¬†echo $output\n¬†docker run -v $(pwd):$(pwd) -w $(pwd) staphb/prokka:latest prokka ‚Äìoutdir /home/ubuntu/Data/annotations/${output} ‚Äìprefix ${output} /home/ubuntu/Data/all_fastas/${output}.fasta\ndone"
  },
  {
    "objectID": "module2/module2.html#bonus-activity",
    "href": "module2/module2.html#bonus-activity",
    "title": "Module 2",
    "section": "Bonus activity",
    "text": "Bonus activity\nInstall Snippy, snp_sites and IQtree and run a SNP tree. See the tutorial here"
  },
  {
    "objectID": "unix-commands.html",
    "href": "unix-commands.html",
    "title": "unix-commands",
    "section": "",
    "text": "Useful General Commands File Management # Compress files gzip large_file.txt\n\nDecompress\ngunzip file.txt.gz\n\n\nArchive directory\ntar -czf archive.tar.gz directory/\n\n\nExtract archive\ntar -xzf archive.tar.gz\n\n\nDisk usage\ndu -sh /home/ubuntu/analysis/*\nText Processing # Count lines wc -l file.txt\n\n\nSearch for pattern\ngrep ‚Äúpattern‚Äù file.txt\n\n\nColumn formatting\ncolumn -t -s‚Äô,‚Äô file.csv\n\n\nSort numerically\nsort -n file.txt\n\n\nUnique entries\nsort file.txt | uniq -c\nProcess Management # Check running processes top\n\n\nKill process\nkill PID\n\n\nRun in background\ncommand &\n\n\nCheck background jobs\njobs\nTroubleshooting Common Issues Out of Memory # Check memory usage free -h\n\n\nReduce threads\n\n\nChange -t 4 to -t 2 in commands\nCommand Not Found # Check if installed which program_name\n\n\nActivate conda environment\nconda activate genomics\n\n\nAdd to PATH\nexport PATH=$PATH:/home/ubuntu/Software/tool/bin\nPermission Denied # Make script executable chmod +x script.sh\n\n\nChange ownership\nsudo chown ubuntu:ubuntu file.txt\nDisk Space Issues # Check space df -h\n\n\nFind large files\ndu -h /home/ubuntu | sort -rh | head -20\n\n\nClean up\nrm -rf /home/ubuntu/analysis/temp/*"
  },
  {
    "objectID": "module3/module3.html",
    "href": "module3/module3.html",
    "title": "Module 3",
    "section": "",
    "text": "Read about Roary here.\nRoary is a high speed stand alone pan genome pipeline, which takes annotated assemblies in GFF3 format (produced by Prokka (Seemann, 2014)) and calculates the pan genome. Using a standard desktop PC, it can analyse datasets with thousands of samples, something which is computationally infeasible with existing methods, without compromising the quality of the results. 128 samples can be analysed in under 1 hour using 1 GB of RAM and a single processor. To perform this analysis using existing methods would take weeks and hundreds of GB of RAM. Roary is not intended for meta-genomics or for comparing extremely diverse sets of genomes.\ndocker run --rm -it -v $(pwd):$(pwd) sangerpathogens/roary roary -f /home/ubuntu/Data/roary /home/ubuntu/Data/annotations/*/*.gff\nThe summary output is present in the summary_statistics.txt\nCore genes (99% &lt;= strains &lt;= 100%) 1861\nSoft core genes (95% &lt;= strains &lt; 99%) 0\nShell genes (15% &lt;= strains &lt; 95%) 962\nCloud genes (0% &lt;= strains &lt; 15%) 838\nTotal genes (0% &lt;= strains &lt;= 100%) 3661\nAdditionally, Roary produces a gene_presence_absence.csv file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.\nDrop the accessory_binary_genes.fa.newick and gene_presence_absence.csv files in Phandango\n\nWhen you analyze your own data, you will need a phylogeny that represents the evolutionary history of your isolates. The inference of a phylogenetic tree is not part of roary‚Äôs functions, but you can use the core gene alignment (file: core_gene_alignment.aln) as input to infer a tree.\nRoary comes with a python script that allows you to generate a few plots to graphically assess your analysis output. You will need the gene_presence_absence.csv file and the phylogeny at /home/ubuntu/Data/trees/tree.nwk.\nYou should get three files: a pangenome matrix, a frequency plot, and a pie chart.\npython /home/ubuntu/Software/roary_plots.py /home/ubuntu/Data/trees/tree_for_phandango.nwk /home/ubuntu/roary/gene_presence_absence.csv\n\n\n\n\n\n\n\nThe first part of this exercise is broadly based on a tutorial by Martin Hunt.\n\n\n\nARIBA is a tool that identifies antibiotic resistance genes. This tutorial will walk you through the analysis of the Staphylococcus aureus data set used in the paper:\nNovel multidrug-resistant sublineages of Staphylococcus aureus clonal complex 22 discovered in India\nAbrudan, Shamanna et al, mSphere 2023 https://doi.org/10.1128/msphere.00185-23\nLearning outcomes By the end of this tutorial you can expect to be able to:\n‚Ä¢ Download and prepare the standard AMR databases for use with ARIBA\n‚Ä¢ Prepare your own database for use with ARIBA\n‚Ä¢ Perform QC on input data and understand why QC is important\n‚Ä¢ Run ARIBA on several samples to identify antibiotic resistance\n‚Ä¢ Understand the different flags produced by ARIBA\n‚Ä¢ Summarise ARIBA results for several samples\n‚Ä¢ Query the AMR results produced by ARIBA\n‚Ä¢ Use Phandango to visualise ARIBA results\nThis module comprises the following sections:\n1. Run ARIBA on a reference database\n2. View summarized results using Phandango\n\nYou can run the commands in this module in the Amazon EC2 instance you were provided by your course instructor.\nARIBA should be already installed on this machine. If you need to use a different machine, please follow the instructions from Module 1 of this course.\n1. Run ARIBA on a reference database\nConfigure the Resfinder and CARD reference databases.\nariba getref resfinder out.resfinder\nariba prepareref -f out.resfinder.fa -m out.resfinder.tsv out.resfinder.prepareref\nariba getref card out.card\nariba prepareref -f out.card.fa -m out.card.tsv out.card.prepareref\nHow to run on one sample\nARIBA needs the database directories, for eg, out.resfinder.prepareref or out.card.prepareref, and two sequencing reads files reads.1.fastq.gz and reads.2.fastq.gz. The command to run ARIBA is:\nariba run out.resfinder.prepareref reads.1.fastq.gz reads.2.fastq.gz outdir\nThe above command will make a new directory called outdir that contains the results.\nRun ARIBA on all samples\nThe S. aureus dataset consists of many samples, and we need to run ARIBA on each sample, which can be done with a ‚Äúfor‚Äù loop. We assume that the reads files are named like this:\nname1_1.fastq.gz name1_2.fastq.gz\nname2_1.fastq.gz name2_2.fastq.gz\nname3_1.fastq.gz name3_2.fastq.gz\nThen we can run ARIBA on all samples like this (you may need to edit this command depending on how your own files are named):\nEg:\nfor sample in `ls *.1.fastq.gz | sed 's/\\.1.fastq.gz//'`\ndo\n  ariba run out.resfinder.prepareref $sample_1.fastq.gz $sample_2.fastq.gz $sample.ariba\ndone\nThe output directory of each sample is called $sample.ariba, for example name1.ariba is the output directory for sample name1.\nEXERCISE: Using the information above, write a script to fit the file names and paths in your own context and run ARIBA on all samples.\nARIBA output\nThe format of the output files are described here.\nViewing ARIBA results in Phandango\nThis section describes how to use Phandango to view a summary of ARIBA results from many samples.\nThe most important output file from ARIBA is the report called report.tsv. For this tutorial, we have all sample reports in the directory /home/ubuntu/Data/ARIBA_output\nls /home/ubuntu/Data/ARIBA_output | wc -l\nARIBA has a functon called ‚Äúsummary‚Äù that can summarise presence/absence of sequences and/or SNPs across samples. It takes at least two ariba reports as input, and makes a CSV file that can be opened in your favourite spreadsheet program, and also makes input files for Phandango. The two Phandango files (a tree and a CSV file) can be dropped straight into the Phandango page for viewing.\nThe tree that ARIBA makes is based on the CSV file, which contains results of presence/absence of sequence and SNPs, and other information such as percent identity between contigs and reference sequences. This means that it does not necessarily represent the real phylogeny of the samples. It is more accurate to provide a tree built from the sequencing data. For this reason, we will use a pre-computed tree file /home/ubuntu/Data/tree_for_phandango.tre.\nBasic usage of ariba summary\nCheck the options of ariba summaryusing the following command:\nariba summary -h\nFirst, let‚Äôs run ariba summary using the default settings, except we will skip making the tree:\nariba summary --no_tree out /home/ubuntu/Data/ARIBA_output/*.tsv\nWe can see that this made two files:\nls out.*\nThey are the same except for the first line, which has Phandango-specific information. ARIBA uses the filenames as sample names in the output:\nhead -n 2 out.phandango.csv\nThe first name is ‚Äú/home/ubuntu/Data/ARIBA_output/*/sample.tsv‚Äù, and the rest are named similarly.\nThis is not ideal, as it will look ugly in Phandango. Further, the names must exactly match the names in the tree file for Phandango to work (have a look in the tree /home/ubuntu/Data/trees/tree_for_phandango.nwk). You could do a little hacking here using the Unix command sed on the CSV file. Instead, we can supply ARIBA with a file of filenames that also tells ariba what to call the samples in its output CSV files. Instead of ‚Äú/home/ubuntu/Data/ARIBA_output/sample.tsv‚Äù, we would like to simply use ‚Äúsample‚Äù, which is cleaner and matches the tree file. It also means we can (and will) repeatedly run ariba summary with different options, and get output files that can be loaded straight into Phandango. This is one way to make the file with the naming information:\nls /home/ubuntu/Data/ARIBA_output/*/report.tsv | awk -F/ '{name=$(NF-1); sub(/\\.ariba$/, \"\", name); print $0, name}' &gt; ARIBA_output/filenames.fofn\nThe file is quite simple. Column 1 is the filename, and column 2 is the name we would like to use in the output.\n head ARIBA_output/filenames.fofn\nNow we can rerun summary using this input file. Note the use of the new option ‚Äìfofn.\nariba summary --no_tree --fofn ARIBA_output/filenames.fofn ARIBA_output/out /home/ubuntu/Data/ARIBA_output/*/report.tsv\nCheck that the renaming worked:\nhead -n 2 out.phandango.csv\nNow go to Phandango and drag and drop the files out.phandango.csv and trees/tree_for_phandango.nwk into the window. The result should like this\n\nThis a very high-level summary of the data. For each cluster, it is simply saying whether or not each sample has a ‚Äòmatch‚Äô. Green means a match, and pink means not a match. For presence/absence genes, this means that the gene must simply be there to count as a match. If it is a ‚Äúvariant only‚Äù gene, then the gene must be there and one of the variants that we told ARIBA about earlier when generating the ARIBA database.\nMore information per cluster\nIn addition to a simple ‚Äúyes‚Äù or ‚Äúno‚Äù as to whether a sample ‚Äúmatches‚Äù a given cluster (as explained above), more columns can be output for each cluster. See the ARIBA summary wiki page for a full description of the options.\nVariants In the previous screenshot, where the option ‚Äìpreset cluster_all, there are two variant columns: ‚Äúknown_var‚Äù and ‚Äúnovel_var‚Äù. Green means ‚Äúyes‚Äù and pink means ‚Äúno‚Äù.\nPart 2: Antimicrobial Resistance Identification using Pathogenwatch\nBrowse the public collection:\nhttps://pathogen.watch/collection/mxebr8oz0wjm-module2-s-aureus\nDownload the AMR genes and AMR SNPs.\nPart 3: Compare results obtained with ARIBA Resfinder versus Pathogenwatch\nOpen the ARIBA summary files and compare them with the AMR genes and AMR SNPs files you downloaded from Pathogenwatch.\n\nWhich tool produced more results? Why?\nWhich tool would you use to report AMR in your collection of S. aureus?\n\nA Python script that compares the ARIBA Resfinder summary result with the Pathogenwatch AMR genes result is available on Github: https://github.com/monicaiabrudan/bacterial-genomics/blob/main/Compare_AMR_files.ipynb\nThe input files can be found here: https://github.com/monicaiabrudan/bacterial-genomics/tree/main/Compare_AMR_files"
  },
  {
    "objectID": "module3/module3.html#core-and-accessory-genomes-with-roary",
    "href": "module3/module3.html#core-and-accessory-genomes-with-roary",
    "title": "Module 3",
    "section": "",
    "text": "Read about Roary here.\nRoary is a high speed stand alone pan genome pipeline, which takes annotated assemblies in GFF3 format (produced by Prokka (Seemann, 2014)) and calculates the pan genome. Using a standard desktop PC, it can analyse datasets with thousands of samples, something which is computationally infeasible with existing methods, without compromising the quality of the results. 128 samples can be analysed in under 1 hour using 1 GB of RAM and a single processor. To perform this analysis using existing methods would take weeks and hundreds of GB of RAM. Roary is not intended for meta-genomics or for comparing extremely diverse sets of genomes.\ndocker run --rm -it -v $(pwd):$(pwd) sangerpathogens/roary roary -f /home/ubuntu/Data/roary /home/ubuntu/Data/annotations/*/*.gff\nThe summary output is present in the summary_statistics.txt\nCore genes (99% &lt;= strains &lt;= 100%) 1861\nSoft core genes (95% &lt;= strains &lt; 99%) 0\nShell genes (15% &lt;= strains &lt; 95%) 962\nCloud genes (0% &lt;= strains &lt; 15%) 838\nTotal genes (0% &lt;= strains &lt;= 100%) 3661\nAdditionally, Roary produces a gene_presence_absence.csv file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.\nDrop the accessory_binary_genes.fa.newick and gene_presence_absence.csv files in Phandango\n\nWhen you analyze your own data, you will need a phylogeny that represents the evolutionary history of your isolates. The inference of a phylogenetic tree is not part of roary‚Äôs functions, but you can use the core gene alignment (file: core_gene_alignment.aln) as input to infer a tree.\nRoary comes with a python script that allows you to generate a few plots to graphically assess your analysis output. You will need the gene_presence_absence.csv file and the phylogeny at /home/ubuntu/Data/trees/tree.nwk.\nYou should get three files: a pangenome matrix, a frequency plot, and a pie chart.\npython /home/ubuntu/Software/roary_plots.py /home/ubuntu/Data/trees/tree_for_phandango.nwk /home/ubuntu/roary/gene_presence_absence.csv\n\n\n\n\n\n\n\nThe first part of this exercise is broadly based on a tutorial by Martin Hunt.\n\n\n\nARIBA is a tool that identifies antibiotic resistance genes. This tutorial will walk you through the analysis of the Staphylococcus aureus data set used in the paper:\nNovel multidrug-resistant sublineages of Staphylococcus aureus clonal complex 22 discovered in India\nAbrudan, Shamanna et al, mSphere 2023 https://doi.org/10.1128/msphere.00185-23\nLearning outcomes By the end of this tutorial you can expect to be able to:\n‚Ä¢ Download and prepare the standard AMR databases for use with ARIBA\n‚Ä¢ Prepare your own database for use with ARIBA\n‚Ä¢ Perform QC on input data and understand why QC is important\n‚Ä¢ Run ARIBA on several samples to identify antibiotic resistance\n‚Ä¢ Understand the different flags produced by ARIBA\n‚Ä¢ Summarise ARIBA results for several samples\n‚Ä¢ Query the AMR results produced by ARIBA\n‚Ä¢ Use Phandango to visualise ARIBA results\nThis module comprises the following sections:\n1. Run ARIBA on a reference database\n2. View summarized results using Phandango\n\nYou can run the commands in this module in the Amazon EC2 instance you were provided by your course instructor.\nARIBA should be already installed on this machine. If you need to use a different machine, please follow the instructions from Module 1 of this course.\n1. Run ARIBA on a reference database\nConfigure the Resfinder and CARD reference databases.\nariba getref resfinder out.resfinder\nariba prepareref -f out.resfinder.fa -m out.resfinder.tsv out.resfinder.prepareref\nariba getref card out.card\nariba prepareref -f out.card.fa -m out.card.tsv out.card.prepareref\nHow to run on one sample\nARIBA needs the database directories, for eg, out.resfinder.prepareref or out.card.prepareref, and two sequencing reads files reads.1.fastq.gz and reads.2.fastq.gz. The command to run ARIBA is:\nariba run out.resfinder.prepareref reads.1.fastq.gz reads.2.fastq.gz outdir\nThe above command will make a new directory called outdir that contains the results.\nRun ARIBA on all samples\nThe S. aureus dataset consists of many samples, and we need to run ARIBA on each sample, which can be done with a ‚Äúfor‚Äù loop. We assume that the reads files are named like this:\nname1_1.fastq.gz name1_2.fastq.gz\nname2_1.fastq.gz name2_2.fastq.gz\nname3_1.fastq.gz name3_2.fastq.gz\nThen we can run ARIBA on all samples like this (you may need to edit this command depending on how your own files are named):\nEg:\nfor sample in `ls *.1.fastq.gz | sed 's/\\.1.fastq.gz//'`\ndo\n  ariba run out.resfinder.prepareref $sample_1.fastq.gz $sample_2.fastq.gz $sample.ariba\ndone\nThe output directory of each sample is called $sample.ariba, for example name1.ariba is the output directory for sample name1.\nEXERCISE: Using the information above, write a script to fit the file names and paths in your own context and run ARIBA on all samples.\nARIBA output\nThe format of the output files are described here.\nViewing ARIBA results in Phandango\nThis section describes how to use Phandango to view a summary of ARIBA results from many samples.\nThe most important output file from ARIBA is the report called report.tsv. For this tutorial, we have all sample reports in the directory /home/ubuntu/Data/ARIBA_output\nls /home/ubuntu/Data/ARIBA_output | wc -l\nARIBA has a functon called ‚Äúsummary‚Äù that can summarise presence/absence of sequences and/or SNPs across samples. It takes at least two ariba reports as input, and makes a CSV file that can be opened in your favourite spreadsheet program, and also makes input files for Phandango. The two Phandango files (a tree and a CSV file) can be dropped straight into the Phandango page for viewing.\nThe tree that ARIBA makes is based on the CSV file, which contains results of presence/absence of sequence and SNPs, and other information such as percent identity between contigs and reference sequences. This means that it does not necessarily represent the real phylogeny of the samples. It is more accurate to provide a tree built from the sequencing data. For this reason, we will use a pre-computed tree file /home/ubuntu/Data/tree_for_phandango.tre.\nBasic usage of ariba summary\nCheck the options of ariba summaryusing the following command:\nariba summary -h\nFirst, let‚Äôs run ariba summary using the default settings, except we will skip making the tree:\nariba summary --no_tree out /home/ubuntu/Data/ARIBA_output/*.tsv\nWe can see that this made two files:\nls out.*\nThey are the same except for the first line, which has Phandango-specific information. ARIBA uses the filenames as sample names in the output:\nhead -n 2 out.phandango.csv\nThe first name is ‚Äú/home/ubuntu/Data/ARIBA_output/*/sample.tsv‚Äù, and the rest are named similarly.\nThis is not ideal, as it will look ugly in Phandango. Further, the names must exactly match the names in the tree file for Phandango to work (have a look in the tree /home/ubuntu/Data/trees/tree_for_phandango.nwk). You could do a little hacking here using the Unix command sed on the CSV file. Instead, we can supply ARIBA with a file of filenames that also tells ariba what to call the samples in its output CSV files. Instead of ‚Äú/home/ubuntu/Data/ARIBA_output/sample.tsv‚Äù, we would like to simply use ‚Äúsample‚Äù, which is cleaner and matches the tree file. It also means we can (and will) repeatedly run ariba summary with different options, and get output files that can be loaded straight into Phandango. This is one way to make the file with the naming information:\nls /home/ubuntu/Data/ARIBA_output/*/report.tsv | awk -F/ '{name=$(NF-1); sub(/\\.ariba$/, \"\", name); print $0, name}' &gt; ARIBA_output/filenames.fofn\nThe file is quite simple. Column 1 is the filename, and column 2 is the name we would like to use in the output.\n head ARIBA_output/filenames.fofn\nNow we can rerun summary using this input file. Note the use of the new option ‚Äìfofn.\nariba summary --no_tree --fofn ARIBA_output/filenames.fofn ARIBA_output/out /home/ubuntu/Data/ARIBA_output/*/report.tsv\nCheck that the renaming worked:\nhead -n 2 out.phandango.csv\nNow go to Phandango and drag and drop the files out.phandango.csv and trees/tree_for_phandango.nwk into the window. The result should like this\n\nThis a very high-level summary of the data. For each cluster, it is simply saying whether or not each sample has a ‚Äòmatch‚Äô. Green means a match, and pink means not a match. For presence/absence genes, this means that the gene must simply be there to count as a match. If it is a ‚Äúvariant only‚Äù gene, then the gene must be there and one of the variants that we told ARIBA about earlier when generating the ARIBA database.\nMore information per cluster\nIn addition to a simple ‚Äúyes‚Äù or ‚Äúno‚Äù as to whether a sample ‚Äúmatches‚Äù a given cluster (as explained above), more columns can be output for each cluster. See the ARIBA summary wiki page for a full description of the options.\nVariants In the previous screenshot, where the option ‚Äìpreset cluster_all, there are two variant columns: ‚Äúknown_var‚Äù and ‚Äúnovel_var‚Äù. Green means ‚Äúyes‚Äù and pink means ‚Äúno‚Äù.\nPart 2: Antimicrobial Resistance Identification using Pathogenwatch\nBrowse the public collection:\nhttps://pathogen.watch/collection/mxebr8oz0wjm-module2-s-aureus\nDownload the AMR genes and AMR SNPs.\nPart 3: Compare results obtained with ARIBA Resfinder versus Pathogenwatch\nOpen the ARIBA summary files and compare them with the AMR genes and AMR SNPs files you downloaded from Pathogenwatch.\n\nWhich tool produced more results? Why?\nWhich tool would you use to report AMR in your collection of S. aureus?\n\nA Python script that compares the ARIBA Resfinder summary result with the Pathogenwatch AMR genes result is available on Github: https://github.com/monicaiabrudan/bacterial-genomics/blob/main/Compare_AMR_files.ipynb\nThe input files can be found here: https://github.com/monicaiabrudan/bacterial-genomics/tree/main/Compare_AMR_files"
  },
  {
    "objectID": "module4/module4.html",
    "href": "module4/module4.html",
    "title": "Module 4",
    "section": "",
    "text": "SARS-CoV-2 surveillance in Romania\nGenomic surveillance of pathogens aims to understand the emergence and dissemination of pathogens or their lineages of risk with the ultimate goal of implementing evidence-based interventions to protect public health. Epidemiological data from patients is collected by healthcare professionals. Data on species identification and any phenotypic or molecular characterization of the isolates is often generated by the microbiology laboratories linked to healthcare facilities and/or the reference laboratory. In an ideal scenario, the different sources of laboratory data are stored in a centralised surveillance system and database (such as WHONET). However, these systems rarely also incorporate genomic data produced by bioinformaticians.\nThe Centre for Genomic Pathogen Surveillance develops free web applications for integration and visualisation of surveillance (and other) data, called data-flo and Microreact. The aim of this module is to highlight the role of the various analytics and sources of data in pathogen surveillance, and how streamlined data integration is essential for real-time decision making, while also introducing resources that can facilitate the inclusion of this topic in teaching curricula."
  },
  {
    "objectID": "module4/module4.html#input-files",
    "href": "module4/module4.html#input-files",
    "title": "Module 4",
    "section": "Input files:",
    "text": "Input files:\nhttps://drive.google.com/drive/folders/1yP-ectAQ0plj8G1rcd_UNqPOyFIC7axa?usp=sharing"
  },
  {
    "objectID": "module4/module4.html#resources-needed",
    "href": "module4/module4.html#resources-needed",
    "title": "Module 4",
    "section": "Resources needed",
    "text": "Resources needed\nInternet access, Projector & screen, Participants should bring their laptops equipped with Google Chrome or Mozilla Firefox.\nAccess to: https://microreact.org/, https://pathogen.watch/, https://data-flo.io/"
  },
  {
    "objectID": "module4/module4.html#introduction",
    "href": "module4/module4.html#introduction",
    "title": "Module 4",
    "section": "Introduction",
    "text": "Introduction\nNote: This case-study is based on real sequence data from¬†the article ‚ÄúMolecular epidemiology analysis of SARS-CoV-2 strains circulating in Romania during the first months of the pandemic.‚Äù by Surleac, Marius, et al., https://doi.org/10.3390/life10080152 , but it has been modified for the purposes of teaching this course.¬†\n\n\n\nThe workflow of this exercise\n\n\nExplore the epidemiological information related to 24 SARS-CoV-2 samples from Romania.\n\n\n\nCountry\nTown\nNAME\nDate\n\n\n\n\nRomania\nBucuresti\nEPI_ISL_468134\n21/03/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468135\n22/03/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468136\n22/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468137\n22/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468138\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468139\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468140\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468141\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468142\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468143\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468144\n25/03/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468145\n28/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468146\n08/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468147\n11/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468148\n11/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468149\n12/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468150\n12/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468151\n15/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468152\n16/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468153\n17/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468154\n17/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468155\n18/04/2020\n\n\nRomania\nSuceava\nEPI_ISL_468157\n29/05/2020\n\n\nRomania\nSuceava\nEPI_ISL_468158\n29/05/2020\n\n\n\nThe genomes of the isolates presented in the table above have been sequenced, assembled and the assemblies have been uploaded to Pathogenwatch for further analyses.\nActivity 1. Data analysis of 24 SARS-CoV-2 genomes from Romania, collected during the early months of the pandemic.\nExplore a collection of 24 SARS-CoV-2 genomes in Pathogenwatch.\nhttps://pathogen.watch/collection/5hpqvjar7gfa-sars-cov-2-romania-v1\nQ1: In the statistics tab, check the genome lengths. Why do you think these have variable values?\nQ2: Why are the values of the N50 equal to the values of the genome lengths?\nQ3: What do you think of the QC statistics of the genome assemblies overall?\nQ4: Go to the Typing tab. What do you think is a ‚Äúlineage‚Äù of SARS-CoV-2? Why were ‚Äúlineages‚Äù defined early in the pandemic? What lineages do these genomes belong to?\nQ5: Go to the ‚ÄúNotable mutations‚Äù tab. Observe that all the genomes in this collection belong to the same lineage and they all share the same notable mutations. What does this tell you about the diversity of this collection?\nQ6: Click on one of the genomes and see the ‚ÄúGenome card‚Äù. Check the details of this lineage in a global context, by pressing View B.1.1 in the COG-UK Global Microreact. Where did this lineage spread and when?\n\n\n\nPathogenwatch screenshot showing a genome card for a SARS-CoV-2 genome from the lineage B1.1 . Notice the View B.1.1 in the COG-UK Global Microreact link\n\n\n\n\n\nMicroreact view showing isolates from the B1.1 lineage.\n\n\nActivity 2. Data integration\nIn this activity, you will merge the epidemiological information related to the isolates, the bioinformatics analysis files produced by Pathogenwatch and with the tree in Newick format, to produce a Microreact.\nDownload all the files used in this exercise from https://drive.google.com/drive/folders/1yP-ectAQ0plj8G1rcd_UNqPOyFIC7axa\nGo to Data-flo: https://data-flo.io/run?4LxncBtNr69TmfzTGj9Szm\nClick on the Run button. Load the input files. Notice that you need an ‚Äúaccess token‚Äù from Microreact.\n\n\n\nThe RUN button in Data-flo\n\n\n\n\n\nYou will find your access token in your Microreact account, under ‚ÄúAccount settings‚Äù\n\n\n\n\n\nThe output of data-flo is a link to a microreact\n\n\n\n\n\nThe microreact produced by the data-flo\n\n\nCopy the data-flo to your account.\n\n\n\nOnce you are logged in to your data-flo account, you will be able to copy a public data-flo to your own account. Press on the symbol showing two overlapping papers on the right top corner to copy the current data-flo to your account\n\n\nRun the data-flo in debug mode and explore how data is being transformed between steps.\n\n\n\nPress the bug symbol at the top left corner to trigger the Debug mode. Notice the Data-flo debugger loading at the bottom of the screen.\n\n\nQ1: Which adaptors are responsible for data entry?\nQ2: What does the ‚ÄúForward geocoding‚Äù adaptor do?\nQ3: Check the Microreact link produced by the data-flo.\nActivity 3. Data visualisation with Microreact\nExplore the Microreact produced by the Data-flo in the previous step.\nColour the leaves of the tree by ‚ÄúTown‚Äù and display the metadata columns for ‚ÄúNotable mutations‚Äù and the ‚ÄúPangolin lineage‚Äù.\n\n\n\nIn order to set the colour of the leaves of a tree in Microreact, go to the eye symbol on top right and choose ‚ÄúTown‚Äù in the ‚ÄúColour Column‚Äù\n\n\n\n\n\nSelect which columns you would like to display in the metadata blocks\n\n\nYour personal Microreact should like this one: https://microreact.org/project/sars-cov-2-romania1\nQ1: Look at the timelime. Which is the first isolate sampled from this collection? When was it sampled and where?\nQ2: When was the first isolate from the Bucharest sampled and when was the first isolate from Suceava sampled?\nActivity 4. Data analysis with context genomes\nThe bioinformatician in your team has found another 26 SARS-CoV-2 genomes from Romania in a public database and wants to include them in your study.\nView the whole collection of 51 genomes in Pathogenwatch https://pathogen.watch/collection/gqhf7ndghsim-test2\nSelect one of the genomes that belong to the lineage B.1.408 and open the ‚ÄúGenome card‚Äù.\nThen click on the View B.1.408 in the COG-UK Global Microreact\nQ1: How many entries of this sublineage are found in the database?\nQ2: In what countries was this sub-lineage found ? What does this tell you about the transmission links between Romania and those countries?\n\n\n\nLineage B.1.408 shown in Microreact\n\n\nSelect the genome that belongs to the B.1.520 lineage, open its ‚ÄúGenome card‚Äù and click on the View B.1.520 in the COG-UK Global Microreact.\nQ3: In what countries was this sub-lineage found ? What does this tell you about the transmission links between Romania and those countries?\n\n\n\nLineage B.1.520 shown in Microreact\n\n\nActivity 5. Data integration for the collection of 51 genomes.\nLoad the same Data-flo as in Activity 2: https://data-flo.io/run?4LxncBtNr69TmfzTGj9Szm\nRun the data-flo with the new set of input files and load the new Microreact.\nActivity 6. Explore the new Microreact.\nColour the nodes of the tree by ‚ÄúTown‚Äù and display the mutations in the metadata blocks.\nYour microreact should look like this: https://microreact.org/project/sars-cov-2-romania-2\nQ1: According to this study, where did SARS-CoV-2 spread first outside of Suceava and Bucuresti?\nQ2: Where was the isolate belonging to the B.1.520 lineage found in Romania?\nActivity 7. Discuss the Romanian sequences in a global context.\n\n\n\nFigure from the original study published at https://doi.org/10.3390/life10080152\n\n\nQ1: How many introductions of SARS-CoV-2 into Romania can you identify based on this figure?\nQ2: Why do you think that the isolates from Suceava are interleaved with isolates from abroad?"
  },
  {
    "objectID": "module4/module4.html#appendix",
    "href": "module4/module4.html#appendix",
    "title": "Module 4",
    "section": "Appendix",
    "text": "Appendix\nData-flo Documentation: https://docs.data-flo.io/introduction/readme\nIf you found data-flo useful and would like to continue using it, start by creating an account https://data-flo.io/signin. There are a few public workflows with useful transformations such as left join two CSVs (https://data-flo.io/run?bzYR3DtxRJsBty5ezb5LoJ) or the Geocoder (https://data-flo.io/run?kvpsi3T8V) that you can try.\n\nA similar workflow to the one used on this course can be found in the documentation https://docs.data-flo.io/tutorials/prep-outbreak-data-for-microreact. In this workflow, one of the inputs is a SNP matrix.¬†\nData-flo also allows you to import data from a Google Spreadsheet (https://docs.data-flo.io/using-data-flo/specific-adaptors/google-spreadsheet) or from a database (https://docs.data-flo.io/tutorials/common-use-cases-solved/connect-directly-to-a-database) (as opposed to downloading data from the spreadsheet/database into a file).¬†\n\nMicroreact documentation: https://docs.microreact.org/\nTo be able to edit and manage projects, please create your own Microreact Account https://microreact.org/api/auth/signin.¬†\nYou can customise the colours of your visualisations by assigning colours to the different variables before you create the Microreact project and/or after the project was created https://docs.microreact.org/instructions/labels-colours-and-shapes.\nEveryone with access to a Microreact project has access to the data the project uses. Privacy and permissions can be configured to change who can access the project and its data https://docs.microreact.org/instructions/access-control-and-project-sharing."
  },
  {
    "objectID": "Day2/AMR-detection.html",
    "href": "Day2/AMR-detection.html",
    "title": "AMR-detection",
    "section": "",
    "text": "Adapted from training materials by Mat Beale, Genome Assembly and Analysis - Costa Rica 2025 course\nOne of the greatest challenges of sequencing a genome is determining how to arrange sequencing reads into chromosomes and plasmids. This process of determining how the reads fit together by looking for overlaps between them is called genome assembly. In this module, we are going to explore genome assembly using short- and long-read sequencing technologies and see how they can be used to characterize isolates of interest. Additionally, we will annotate these genomes and use comparative genomics to analyze regions of difference and identify genetic determinants of antibiotic resistance.\nAims of this exercise:\n\nUse a resistome prediction tool to identify the genetic determinants for antibiotic resistance from sequencing reads.\nShow how short-read data can be assembled into a draft genome.\nOrder the draft genome against a reference sequence.\nAnnotate the reordered draft genome.\nDemonstrate how comparative genomics can identify and analyse regions of difference that distinguish genomes.\nIdentify the genetic basis of resistance and explain the evolution of resistance in the isolates investigated.\n\n\n\nStaphylococcus aureus is a bacterial pathogen that has gained attention in recent years due to its capacity to evolve into highly virulent and antibiotic-resistant strains. Its prevalence in hospital settings poses a significant burden on healthcare systems worldwide, being the leading cause of hospital-acquired infections. The rise of antibiotic resistance among S. aureus strains, particularly to Œ≤-lactam antibiotics like methicillin, has reached alarming levels in regions such as Europe, the US, and Japan, where 40-60% of hospital-acquired S. aureus infections are now methicillin-resistant. The emergence of methicillin-resistant S. aureus (MRSA) dates back to the 1960s, and since then, various successful clones have disseminated globally.\n\n\n\nIn this module, we will assemble the genome of a strain of Staphylococcus aureus, , which was sequenced as part of an MRSA outbreak investigation (K√∂ser et al., 2012, N Engl J Med. 366:2267-75). Through multilocus sequence typing (MLST), the isolate was identified as belonging to sequence type 1 (ST1), a lineage of S. aureus more frequently associated with community infections rather than hospital-acquired infections. ST1 strains typically exhibit lower antibiotic resistance compared to those commonly found in hospitals.\n\n\n\nWe will conduct a comprehensive analysis starting with querying the resistome of against a resistance gene database. Additionally, we will perform genome assembly and comparative analysis against the chromosomes of two other ST1 isolates: MSSA476, isolated in the UK (Holden et al., 2004, PNAS. 101:9786-91), and MW2, isolated in the USA (Baba et al., 2002, Lancet 359:1819-27). Both MSSA476 and MW2 have been fully sequenced, annotated, and deposited in EMBL, providing valuable reference genomes for our comparative genomic study.\n\n\n\nThe three ST1 isolates are closely related but exhibit different antibiotic resistance profiles:\n- 16B is resistant to penicillin, fusidic acid, methicillin and erythromycin\n- MSSA476 is resistant to penicillin and fusidic acid\n- MW2 is resistant to penicillin and methicillin.\nUsing a comparative genomic approach we will identify regions of difference, the genetic basis of the antibiotic resistance in , and the genetic mechanisms that drive the evolution of resistance.\n\n\n\nThe directory contains:\n\nThree pairs of sequencing reads :\n\n16B_1.fastq, 16B_2.fastq\nMSSA476_1.fastq, MSSA476_2.fastq\nMW2_1.fastq, MW2_2.fastq\n\nfasta format files for the chromosomes of MW2 and MSSA476 (MW2.dna and MSSA476.dna)\nEMBL format files of the annotation of the chromosomes of MW2 and MSSA476 (MW2.embl and MSSA476.embl)\nEMBL format files of mobile genetic elements of the chromosomes of MW2 and MSSA476 (MW2_MGEs.tab and MSSA476_MGEs.tab)\nA directory bakta_database, containing an annotation database"
  },
  {
    "objectID": "Day2/AMR-detection.html#background",
    "href": "Day2/AMR-detection.html#background",
    "title": "AMR-detection",
    "section": "",
    "text": "Staphylococcus aureus is a bacterial pathogen that has gained attention in recent years due to its capacity to evolve into highly virulent and antibiotic-resistant strains. Its prevalence in hospital settings poses a significant burden on healthcare systems worldwide, being the leading cause of hospital-acquired infections. The rise of antibiotic resistance among S. aureus strains, particularly to Œ≤-lactam antibiotics like methicillin, has reached alarming levels in regions such as Europe, the US, and Japan, where 40-60% of hospital-acquired S. aureus infections are now methicillin-resistant. The emergence of methicillin-resistant S. aureus (MRSA) dates back to the 1960s, and since then, various successful clones have disseminated globally."
  },
  {
    "objectID": "Day2/AMR-detection.html#an-outbreak-sample",
    "href": "Day2/AMR-detection.html#an-outbreak-sample",
    "title": "AMR-detection",
    "section": "",
    "text": "In this module, we will assemble the genome of a strain of Staphylococcus aureus, , which was sequenced as part of an MRSA outbreak investigation (K√∂ser et al., 2012, N Engl J Med. 366:2267-75). Through multilocus sequence typing (MLST), the isolate was identified as belonging to sequence type 1 (ST1), a lineage of S. aureus more frequently associated with community infections rather than hospital-acquired infections. ST1 strains typically exhibit lower antibiotic resistance compared to those commonly found in hospitals."
  },
  {
    "objectID": "Day2/AMR-detection.html#analyses",
    "href": "Day2/AMR-detection.html#analyses",
    "title": "AMR-detection",
    "section": "",
    "text": "We will conduct a comprehensive analysis starting with querying the resistome of against a resistance gene database. Additionally, we will perform genome assembly and comparative analysis against the chromosomes of two other ST1 isolates: MSSA476, isolated in the UK (Holden et al., 2004, PNAS. 101:9786-91), and MW2, isolated in the USA (Baba et al., 2002, Lancet 359:1819-27). Both MSSA476 and MW2 have been fully sequenced, annotated, and deposited in EMBL, providing valuable reference genomes for our comparative genomic study."
  },
  {
    "objectID": "Day2/AMR-detection.html#the-research-questions",
    "href": "Day2/AMR-detection.html#the-research-questions",
    "title": "AMR-detection",
    "section": "",
    "text": "The three ST1 isolates are closely related but exhibit different antibiotic resistance profiles:\n- 16B is resistant to penicillin, fusidic acid, methicillin and erythromycin\n- MSSA476 is resistant to penicillin and fusidic acid\n- MW2 is resistant to penicillin and methicillin.\nUsing a comparative genomic approach we will identify regions of difference, the genetic basis of the antibiotic resistance in , and the genetic mechanisms that drive the evolution of resistance."
  },
  {
    "objectID": "Day2/AMR-detection.html#finding-the-data",
    "href": "Day2/AMR-detection.html#finding-the-data",
    "title": "AMR-detection",
    "section": "",
    "text": "The directory contains:\n\nThree pairs of sequencing reads :\n\n16B_1.fastq, 16B_2.fastq\nMSSA476_1.fastq, MSSA476_2.fastq\nMW2_1.fastq, MW2_2.fastq\n\nfasta format files for the chromosomes of MW2 and MSSA476 (MW2.dna and MSSA476.dna)\nEMBL format files of the annotation of the chromosomes of MW2 and MSSA476 (MW2.embl and MSSA476.embl)\nEMBL format files of mobile genetic elements of the chromosomes of MW2 and MSSA476 (MW2_MGEs.tab and MSSA476_MGEs.tab)\nA directory bakta_database, containing an annotation database"
  },
  {
    "objectID": "Day2/AMR-detection.html#using-the-genome-to-predict-antibiotic-resistance-phenotype",
    "href": "Day2/AMR-detection.html#using-the-genome-to-predict-antibiotic-resistance-phenotype",
    "title": "AMR-detection",
    "section": "Using the genome to predict antibiotic resistance phenotype",
    "text": "Using the genome to predict antibiotic resistance phenotype\nOne of the benefits of whole genome sequencing bacterial pathogens is that you capture the genomic inventory of the organism. This has been capitalized on in clinical microbiology for the in silico prediction of antibiotic resistance directly from whole genome sequencing data. This is being developed as an alternative to phenotypic sensitivity testing of microorganisms in the laboratory, where microorganisms are routinely sequenced.\nFor many microorganisms the genetic basis of antibiotic resistance has been extensively studied. This means that the genes responsible for resistance have been identified and sequenced, and can be used to compile a database of resistance determinants and used to query an organism‚Äôs genome and define its resistome. Based on the presence or absence of genes or mutations it is possible to make a prediction of the antibiotic sensitives of an organism. For some species of bacteria this works better than others. For example, S. aureus the correlation between the genotype and phenotype for most commonly used antibiotics is above 99%. However, for other organisms, such as members of the Enterobacteriaceae, the concordance is a lot lower, as these organisms have a more extensive array of resistance mechanisms and determinants.\nA recent review from a EUCAST subcommittee summarized the current development status of whole genome sequencing for bacterial antimicrobial susceptibility testing (AST) for a range or organisms: Ellington MJ, et al., (2017) The role of whole genome sequencing in antimicrobial susceptibility testing of bacteria: report from the EUCAST Subcommittee. Clin Microbiol Infect. 23:2-22. PubMed PMID: 27890457."
  },
  {
    "objectID": "Day2/AMR-detection.html#resistance-phenotype-of-16b-mw2-and-mssa476",
    "href": "Day2/AMR-detection.html#resistance-phenotype-of-16b-mw2-and-mssa476",
    "title": "AMR-detection",
    "section": "Resistance phenotype of 16B, MW2 and MSSA476",
    "text": "Resistance phenotype of 16B, MW2 and MSSA476\nFrom the phenotypic data you have been given you know that 16B exhibits resistance to penicillin, fusidic acid, methicillin and erythromycin, however you do not know what genes are responsible for this in this isolate. In the first part of this exercise you are going to use a piece of software, ariba, and a publicly available curated antibiotic resistance gene database from ResFinder, to rapidly predict the resistome of 16B from the Illumina sequence reads. You will also do this for this other ST1 S. aureus isolates MW2 and MSSA476, and correlate the phenotypic metadata with the genetic information."
  },
  {
    "objectID": "Day2/AMR-detection.html#determining-the-antibiotic-resistance-genotype",
    "href": "Day2/AMR-detection.html#determining-the-antibiotic-resistance-genotype",
    "title": "AMR-detection",
    "section": "Determining the antibiotic resistance genotype",
    "text": "Determining the antibiotic resistance genotype\nariba (Antimicrobial Resistance Identifier by Assembly) is a freely available tool on GitHub. This tool requires a FASTA input of reference sequences, which can be either a multi-FASTA file or a database of antibiotic resistance genes or non-coding sequences. The database serves as one of your inputs, while the other input is paired sequence reads. ariba reports which of the reference sequences were found and provides detailed information on the quality of the assemblies and any variants between the sequencing reads and the reference sequences.\nResFinder is a web resource for the prediction of antibiotic resistance (available at www.genomicepidemiology.org). It utilises a curated database of over 2100 acquired antibiotic resistance determinants (Zankari E, et al., 2012. ‚ÄúIdentification of acquired antimicrobial resistance genes.‚Äù J Antimicrob Chemother. 67:2640-4).\nariba is installed on the virtual machine. You will use it to download the ResFinder database locally and then use ariba to examine the resistome of your isolates. Further information about ariba can be found in the ariba wiki (Hunt M, et al., 2017. ‚Äúariba: rapid antimicrobial resistance genotyping directly from sequencing reads.‚Äù Microb Genom. 3:e000131).\nThe results can then be visualised using Phandango, an interactive web tool for viewing your outputs."
  },
  {
    "objectID": "Day2/AMR-detection.html#download-the-resfinder-database",
    "href": "Day2/AMR-detection.html#download-the-resfinder-database",
    "title": "AMR-detection",
    "section": "Download the ResFinder database",
    "text": "Download the ResFinder database\nTo download the database you use the ariba getref command.\nIn the command below we:\n\nSpecify the database to download\n\nresfinder\n\nSpecify the output name prefix\n\nout.resfinder\n\n\nariba has been installed using conda. We must activate the relavent conda environment before running any commands using ariba.\nconda activate ariba-env\nNow we can run ariba\nariba getref resfinder out.resfinder\nAlternative database options that can be used are: card, plasmidfinder, resfinder, vfdb_core, vfdb_full.\nNext you need to format the reference database using the ariba prepareref command.\nIn the command below we:\n\nSpecify the file of resistance genes in fasta format\n\n‚Äìf out.resfinder.fa\n\nSpecify the metadata file for the resistance genes\n\n‚Äìm out.resfinder.tsv\n\nSpecify directory that will contained the prepared database files for running ariba\n\nout.resfinder.prepareref\n\n\nariba prepareref -f out.resfinder.fa -m out.resfinder.tsv out.resfinder.prepareref \nThis command may generate warnings indicating sequences or variants were removed from the database upon formatting. Despite the warnings, the command has ran successfully and you may proceed on to the next step."
  },
  {
    "objectID": "Day2/AMR-detection.html#run-ariba-on-16b",
    "href": "Day2/AMR-detection.html#run-ariba-on-16b",
    "title": "AMR-detection",
    "section": "Run ariba on 16B",
    "text": "Run ariba on 16B\nNext using the 16B fastq files run local assemblies and call variants using the ariba run command. As the command is running, identified variants will be printed to screen.\n\nSpecify the directory containing the ResFinder database files\n\nout.resfinder.prepareref\n\nSpecify the 16B forward and reverse fastq files\n\n16B_1.fastq 16B_2.fastq\n\nSpecify the the directory containing the results\n\n16B_out.run\n\n\nariba run out.resfinder.prepareref 16B_1.fastq 16B_2.fastq 16B_out.run"
  },
  {
    "objectID": "Day2/AMR-detection.html#run-ariba-on-mw2",
    "href": "Day2/AMR-detection.html#run-ariba-on-mw2",
    "title": "AMR-detection",
    "section": "Run ariba on MW2",
    "text": "Run ariba on MW2\nRepeat the ariba run on the MW2 fastq files.\n\nSpecify the directory containing the ResFinder database files\n\nout.resfinder.prepareref\n\nSpecify the MW2 forward and reverse fastq files\n\nMW2_1.fastq MW2_2.fastq\n\nSpecify the the directory containing the results\n\nMW2_out.run\n\n\nariba run out.resfinder.prepareref MW2_1.fastq MW2_2.fastq MW2_out.run"
  },
  {
    "objectID": "Day2/AMR-detection.html#run-ariba-on-mssa476",
    "href": "Day2/AMR-detection.html#run-ariba-on-mssa476",
    "title": "AMR-detection",
    "section": "Run ariba on MSSA476",
    "text": "Run ariba on MSSA476\nRepeat the ariba run on the MSSA476 fastq files.\n\nSpecify the directory containing the ResFinder database files\n\nout.resfinder.prepareref\n\nSpecify the MSSA476 forward and reverse fastq files\n\nMSSA476_1.fastq MSSA476_2.fastq\n\nSpecify the the directory containing the results\n\nMSSA476_out.run\n\n\nariba run out.resfinder.prepareref MSSA476_1.fastq MSSA476_2.fastq MSSA476_out.run"
  },
  {
    "objectID": "Day2/AMR-detection.html#compile-the-ariba-results",
    "href": "Day2/AMR-detection.html#compile-the-ariba-results",
    "title": "AMR-detection",
    "section": "Compile the ariba results",
    "text": "Compile the ariba results\nNext you need to compile the ariba results from the three isolates using the the ariba summary command.\n\nSpecify the prefix for the output files\n\nout.summary\n\nSpecify the report files made by the separate runs of ariba for each isolate\n\n16B_out.run/report.tsv MW2_out.run/report.tsv MSSA476_out.run/report.tsv\n\n\nariba summary out.summary 16B_out.run/report.tsv MW2_out.run/report.tsv MSSA476_out.run/report.tsv\nWe must now deactivate the ariba conda environment. Failure to deactivate the environment will prevent usage of downstream tools.\nconda deactivate"
  },
  {
    "objectID": "Day2/AMR-detection.html#visualize-in-phandango",
    "href": "Day2/AMR-detection.html#visualize-in-phandango",
    "title": "AMR-detection",
    "section": "Visualize in Phandango",
    "text": "Visualize in Phandango\nThe ariba summary command generates three files. You can see these in your directory with the ls -l command:\n\nout.summary.csv - summary of identifying genes and matches in the isolates\nout.summary.phandango.csv - a version of summary file for viewing in Phandango\nout.summary.phandango.tre - tree based on matches in the out.summary.csv file\n\nTo visualize the results open up the Firefox web browser, and type in the URL: https://jameshadfield.github.io/phandango/\nFrom a file view window drag and drop the two phandango files, out.summary.phandango.tre and out.summary.phandango.csv, into the browser window.\n\n\nIn the browser window the tree is displayed on the left and represents relationships of the isolates based on the shared resistance determinants displayed in the right-hand panel, where the column indicate genes, and the green blocks indicate matches. The pink blocks indicate that the isolates are negative for those genes.\nWhat are the genes identified, and which antibiotics do they encode resistance for? To help you understand what what genes ResFinder is using for different antibiotics you can explore here: https://cge.food.dtu.dk/services/ResFinder/gene_overview.php\nHow do the resistomes predicted for each isolate compare with the phenotypic data? You can find the resistance phenotypes here: Resistance phenotype of 16B, MW2 and MSSA476\nPhandango interpretation:\n\nTree shows relationships between samples\nColored cells show AMR gene presence\nDarker colors = higher confidence matches\nWhite = gene absent\nAlternative: AMRFinderPlus"
  },
  {
    "objectID": "Day3/manual.html",
    "href": "Day3/manual.html",
    "title": "Web tools for genomic epidemiology",
    "section": "",
    "text": "&lt;&lt;&lt; Go back to Manual Contents Page"
  },
  {
    "objectID": "Day3/manual.html#collect-field-data-with-epicollect",
    "href": "Day3/manual.html#collect-field-data-with-epicollect",
    "title": "Web tools for genomic epidemiology",
    "section": "1. Collect field data with Epicollect",
    "text": "1. Collect field data with Epicollect\nhttps://five.epicollect.net/\nThe disease detectives will work in pairs, and will be provided with a collection tube with a barcode sample identifier to collect a surface or water sample, and will use their mobile phone to collect associated metadata in the field.\nMake sure your phone is connected to WiFi. Open the Epicollect5 app on your mobile phone.\n\nAdd the project San Jose 2025 K pneumoniae Outbreak by clicking on + ADD PROJECT and typing into the search box ‚Äìyou will need to be connected to WiFi.\n\n\nUsing the plan of the UEPV campus provided, walk to the location of your collection point. Locations are indicated by the black dots labelled a-j. You will not need internet access to collect data in the field.\n\nOpen the San Jose 2025 K pneumoniae Outbreak project and add an entry. Follow the form to collect a sample and associated metadata.\n\nYour team will collect only one sample, but both of you can collect the associated data on Epicollect5. However, make sure that only one of you uploads the data later on to avoid duplications.\nReturn to the Epidemic Intelligence Center (i.e.¬†the classroom).\nOne of the pair members only. Upload your entry (and image) to the Epicollect5 server ‚Äìmake sure you are connected to the WiFi.\n\nSubmit your sample to the instructor, which will be sent to the reference lab for culture confirmation.\nOnce all entries are uploaded by the disease detectives we will take a look at the data together on https://five.epicollect.net/project/san-jose-2025-k-pneumoniae-outbreak\nAnswer the following questions:\n\n\nWhat is the most common type of source?\nWas it possible to collect a sample from all sources? If not, what were the reasons?"
  },
  {
    "objectID": "Day3/manual.html#analyse-genomes-with-pathogenwatch",
    "href": "Day3/manual.html#analyse-genomes-with-pathogenwatch",
    "title": "Web tools for genomic epidemiology",
    "section": "2. Analyse genomes with PathogenWatch",
    "text": "2. Analyse genomes with PathogenWatch\nThe reference lab has sent you the culture results in an Excel file called (lab_results.xlsx). The lab reported that 2 out of 13 water samples from the UEPV campus were positive for K. pneumoniae. This immediately prompted the closure of the 2 water sources and internal investigation.\n\nDoes this confirm that the source of the outbreak can be found in the campus of UEPV?\n\nOne colony from each source was sequenced on Illumina MiSeq by the reference lab and the bioinformatics team has assembled each of them. We will use https://pathogen.watch to identify the sequence type (ST) of these genomes using the Pasteur scheme.\nClick on the upload link at the top right\n\nClick on Single Genome FASTAs\n\nDrag and Drop provided fasta files (file1 and file2) into the browser and wait for analysis to finish\n\nClick on VIEW GENOMES\n\nSelect the two analysed genomes\n\nClick on Selected Genomes\n\nClick on Download data\n\nClick on MLST (Pasteur). This will download a csv file with the results.\n\nYou should now be able to find the mlst-Pasteur.csv in your Downloads folder."
  },
  {
    "objectID": "Day3/manual.html#merge-data-with-data-flo",
    "href": "Day3/manual.html#merge-data-with-data-flo",
    "title": "Web tools for genomic epidemiology",
    "section": "3. Merge data with Data-flo",
    "text": "3. Merge data with Data-flo\nhttps://data-flo.io/\nNote: you need to sign-up for data-flo and Microreact. See instructions in the Resources section. Creating your own account will allow you to manage and edit your projects.\nA maximum likelihood phylogenetic tree (tree.nwk) was inferred from the genomes of the 34 clinical samples and 2 environmental (water) samples. Six genomes from a previous outbreak (Mar-Apr 2022) were also included in the tree inference and their associated data added to the epi_data.csv file.\nThe disease detectives now have the information needed for the investigation in the following formats:\n\nepi_data.csv Epi data from 34 clinical cases and 6 cases from previous outbreak\nEpicollect project Metadata of 13 water sources from the UEPV campus\nlab_results.xlsx Culture and serotyping results\nmlst-Pasteur.csv MLST results from PathogenWatch\ntree.nwk Phylogenetic tree of 34 clinical cases, 2 culture-positive water samples, and 6 cases from previous outbreak\n\nThe files are located in this link.\nWe will combine data from these different sources with a data-flo workflow that takes the files above and the data from the Epicollect project as input, and creates as an output a Microreact project where the data can be visualised.\nOpen the data-flo workflow (https://www.data-flo.io/run/is2MaZpxgjgT1o1uDkff8g)\n\nCopy the workflow to your own dataflo account.\n\nThis will open a copy of this workflow in your dataflo account.\n\nOn a different browser tab, get your microreact API access token at https://microreact.org/my-account/settings (you must already have created your microreact account).\n\nEdit the workflow to include your own microreact API access token.\n\nClick on *access token in the Create microreact project box\nFrom the options on the right, select Bind to value\nPaste your API access token in the VALUE box\n\n\nSave your newly edited dataflo project by clicking on the save icon.\n\nNow lets go to the implementation page by clicking on the RUN option.\n\nTo run the workflow paste the url of the Epicollect5 project (https://www.data-flo.io/run/jPZFNdqdkixkRnyuDkH2ti-san-jose-2025-k-pneumoniae) and upload the files, found here(file1 and file2).\n\nClick on Run. The Outputs box now shows the url of a Microreact project created by data-flo.\n\nBefore you open the Microreact project answer the following questions:\n\n\nIf later on we wanted to add additional entries (i.e.¬†water samples) to the Epicollect project, could we use the same data-flo? If so, how would this be beneficial?\nDo you use/know of any other methods to join data? If so, how do they compare with data-flo?"
  },
  {
    "objectID": "Day3/manual.html#explore-phylogenetic-and-epidemiological-data-with-microreact",
    "href": "Day3/manual.html#explore-phylogenetic-and-epidemiological-data-with-microreact",
    "title": "Web tools for genomic epidemiology",
    "section": "4. Explore phylogenetic and epidemiological data with Microreact",
    "text": "4. Explore phylogenetic and epidemiological data with Microreact\nhttps://microreact.org/\nOpen the Microreact link in the output from data-flo. It should look something like this:\n\nNow change the colour column to ‚ÄúHospital‚Äù. Click on the eye icon at the top right of the page. Select ‚ÄúHospital from the Colour Column dropdown list.\n\nScale the markers on the map by clicking on the sliders icon, then in the opening options click on Markers. Finally, toggle the Scale markers option on.\n\nAnswer the following questions:\n\n\nWhat do the markers on the map represent?\nWhich hospital reported the index (first) case? Tip: Click on the earliest sample on the timeline.\n\n\nChange the colour column to ‚ÄúSource‚Äù. Click on the eye icon at the top left of the page. Then select ‚ÄúSource‚Äù from the Colour Column dropdown list.\n\nMicroreact assigns colours automatically to every column in your metadata table, but you can customise them. Let‚Äôs change the colour column to ‚ÄúInfection Type‚Äù. Click on the eye icon at the top left of the page. Then select ‚ÄúInfection Type‚Äù from the Colour Column dropdown list.\n\nNow Select ‚ÄúCategorical‚Äù from the Colour Palette option. Next Select ‚Äúqualitative‚Äù from the Palette type dropdown list and ‚Äú5‚Äù from the Number of colours dropdown list. Finally, select the second option from the list of palettes.\n\nAdd the columns ‚ÄúWorkplace‚Äù and ‚ÄúWorkplace details‚Äù as metadata blocks. For this, select the slider icon from top right of the tree panel, then, click on the Metadata blocks button. Finally, select ‚ÄúWorkplace‚Äù and ‚ÄúWorkplace details‚Äù from the list of options.\n\nNow eexplore the microreact you just created and try to answer the following questions:"
  },
  {
    "objectID": "Day3/manual.html#insights",
    "href": "Day3/manual.html#insights",
    "title": "Web tools for genomic epidemiology",
    "section": "Insights",
    "text": "Insights\n\nCan you confirm that the K. pneumoniae isolated from the UEPV campus are closely related to the patient isolates?\nWas it useful to include genomes from a previous outbreak in the analysis? Is the current outbreak related to the past outbreak from Mar-Apr 2022?\nIs the UEPV Campus the origin of the outbreak?\nShould we investigate another location in the city?"
  },
  {
    "objectID": "Day3/manual.html#install-and-test-epicollect5-on-your-mobile-phone",
    "href": "Day3/manual.html#install-and-test-epicollect5-on-your-mobile-phone",
    "title": "Web tools for genomic epidemiology",
    "section": "Install and test Epicollect5 on your mobile phone",
    "text": "Install and test Epicollect5 on your mobile phone\nEpicollect5 is available on Android and iOS. To install it on your mobile device, get it from Google Play or the Apple App Store. To test Epicollect5 on your phone, open the application and click on the pre-loaded EC5 Demo Project. Click on + Add entry and follow the questionnaire. Please answer all the questions, including taking a photo with your phone‚Äôs camera. Once you‚Äôve answered the questions, save and upload the entry. Please note that you must be connected to the internet (or to a mobile network) to be able to upload an entry."
  },
  {
    "objectID": "Day3/manual.html#sign-in-to-data-flo",
    "href": "Day3/manual.html#sign-in-to-data-flo",
    "title": "Web tools for genomic epidemiology",
    "section": "Sign-in to data-flo",
    "text": "Sign-in to data-flo\nhttps://docs.data-flo.io/introduction/getting-started-sign-in"
  },
  {
    "objectID": "Day3/manual.html#sign-in-to-microreact",
    "href": "Day3/manual.html#sign-in-to-microreact",
    "title": "Web tools for genomic epidemiology",
    "section": "Sign-in to Microreact",
    "text": "Sign-in to Microreact\nHead over to https://microreact.org/my-account, and sign up using your email address or any other authenticating option"
  },
  {
    "objectID": "Day3/manual.html#web-colours",
    "href": "Day3/manual.html#web-colours",
    "title": "Web tools for genomic epidemiology",
    "section": "Web colours",
    "text": "Web colours\nWeb colours are used on web pages, such as the microreact.org projects, and are usually specified in hexadecimal format preceded by ‚Äã‚Äãa number sign (or hashtag sign if you were born before the Jurassic period). For example, #FFFFFF is the hexadecimal code (or hex code) for the colour ‚Äúwhite‚Äù, #000000 is the hex code for the colour ‚Äúblack‚Äù, and #FF00FF is the hex code for the colour ‚Äúmagenta‚Äù. For more information on the format of the hexadecimal code see Wikipedia page https://en.wikipedia.org/wiki/Web_colors\nUseful websites to work with web colours:\n\nColorBrewer\nhttps://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3 Colour palettes for maps\n\n\nColorHexa\nhttps://www.colorhexa.com/ Get complementary colours, colour gradients, etc.\n\n\nCoolors\nhttps://coolors.co/palettes/trending Colour palette generator\n\n\nWes Anderson colour palettes\nhttps://github.com/karthik/wesanderson Hipster colour palettes inspired by Wes Anderson movies"
  },
  {
    "objectID": "Day3/manual.html#bonus-activities",
    "href": "Day3/manual.html#bonus-activities",
    "title": "Web tools for genomic epidemiology",
    "section": "Bonus activities",
    "text": "Bonus activities\n\n1. Download data from the Epicollect5 project on the web.\n\nGo to the epicollect link provided above.\nSelect VIEW DATA.\n\n\n\nClick on Download and select your format of choice.\n\n\n\n\n2. Create a microreact project using a metadata in csv format and a phylogenetic tree.\n\nClick on the menu icon at the top left of the screen\n\n\n\nClick on Upload\n\n\n\nClick on the + symbol at the bottom right of the screen.\n\n\n\nSelect Browse Files\n\n\n\nChoose the files tree.nwk and epi_data.csv, and click on the Open button (These files were downloaded in the dataflo section).\nClick on Continue in the next prompts.\nYour view will look like this:\n\n\n\nYou can add a Timeline by clicking on the pencil icon at the top left of the screen, and clicking on Create New Timeline\n\n\n\nThen position the new panel by dragging the pointer to the bottom panel.\n\n\n\nFinally, select ‚ÄúOne column: Formatted Values‚Äù from the Temporal Data Type dropdown list, and ‚ÄúCollection Date‚Äù from the Temporal Data Column dropdown list. Click on CLOSE"
  },
  {
    "objectID": "Day3/manual.html#create-a-microreact-project-from-a-google-spreadsheet",
    "href": "Day3/manual.html#create-a-microreact-project-from-a-google-spreadsheet",
    "title": "Web tools for genomic epidemiology",
    "section": "3. Create a Microreact project from a Google Spreadsheet",
    "text": "3. Create a Microreact project from a Google Spreadsheet\n\nOpen this Google spreadsheet\n\n\n\nMake a copy of this in your own Google account by selecting Make a copy from the File menu item. When prompted, click on Make a copy. This will open a separate tab where the copy will be available.\n\n\n\n\nSet access to shareable by clicking on the Share button at the top right of the screen. A dialogue screen will pop up, here, click on the Restricted button, select Anyone with the link, and click on Done\n\n\n\nNow to publish the google spreadsheet click on the File menu item, select Share, and click on Publish on web\n\n\n\nIn the popup message click on Web page, and select Comma-separated values (.csv). Also, make sure the Automatically republish when changes are made option is turned on under the Published content and settings section\n\n\n\n\nConfirm your choices in the popup message clicking on OK. This will provide more details about your Google spreadsheet, copy the url.\n\n\n\n\nGo to microreact.org, and select Upload from the main menu.\n\n\n\nSelect the plus icon at the bottom right, and click on Add URLs\n\n\n\nPaste the url from step f and select Data (CSV or TSV) under File kind. Finally click on CONTINUE to see your microreact.\n\n\n\nYou should get a screen like this!\n\n\n \n&lt;&lt;&lt; Go back to Manual Contents Page"
  },
  {
    "objectID": "Day3/GenEpi_Surveillance.html",
    "href": "Day3/GenEpi_Surveillance.html",
    "title": "Genomic Epidemiology for Pathogen Surveillance",
    "section": "",
    "text": "This exercise used tools developed by The Centre for Genomic Pathogen Surveillance (CGPS) at University of Oxford\nSimilar exercises have been developed by Monica Abrudan as part of T3Connect https://wcscourses.github.io/T3connectResources/\n\nFor Microreact documentation, go to https://docs.microreact.org/\n\n\nIn this series of exercises, you will use Microreact to gain knowledge regarding a large collection of Klebsiella pneumoniae isolates, sampled from Colombia, between 2013 and 2019. At the end of this series of exercises, you will be able to identify a high risk clone circulating in the country.\n\n\n\nTask 1: Create an editable project.\nTask 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.¬†\nTask 3: What are the dominating sequence types (STs) in Colombia?\nTask 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\nTask 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nThis is a screenshot of a Microreact project called ‚ÄúKlebsiella pneumoniae Colombia‚Äù. Notice the main panels in this project: the map, the tree, the timeline and the saved views."
  },
  {
    "objectID": "Day3/GenEpi_Surveillance.html#section",
    "href": "Day3/GenEpi_Surveillance.html#section",
    "title": "Genomic Epidemiology for Pathogen Surveillance",
    "section": "",
    "text": "This exercise used tools developed by The Centre for Genomic Pathogen Surveillance (CGPS) at University of Oxford\nSimilar exercises have been developed by Monica Abrudan as part of T3Connect https://wcscourses.github.io/T3connectResources/\n\nFor Microreact documentation, go to https://docs.microreact.org/\n\n\nIn this series of exercises, you will use Microreact to gain knowledge regarding a large collection of Klebsiella pneumoniae isolates, sampled from Colombia, between 2013 and 2019. At the end of this series of exercises, you will be able to identify a high risk clone circulating in the country.\n\n\n\nTask 1: Create an editable project.\nTask 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.¬†\nTask 3: What are the dominating sequence types (STs) in Colombia?\nTask 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\nTask 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nThis is a screenshot of a Microreact project called ‚ÄúKlebsiella pneumoniae Colombia‚Äù. Notice the main panels in this project: the map, the tree, the timeline and the saved views."
  },
  {
    "objectID": "Day3/GenEpi_Surveillance.html#task-1-create-an-editable-project.",
    "href": "Day3/GenEpi_Surveillance.html#task-1-create-an-editable-project.",
    "title": "Genomic Epidemiology for Pathogen Surveillance",
    "section": "Task 1: Create an editable project.",
    "text": "Task 1: Create an editable project.\n\n\n\nTask 1: Create an editable project. Step 1: Notice the crossed out pen on right top corner of your screen. This indicates that you cannot edit the current Microreact project.\n\n\n\n\n\nTask 1: Create an editable project. Step 2: Click the crossed out ‚ÄúPen‚Äù symbol in the top right of the screen. A window appears asking you to ‚ÄúSIGN IN TO EDIT‚Äù.\n\n\n\n\n\nTask 1: Create an editable project. Step 3: Once you sign in, the message changes, and you are invited to ‚ÄúMAKE A COPY‚Äù of the current project. Make a copy, to proceed.\n\n\n\n\n\nTask 1: Create an editable project. Step 4: Notice that, once you made a copy of the project, the crossed out ‚Äúpen‚Äù symbol will change to a ‚Äúnormal pen‚Äù, and you will be able to edit and save the project."
  },
  {
    "objectID": "Day3/GenEpi_Surveillance.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "href": "Day3/GenEpi_Surveillance.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "title": "Genomic Epidemiology for Pathogen Surveillance",
    "section": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.",
    "text": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 1: Select the ‚ÄúKpn Colombia‚Äù view. Click on the ‚ÄúPen‚Äù symbol on the top right menu. Click on the ‚ÄúCreate New Chart‚Äù\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 2: Drag the new chart to overlap with the tree.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 3: An empty panel for the new chart will be shown on top of the tree panel.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 4: In the Chart Type dropdown list select ‚ÄúBar Chart‚Äù.\n\n\n\n\n\nTask2: Present WGS QC statistics in a chart. Step 5: A new window appears. In the X Axis Column, select ‚ÄúWGS_QC_no_contigs‚Äù and for ‚ÄúMaximum number of bins‚Äù select 10.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 6: The bar chart will look like above. Observe that most genomes have less the 100 contigs.\n\n\n\nTask 3: What are the dominating sequence types (STs) in Colombia?\n\n\n\nTask 3: What are the dominating STs in Colombia? Now that you‚Äôve created one chart, you can create another one! Step 1: Go to the ‚ÄúPen: symbol on the right hand side and click on the‚ÄùCreate New Chart‚Äù.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 2: The new chart can stay right on top of the previously created one.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 3: Notice a white canvas on top of the previously generated chart.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 4: Once again, from the Chart Type dropdown menu, select ‚ÄúBar Chart‚Äù, and when the new view shows up on the ‚ÄúX Axis Column‚Äù, select ST.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 5: A new chart will appear. The labels on the x-axis appear squished and they are hard to read. Drag the panel divider on the left hand side of the chart, to increase the width of the panel.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 6: The information on the x-axis should be readable now. The 3 most abundant STs are ST11, ST258 and ST512.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 7: Click the ‚ÄúViews‚Äù panel on the left hand side, hover over ‚ÄúKpn Colombia‚Äù, click on the three dots on the corner of the view and hit ‚ÄúUpdate View‚Äù\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 8: Go to the Save icon on the right corner, press the icon and choose ‚ÄúUpdate This Project‚Äù\n\n\n\n\nTask 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 1: Press the icon on the top right of the tree panel.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 2: In the Metadata blocks dropdown list, tick all boxes containing KPC, NDM, VIM, OXA and CTX-M-15 genes.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 3: The tree panel will show metadata columns. Yellow indicates presence of a certain gene and green indicates absence. In the panel shown here, it appears that the most common genes present are KPC-2, KPC-3, NDM-1 and CTX-M-15.\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 1: Go to the ‚ÄúMetadata blocks‚Äù and check the ST box.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 2: Observe the new metadata column next to the tree, with the header ‚ÄúST‚Äù.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? In the tree with added metadata blocks, we can observe a large brown block of isolates belonging to ST258. You will need to check the Legend on the very right of Microreact.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? On a close look, we can observe that ST258 is associated with the presence of the carbapenamase gene KPC-3.\n\n\nMicroreact demo link\nData taken from¬†a series of articles published in 2021 in the journal Clinical Infectious Diseases:\n\nGlobal collection article\n\nPhilippines article\nIndia article\nNigeria article\nColombia article"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html",
    "href": "Day3/Mapping+Phylo.html",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "",
    "text": "&lt;&lt;&lt; Go back to Manual Contents Page"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#mapping-to-reference-genome",
    "href": "Day3/Mapping+Phylo.html#mapping-to-reference-genome",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Mapping to reference genome",
    "text": "Mapping to reference genome\n\n\n\n\nThis is an image"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#sequencing-data-formats",
    "href": "Day3/Mapping+Phylo.html#sequencing-data-formats",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Sequencing data formats",
    "text": "Sequencing data formats\nRaw data directly from the sequencer may be in sequencer-specific formats, but will always be converted to fastq format for downstream analysis. fastq is a plain text format that is standard within bioinformatics."
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#fastq-format",
    "href": "Day3/Mapping+Phylo.html#fastq-format",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Fastq format",
    "text": "Fastq format\n\n\n\nfastq multiline\n\n\n\nIn a fastq file, each sequencing read is represented by 4 different rows\n\nRead ID\n\nalways begins with ‚Äò@‚Äô\nOften includes information about sequencer and location on flow cell\n\nSequence info\n\nThe DNA sequence (A, T, G, C, N) at each position in read\n\nComment Line\n\nBegins ‚Äò+‚Äô, allows for additional information to be included ‚Äì rarely used\n\nQuality scores\n\nScore for each base position in read ‚Äì in ASCII code.\n\n\n\n\n\n\nfastq explainer"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#map-to-reference---sam-format",
    "href": "Day3/Mapping+Phylo.html#map-to-reference---sam-format",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Map to reference - SAM format",
    "text": "Map to reference - SAM format\nEach FASTQ contains thousands to millions of sequencing reads\nMatching each read to the correct place in the genome (allowing for changes and errors) is computationally challenging\nWe use a ‚Äòread aligner‚Äô to map the position each read belongs to on a reference genome\n\nShort Reads - BWA, Bowtie2\nLong Reads - minimap2\n\nMapping produces a SAM file (or the compressed binary version BAM file)\nWe often use a tool called samtools to manage and view these large and complex files\n\n\n\n\nSamtool Image"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#variant-calling",
    "href": "Day3/Mapping+Phylo.html#variant-calling",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Variant Calling",
    "text": "Variant Calling\nWhen reads are aligned to a reference genome, point mutations (SNPs) between the sequencing reads and the reference can be identified. In the plot below: * Reads are in blue / green. * Changes from the reference are highlighted for each read in  red.\n\n\n\n\nBAM-pileup\n\n\n\nSome variants can be very easy to determine However, sequencing error can lead to changes that are technical artefacts\nSome variants are therefore more difficult to determine (e.g.¬†a low coverage region with very few reads)\nWhat if some reads correspond to a SNP and others do not?\nVariant callers (e.g.¬†bcftools, GATK, FreeBayes) apply statistical models to determine true variants, accounting for:\nReads counts/proportions supporting each allele\nQuality scores for base, reads, mapping, position in the read a variant occurs\nVariant callers output their analysis in a ‚Äòvariant call format‚Äô ‚Äì VCF file\n\n\n\n\n\nvcf"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#consensus-calling",
    "href": "Day3/Mapping+Phylo.html#consensus-calling",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Consensus calling",
    "text": "Consensus calling\n\nVariant calls will be filtered and assessed (generally in an automated way).\nOnce we have a filtered variant set, variants can be applied to the reference genome.\nThis generates a consensus ‚Äòpseudosequence‚Äô in FASTA format.\u000b\n\n\n \n\nNote, that one important step that can be overlooked is false negative variant calls\n\nVCF format only highlights changes from the reference (i.e.¬†positive calls)\nIf a base position has low coverage and no SNP is called, how do you know if there is a SNP or not?\nSome methods will simply assume that the site is ‚Äòreference‚Äô, without evidence to the contrary\nThis can give misleading results for some analyses (e.g.¬†phylogenetics)\nImportant to ensure all sites without support are tested, and marked as ‚ÄòN‚Äô.\n\n\n\nWe can use integrated pipelines to automate parts of these processes\n\n\n\n\nworkflow-tools"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#finding-the-data",
    "href": "Day3/Mapping+Phylo.html#finding-the-data",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Finding the data",
    "text": "Finding the data\nNavigate to the Module_4_Mapping_Phylogeny directory\ncd Module_4_Mapping_Phylogeny\nWe can confirm where we are\npwd\nWe can also examine the contents of this file\nls -l\n\n\n\ndirectory_contents\n\n\n\nThe folder contains:\n\ntwo pairs of sequencing reads :\n\nnew-sample-1_1.fastq.gz, new-sample-1_2.fastq.gz\nnew-sample-2_1.fastq.gz, new-sample-2_2.fastq.gz\n\na folder containing indexed reference genomes\nan archive file containing previous snippy runs (snippy.runs.1.tar.gz)"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#perform-mapping-of-reads-to-reference-genome-using-snippy",
    "href": "Day3/Mapping+Phylo.html#perform-mapping-of-reads-to-reference-genome-using-snippy",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Perform mapping of reads to reference genome using snippy",
    "text": "Perform mapping of reads to reference genome using snippy\nSnippy is an integrated pipeline that maps reads to a reference genome and produces a range of outputs. For the purposes of this tutorial, we will use snippy to organise analyse our genomes.\nYou can view the options for snippy using the following code:\nsnippy -h\n\n\n\nsnippy.h\n\n\n\nNow run snippy on the pair of fastqs (for this exercise we‚Äôll call this ‚Äònew-sample-1‚Äô)\nsnippy --outdir new-sample-1 --R1 new-sample-1_1.fastq.gz --R2 new-sample-1_2.fastq.gz --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa --cpus 4 --ram 4 --force --quiet\nWait for snippy to finish\n\n\n\nsnippy.run1\n\n\nExamine results of snippy\nls -lh new-sample-1/\n\n\n\nsnippy.run1.output\n\n\nYou can examine the log file to see exactly what snippy has done\nless new-sample-1/snps.log\n\n\nExamine snippy logs\nWe‚Äôll use grep to retrieve the relevant lines for each command from the log file.\nFirst, here is the command we used to set snippy running\ngrep \"outdir\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep1\n\n\nAfter ensuring the reference genome is indexed, snippy maps the reads to the reference genome using bwa mem\ngrep \"bwa mem\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep2\n\n\nWithin that command, snippy also marks duplicate sequencing reads using samtools markdup (we extract only the first part of that command here, but you can look for it with less)\ngrep \"COMMAND: samtools\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep3\n\n\nSnippy then uses FreeBayes to call variants against the reference genome, producing a variant call file (.vcf)\ngrep \"freebayes\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep4\n\n\nSnippy then applies some filters to assess the quality of those variants. It then applies the high quality variants to the reference genome to create a ‚Äòpseudosequence consensus‚Äô representation of our new genome\ngrep \"bcftools consensus\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep5\n\n\nSnippy creates two versions of the pseudosequence consensus: - snps.consensus.fa contains all high quality variants - snps.consensus.subs.fa contains only high quality SNPs (no INDELS)\n\n\n\nExamine snippy outputs\nThe bam file contains all the mapping positions on the genome for each individual read, along with metrics around mapping quality. This is a binary (machine readable) file, but we can view it using samtools view\nsamtools view new-sample-1/snps.bam | head -2\n\n\n\nsnippy.run1.samtools-view\n\n\nThe VCF file contains all the variants that have been called in our new genome compared to the reference genome\nhead -35 new-sample-1/snps.vcf \n\n\n\nsnippy.run1.vcf\n\n\nThe first ~28 lines here are ‚Äòheaders‚Äô and contain information about what has been done to call the variants, and helps you to interpret what different columns mean.\nThe last ~7 lines are individual variants, one per line. Variants columns are labelled\n#CHROM POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  new-sample\nWe can view a slightly easier summary of these variants in the snps.tab file\nhead -5 new-sample-1/snps.tab\n\n\n\nsnippy.run1.snps\n\n\nIn this file, we have not provided gene information, so only the first 6 columns are relevant\nSnippy has also created a fasta file with our pseudosequence genome\nhead new-sample-1/snps.consensus.fa\n\n\n\nsnippy.run1.consensus.fa"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#map-a-second-genome-to-reference",
    "href": "Day3/Mapping+Phylo.html#map-a-second-genome-to-reference",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Map a second genome to reference",
    "text": "Map a second genome to reference\nWe now need to map the second genome (new-sample-2) to our reference\nAdapt the code examples provided above to run snippy on the second set of fastq files\nExamine the results of snippy for new-sample-2 using grep, ls and less"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#use-an-existing-assembly-as-input-to-snippy",
    "href": "Day3/Mapping+Phylo.html#use-an-existing-assembly-as-input-to-snippy",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Use an existing assembly as input to snippy",
    "text": "Use an existing assembly as input to snippy\nSometimes we only have assemblies available, or we want to integrate long-read data into our analysis of short-read genomes. Snippy allows us to do this using the --ctgs argument. This takes an existing assembly file and simulates short reads, before calling variants against the reference.\nWe are going to integrate assemblies from a genome called ‚ÄúCTMA_1441‚Äù into our mapping analysis analysis. This genome was published in 2020 (https://pubmed.ncbi.nlm.nih.gov/32586863/), and used both short and long reads. For the purposes of this exercise, we have assembled it using three different approaches:\n\nShort read assembly (SPAdes) from Illumina reads\nLong read assembly (Dragonflye) from Oxford Nanopore reads\nHybrid assembly from long reads (Dragonflye), followed by short read polishing\n\nNote that we will explore the differences between these assembly methods more in a later module.\n\n\nLocate existing assemblies\nThe reads are hosted in the course GitHub repository. If that has been updated on your local VM, the files should be available to access. Let‚Äôs check:\nls -lh ~/github_repository/course_data_2025/mapping_and_phylo/\n\n\n\ngithub.repo.ls\n\n\nIf the files are missing, check with the instructors.\n\nAssuming they are available, we can copy the assemblies to our current working directory to make things easier:\ncp ~/github_repository/course_data_2025/mapping_and_phylo/CTMA_1441.* .\n\n\n\nMap an existing short-read assembly\nThe short read assembly is called CTMA_1441.unicycler-short.fasta. We can add the short-read assembly to snippy like this:\nsnippy --outdir CTMA_1441.short --ctgs CTMA_1441.unicycler-short.fasta --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa --cpus 4 --ram 4 --force --quiet\n\n\n\n\nsnippy_assembly_example\n\n\n\n\n\nMap a long-read assembly to reference\nWe now need to map the two long read assemblies to our analysis: - CTMA_1441.filtreads.dragonflye.long_4.1_polish.contigs.fa - CTMA_1441.filtreads.dragonflye.long_4.1_nopolish.contigs.fa\n\nAdapt the code examples provided above to run snippy on the long-read assemblies using the output directories CTMA_1441.long_polish and CTMA_1441.long_nopolish.\nExamine the results of snippy for each assembly using grep, ls and less.\n\nWhat do you notice about the number of variable sites (reported at the end of the snippy run) identified with the different assemblies?"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#contextualise-new-genomes-with-a-collection",
    "href": "Day3/Mapping+Phylo.html#contextualise-new-genomes-with-a-collection",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Contextualise new genomes with a collection",
    "text": "Contextualise new genomes with a collection\nNow lets look at this new genome in context with a collection.\nWe can add this genome to a collection of genomes mapped using snippy-core.\nFor this exercise, we will again retrieve our data from the github folder:\ncp ~/github_repository/course_data_2025/mapping_and_phylo/old.snippy.runs_2025.tar.gz .\nCheck the files are there:\nls -lh\n\n\n\nsnippy-context.copy-git.ls\n\n\n(Note that there is an existing file in our directory called `old.snippy.runs.2023.tar.gz - we will not be using this).\n\nFirst, let‚Äôs extract the old snippy runs from our archive file\ntar -zxf old.snippy.runs_2025.tar.gz\nWe can see that we now have a new directory old.snippy.runs_2024\nls -lh\n\n\n\nsnippy-context.tar.ls1\n\n\nls -lh old.snippy.runs_2024\n\n\n\nsnippy-context.tar.ls2\n\n\n\nNow lets use snippy-core to summarise all these genomes along with the new ones and create a multiple sequence alignment\nsnippy-core --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa old.snippy.runs_2024/* new-sample-1 new-sample-2 CTMA_1441.short CTMA_1441.long_polish CTMA_1441.long_nopolish\n\n\n\nsnippy-core.run1\n\n\nSnippy has now created a number of files, including a ‚Äòcore SNP alignment‚Äô\nls -l core.*\n\n\n\nsnippy-core.run2\n\n\nWe have various files that summarise our variants, e.g.\nhead core.tab \n\n\n\nsnippy-core.run3\n\n\nAnd our multiple sequence alignment containing all genomes:\nhead core.full.aln\n\n\n\nsnippy-core.run4\n\n\nThis file masks sequences with low confidence in different ways, but for some applications we want everything masked in the same way. Let‚Äôs change that so anything uncertain is marked as ‚ÄôN‚Äô using the snippy-clean_full_aln script that comes with snippy.\nsnippy-clean_full_aln core.full.aln &gt; clean.full.aln\n\n\n\nsnippy-core.run5"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#phylogenetics",
    "href": "Day3/Mapping+Phylo.html#phylogenetics",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Phylogenetics",
    "text": "Phylogenetics\nNow that we have a clean multiple sequence alignment, we are now going to use IQ-TREE to build a maximum likelihood phylogeny.\n\n\nMake a SNP-only alignment using snp-sites\nCalculating a phylogeny on whole genome sequences can be very time consuming. We can speed this up by only using the variable sites (SNPs). However, we need to be aware that only including variable sites can affect the evolutionary rate estimates made by phylogenetics software - therefore, we need to account for the sites we remove in our analysis.\nWe will use snp-sites to do this. You can view the options for snp-sites\nsnp-sites\n\n\n\nsnp-sites.1\n\n\nFirst, remove all the invariant sites and create a SNP-only multiple sequence alignment.\nsnp-sites -o clean.full.SNPs.aln clean.full.aln\nWe can also use snp-sites to find out how many invariant sites were removed (and what proportion of A, T, G, C they were). Use snp-sites -C to do this for the file above.\n\n\n\nsnp-sites.2\n\n\n\n\n\nCalculate a phylogenetic tree from the SNPs using IQ-TREE2\nWe can look at the options for IQ-TREE below\niqtree -h\n\n\n\niqtree.1\n\n\n\nIn the command below, we: - specify the multiple sequence alignment using -s clean.full.SNPs.aln - ask IQ-TREE to take account of missing invariant sites using -fconst $(snp-sites -C clean.full.aln) - specify an evolutionary model we want IQ-TREE to use -m GTR+F+I - tell IQ-TREE to use a maximum of 2 CPUs (threads) and 2GB memory -T 2 -mem 2G - perform 1000 ultrafast bootstraps -B 1000 - use sample M66 as an outgroup -o M66\niqtree -s clean.full.SNPs.aln -fconst $( snp-sites -C clean.full.aln ) -m GTR+F+I -T 2 -mem 2G -B 1000 -o M66\n\n\n\niqtree.2\n\n\n\nLook at folder\nls -lh clean.full.*\n\n\n\niqtree.3\n\n\n\nOur maximum likelihood tree is labelled clean.full.SNPs.aln.treefile. The treefile suffix is not always correctly identified by many tools, so we‚Äôll relabel this as something else:\ncp clean.full.SNPs.aln.treefile clean.full.SNPs.aln.tre\n\n\n\nVisualise a phylogenetic tree\nTo examine our tree, we can look at the text file:\ncat clean.full.SNPs.aln.tre\n\n\n\niqtree.4\n\n\nBut this is not very helpful - it‚Äôs just raw text in ‚Äònewick‚Äô format.\nInstead, we can visualise this using figtree\nfigtree clean.full.SNPs.aln.tre &\n(Note: Remember to include the & symbol at the end - this allows figtree to run in the background, and allows you to still use the command prompt)\n\n\n\n\nfigtree.1\n\n\nIgnore the java error messages. The popup box is asking you how to describe the ‚Äòbootstrap values‚Äô. You can click ‚ÄòOK‚Äô here.\nYou should now have a visualisation of the tree we just generated\n\n\n\nfigtree.2\n\n\n\nExamine the tree: - How are new-sample-1 and new-sample-2 related to each other? - What do you notice about the different versions of the assembled genomes CTMA_1441? - What does this tell you about the different assembly methods?"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#adjusting-a-dataset-to-remove-a-poor-quality-genome",
    "href": "Day3/Mapping+Phylo.html#adjusting-a-dataset-to-remove-a-poor-quality-genome",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Adjusting a dataset to remove a poor quality genome",
    "text": "Adjusting a dataset to remove a poor quality genome\nThe unpolished long-read assembly CTMA_1441.long_nopolish is filled with uncorrected errors. We therefore should not use it in our analysis. We will also remove the duplicate examples of CTMA_1441, so we are only using a single example. We can rerun snippy-core to rebuild the alignment without the unnecessary samples:\nsnippy-core --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa old.snippy.runs_2024/* new-sample-1 new-sample-2\nsnippy-clean_full_aln core.full.aln &gt; clean2.full.aln\n\nAnd now build a new phylogeny:\n\nRun snp-sites to create a file called clean2.full.SNPs.aln\nRun iqtree to calculate a phylogeny for clean2.full.SNPs.aln\nRename the final treefile to clean2.full.SNPs.aln.tre\n\n\nYou can recheck the tree using figtree."
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#tree-visualisation-using-microreact",
    "href": "Day3/Mapping+Phylo.html#tree-visualisation-using-microreact",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Tree Visualisation using Microreact",
    "text": "Tree Visualisation using Microreact\nWe can also visualise our tree using a webtool called Microreact\n\nOpen Firefox and navigate to https://microreact.org/\nClick ‚Äòupload‚Äô\n\n\n\n\nmicroreact.1\n\n\n\n\nSelect the file upload, or drag the tree file clean2.full.SNPs.aln.tre onto the upload screen\n\n\n\n\nmicroreact.2\n\n\n\nMicroreact may not recognise the file, so let‚Äôs tell micoreact this this is a tree in newick format. Note that we can also use this screen to add metadata in csv format.\n\n\n\nmicroreact.3\n\n\n\nClick continue to view the tree\n\n\n\nmicroreact.4"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#accounting-for-recombination-with-gubbins",
    "href": "Day3/Mapping+Phylo.html#accounting-for-recombination-with-gubbins",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Accounting for recombination with gubbins",
    "text": "Accounting for recombination with gubbins\nWe can use gubbins to infer recombining sites by looking for increased SNP density that occurs in specific ancestral nodes\ngubbins has been installed using conda. We must activate the relavent conda environment before running any commands using gubbins.\n\nconda activate gubbins-env\n\nNow we can run gubbins\nrun_gubbins.py -h\n\n\n\ngubbins.1\n\n\n\nThe following command runs gubbins on standard settings, with 4 CPUs.\nNote: the -c option tells the program to use 4 CPUs. Note: the -p option tells the program to name all files with the prefix gubbins This command can take a few minutes to run.\nrun_gubbins.py -c 4 -p gubbins clean2.full.aln\n\n\n\nrun_gubbins_hanging\n\n\n\n\n\n\nrun_gubbins_hanging\n\n\n\nNote: gubbins can take a long time to run on some computers. If gubbins takes more than 10 mins to complete, we have already run it for you - the files are available in the GitHub folder at ~/github_repository/Modules/Mapping_and_Phylo/gubbins_backup_2024.tar.gz.\nls -lh ~/github_repository/course_data_2025/mapping_and_phylo/\ncp ~/github_repository/course_data_2025/mapping_and_phylo/gubbins_backup_2024.tar.gz .\ntar -zxf gubbins_backup_2024.tar.gz\nNote: You may notice there is a directory called gubbins_backups - this is from a previous year, and will not be used today.\n \nLets look at what gubbins has done\nls -l gubbins.*\n\n\n\ngubbins.backup.files\n\n\n\nYou can explore these files For example gubbins.recombination_predictions.gff is a gff file that contains a record of each recombination block identified, how many SNPs it contains, and what samples are affected.\nhead gubbins.recombination_predictions.gff\n\n\n\ngubbins.3\n\n\n\ngubbins.filtered_polymorphic_sites.fasta is a fasta file containing only SNPs, where recombining sites have been ‚Äòmasked to N‚Äô.\nhead gubbins.filtered_polymorphic_sites.fasta\n\n\n\ngubbins.4\n\n\n\ngubbins.final_tree.tre is a phylogeny in which recombination has already been accounted for.\nYou can visualise this in figtree or microreact as above.\n\n\nRoot a phylogeny\nThe trees outputted by iqtree and gubbins are unrooted, but we may want to apply some evolutionary direction to them. One approach that is commonly used for bacterial datasets is midpoint rooting. Midpoint rooting involves locating the midpoint of the longest path between any two tips and putting the root in that location. Note that this does not necessarily infer the true root, and this should be used with caution.\nTo midpoint root our tree, we will use a simple script written in python that uses the ete package. You can examine the code:\nless midpoint.root.py\nTo make the code work in the VM, we need to load another conda environment:\nconda activate ete3-env\n\n\n\nmidpoint.script\n\n\n\nWe can use this script to midpoint root our tree\npython midpoint.root.py gubbins.final_tree.tre &gt; gubbins.final_tree.midpoint.tre\nYou can visualise this in figtree or microreact as above. How does it compare to the unrooted version?\n \n\n\nVisualising recombination blocks using phandango\nWe can also visualise the tree together with the recombination blocks inferred by gubbins using a webtool called phandango\n\nUsing your browser, navigate to (https://jameshadfield.github.io/phandango/#/)\n\n\n\nphandango.1\n\n\n\nOn your VM desktop, go to the file manager, and navigate to /home/manager/Module_4_Mapping_Phylogeny\nDrag and drop the tree file gubbins.final_tree.midpoint.tre into the phandango browser window\nDrag and drop the recombination gff file gubbins.recombination_predictions.gff into the phandango browser window\nPhandango should automatically display blocks of recombination in  red  (ancestral) and  blue (specific to a sample)\n\n\n\nphandango.1"
  },
  {
    "objectID": "Day3/Mapping+Phylo.html#clustering-genomes-using-fastbaps",
    "href": "Day3/Mapping+Phylo.html#clustering-genomes-using-fastbaps",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Clustering genomes using fastBAPS",
    "text": "Clustering genomes using fastBAPS\nWe can cluster genomes for epidemiology in a variety of ways, depending on the goal and genetic distances involved. Here will use fastBAPS, which is an optimised implementation of the original hierBAPS algorithm for hierarchical partitioning and Bayesian clustering of genomes. fastBAPS can be run in R as well as from the command line.\nIn the command below, we: * specify a set of SNPs/alleles for use by the model. * -i gubbins.filtered_polymorphic_sites.fasta * specify the name of the output file. * -o fastbaps.clusters.csv * specify that we want hierarchical clustering at two different levels (i.e.¬†BAPS will attempt to subdivide ‚Äúlevel 1 clusters‚Äù to form ‚Äúlevel 2 clusters‚Äù). * --levels 2 * specify a model for delineating genomes - we will use a relaxed clustering model which should maximise the number of clusters we get at level 2. * -p optimise.baps * specify an existing phylogenetic tree that we want to use for conditioning the statistical model. This option is not essential, but it ensures that the tree and the clusters are consistent with one another. * --phylogeny gubbins.final_tree.tre\nrun_fastbaps -i gubbins.filtered_polymorphic_sites.fasta -o fastbaps.clusters --levels 2 -p optimise.baps --phylogeny gubbins.final_tree.tre\nWe can view the output using head or cat\n\n\n\nfastbaps.command\n\n\n\nNow that we have clusters, let‚Äôs see how they look against the phylogenetic tree we made earlier.\n\nFirst, we‚Äôll need to make the csv file with our clusters compatible with microreact. We need to change the header of the first column from Isolates to ID. We‚Äôll use some simple code on the command line to do this:\nsed s/Isolates/ID/ fastbaps.clusters &gt; fastbaps.clusters.fixid.csv\nhead fastbaps.clusters.fixid.csv\n\n\n\nfastbaps.fixid.code\n\n\n \nGo to the microreact webpage, and try uploading the new gubbins filtered tree and the fastbaps clusters\n\n\n\nmicroreact.fastbaps.upload\n\n\n\n\n\n\nmicroreact.fastbaps.upload2\n\n\nYou can see that the first column has been correctly identified as the ‚ÄòID‚Äô - this will be linked to the taxa in the tree\nClick Continue\nYou should now see a tree coloured by your fastbaps groups. You can use the toggles to change the colouring and to add metadata blocks.\n\n\n\nmicroreact.fastbaps.final\n\n\n\n \nEND\n&lt;&lt;&lt; Go back to Manual Contents Page"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html",
    "href": "Day3/images/Mapping+Phylo.html",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "",
    "text": "&lt;&lt;&lt; Go back to Manual Contents Page"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#mapping-to-reference-genome",
    "href": "Day3/images/Mapping+Phylo.html#mapping-to-reference-genome",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Mapping to reference genome",
    "text": "Mapping to reference genome\n\n\n\n\nThis is an image"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#sequencing-data-formats",
    "href": "Day3/images/Mapping+Phylo.html#sequencing-data-formats",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Sequencing data formats",
    "text": "Sequencing data formats\nRaw data directly from the sequencer may be in sequencer-specific formats, but will always be converted to fastq format for downstream analysis. fastq is a plain text format that is standard within bioinformatics."
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#fastq-format",
    "href": "Day3/images/Mapping+Phylo.html#fastq-format",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Fastq format",
    "text": "Fastq format\n\n\n\nfastq multiline\n\n\n\nIn a fastq file, each sequencing read is represented by 4 different rows\n\nRead ID\n\nalways begins with ‚Äò@‚Äô\nOften includes information about sequencer and location on flow cell\n\nSequence info\n\nThe DNA sequence (A, T, G, C, N) at each position in read\n\nComment Line\n\nBegins ‚Äò+‚Äô, allows for additional information to be included ‚Äì rarely used\n\nQuality scores\n\nScore for each base position in read ‚Äì in ASCII code.\n\n\n\n\n\n\nfastq explainer"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#map-to-reference---sam-format",
    "href": "Day3/images/Mapping+Phylo.html#map-to-reference---sam-format",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Map to reference - SAM format",
    "text": "Map to reference - SAM format\nEach FASTQ contains thousands to millions of sequencing reads\nMatching each read to the correct place in the genome (allowing for changes and errors) is computationally challenging\nWe use a ‚Äòread aligner‚Äô to map the position each read belongs to on a reference genome\n\nShort Reads - BWA, Bowtie2\nLong Reads - minimap2\n\nMapping produces a SAM file (or the compressed binary version BAM file)\nWe often use a tool called samtools to manage and view these large and complex files\n\n\n\n\nSamtool Image"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#variant-calling",
    "href": "Day3/images/Mapping+Phylo.html#variant-calling",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Variant Calling",
    "text": "Variant Calling\nWhen reads are aligned to a reference genome, point mutations (SNPs) between the sequencing reads and the reference can be identified. In the plot below: * Reads are in blue / green. * Changes from the reference are highlighted for each read in  red.\n\n\n\n\nBAM-pileup\n\n\n\nSome variants can be very easy to determine However, sequencing error can lead to changes that are technical artefacts\nSome variants are therefore more difficult to determine (e.g.¬†a low coverage region with very few reads)\nWhat if some reads correspond to a SNP and others do not?\nVariant callers (e.g.¬†bcftools, GATK, FreeBayes) apply statistical models to determine true variants, accounting for:\nReads counts/proportions supporting each allele\nQuality scores for base, reads, mapping, position in the read a variant occurs\nVariant callers output their analysis in a ‚Äòvariant call format‚Äô ‚Äì VCF file\n\n\n\n\n\nvcf"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#consensus-calling",
    "href": "Day3/images/Mapping+Phylo.html#consensus-calling",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Consensus calling",
    "text": "Consensus calling\n\nVariant calls will be filtered and assessed (generally in an automated way).\nOnce we have a filtered variant set, variants can be applied to the reference genome.\nThis generates a consensus ‚Äòpseudosequence‚Äô in FASTA format.\u000b\n\n\n \n\nNote, that one important step that can be overlooked is false negative variant calls\n\nVCF format only highlights changes from the reference (i.e.¬†positive calls)\nIf a base position has low coverage and no SNP is called, how do you know if there is a SNP or not?\nSome methods will simply assume that the site is ‚Äòreference‚Äô, without evidence to the contrary\nThis can give misleading results for some analyses (e.g.¬†phylogenetics)\nImportant to ensure all sites without support are tested, and marked as ‚ÄòN‚Äô.\n\n\n\nWe can use integrated pipelines to automate parts of these processes\n\n\n\n\nworkflow-tools"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#finding-the-data",
    "href": "Day3/images/Mapping+Phylo.html#finding-the-data",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Finding the data",
    "text": "Finding the data\nNavigate to the Module_4_Mapping_Phylogeny directory\ncd Module_4_Mapping_Phylogeny\nWe can confirm where we are\npwd\nWe can also examine the contents of this file\nls -l\n\n\n\ndirectory_contents\n\n\n\nThe folder contains:\n\ntwo pairs of sequencing reads :\n\nnew-sample-1_1.fastq.gz, new-sample-1_2.fastq.gz\nnew-sample-2_1.fastq.gz, new-sample-2_2.fastq.gz\n\na folder containing indexed reference genomes\nan archive file containing previous snippy runs (snippy.runs.1.tar.gz)"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#perform-mapping-of-reads-to-reference-genome-using-snippy",
    "href": "Day3/images/Mapping+Phylo.html#perform-mapping-of-reads-to-reference-genome-using-snippy",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Perform mapping of reads to reference genome using snippy",
    "text": "Perform mapping of reads to reference genome using snippy\nSnippy is an integrated pipeline that maps reads to a reference genome and produces a range of outputs. For the purposes of this tutorial, we will use snippy to organise analyse our genomes.\nYou can view the options for snippy using the following code:\nsnippy -h\n\n\n\nsnippy.h\n\n\n\nNow run snippy on the pair of fastqs (for this exercise we‚Äôll call this ‚Äònew-sample-1‚Äô)\nsnippy --outdir new-sample-1 --R1 new-sample-1_1.fastq.gz --R2 new-sample-1_2.fastq.gz --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa --cpus 4 --ram 4 --force --quiet\nWait for snippy to finish\n\n\n\nsnippy.run1\n\n\nExamine results of snippy\nls -lh new-sample-1/\n\n\n\nsnippy.run1.output\n\n\nYou can examine the log file to see exactly what snippy has done\nless new-sample-1/snps.log\n\n\nExamine snippy logs\nWe‚Äôll use grep to retrieve the relevant lines for each command from the log file.\nFirst, here is the command we used to set snippy running\ngrep \"outdir\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep1\n\n\nAfter ensuring the reference genome is indexed, snippy maps the reads to the reference genome using bwa mem\ngrep \"bwa mem\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep2\n\n\nWithin that command, snippy also marks duplicate sequencing reads using samtools markdup (we extract only the first part of that command here, but you can look for it with less)\ngrep \"COMMAND: samtools\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep3\n\n\nSnippy then uses FreeBayes to call variants against the reference genome, producing a variant call file (.vcf)\ngrep \"freebayes\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep4\n\n\nSnippy then applies some filters to assess the quality of those variants. It then applies the high quality variants to the reference genome to create a ‚Äòpseudosequence consensus‚Äô representation of our new genome\ngrep \"bcftools consensus\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep5\n\n\nSnippy creates two versions of the pseudosequence consensus: - snps.consensus.fa contains all high quality variants - snps.consensus.subs.fa contains only high quality SNPs (no INDELS)\n\n\n\nExamine snippy outputs\nThe bam file contains all the mapping positions on the genome for each individual read, along with metrics around mapping quality. This is a binary (machine readable) file, but we can view it using samtools view\nsamtools view new-sample-1/snps.bam | head -2\n\n\n\nsnippy.run1.samtools-view\n\n\nThe VCF file contains all the variants that have been called in our new genome compared to the reference genome\nhead -35 new-sample-1/snps.vcf \n\n\n\nsnippy.run1.vcf\n\n\nThe first ~28 lines here are ‚Äòheaders‚Äô and contain information about what has been done to call the variants, and helps you to interpret what different columns mean.\nThe last ~7 lines are individual variants, one per line. Variants columns are labelled\n#CHROM POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  new-sample\nWe can view a slightly easier summary of these variants in the snps.tab file\nhead -5 new-sample-1/snps.tab\n\n\n\nsnippy.run1.snps\n\n\nIn this file, we have not provided gene information, so only the first 6 columns are relevant\nSnippy has also created a fasta file with our pseudosequence genome\nhead new-sample-1/snps.consensus.fa\n\n\n\nsnippy.run1.consensus.fa"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#map-a-second-genome-to-reference",
    "href": "Day3/images/Mapping+Phylo.html#map-a-second-genome-to-reference",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Map a second genome to reference",
    "text": "Map a second genome to reference\nWe now need to map the second genome (new-sample-2) to our reference\nAdapt the code examples provided above to run snippy on the second set of fastq files\nExamine the results of snippy for new-sample-2 using grep, ls and less"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#use-an-existing-assembly-as-input-to-snippy",
    "href": "Day3/images/Mapping+Phylo.html#use-an-existing-assembly-as-input-to-snippy",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Use an existing assembly as input to snippy",
    "text": "Use an existing assembly as input to snippy\nSometimes we only have assemblies available, or we want to integrate long-read data into our analysis of short-read genomes. Snippy allows us to do this using the --ctgs argument. This takes an existing assembly file and simulates short reads, before calling variants against the reference.\nWe are going to integrate assemblies from a genome called ‚ÄúCTMA_1441‚Äù into our mapping analysis analysis. This genome was published in 2020 (https://pubmed.ncbi.nlm.nih.gov/32586863/), and used both short and long reads. For the purposes of this exercise, we have assembled it using three different approaches:\n\nShort read assembly (SPAdes) from Illumina reads\nLong read assembly (Dragonflye) from Oxford Nanopore reads\nHybrid assembly from long reads (Dragonflye), followed by short read polishing\n\nNote that we will explore the differences between these assembly methods more in a later module.\n\n\nLocate existing assemblies\nThe reads are hosted in the course GitHub repository. If that has been updated on your local VM, the files should be available to access. Let‚Äôs check:\nls -lh ~/github_repository/course_data_2025/mapping_and_phylo/\n\n\n\ngithub.repo.ls\n\n\nIf the files are missing, check with the instructors.\n\nAssuming they are available, we can copy the assemblies to our current working directory to make things easier:\ncp ~/github_repository/course_data_2025/mapping_and_phylo/CTMA_1441.* .\n\n\n\nMap an existing short-read assembly\nThe short read assembly is called CTMA_1441.unicycler-short.fasta. We can add the short-read assembly to snippy like this:\nsnippy --outdir CTMA_1441.short --ctgs CTMA_1441.unicycler-short.fasta --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa --cpus 4 --ram 4 --force --quiet\n\n\n\n\nsnippy_assembly_example\n\n\n\n\n\nMap a long-read assembly to reference\nWe now need to map the two long read assemblies to our analysis: - CTMA_1441.filtreads.dragonflye.long_4.1_polish.contigs.fa - CTMA_1441.filtreads.dragonflye.long_4.1_nopolish.contigs.fa\n\nAdapt the code examples provided above to run snippy on the long-read assemblies using the output directories CTMA_1441.long_polish and CTMA_1441.long_nopolish.\nExamine the results of snippy for each assembly using grep, ls and less.\n\nWhat do you notice about the number of variable sites (reported at the end of the snippy run) identified with the different assemblies?"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#contextualise-new-genomes-with-a-collection",
    "href": "Day3/images/Mapping+Phylo.html#contextualise-new-genomes-with-a-collection",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Contextualise new genomes with a collection",
    "text": "Contextualise new genomes with a collection\nNow lets look at this new genome in context with a collection.\nWe can add this genome to a collection of genomes mapped using snippy-core.\nFor this exercise, we will again retrieve our data from the github folder:\ncp ~/github_repository/course_data_2025/mapping_and_phylo/old.snippy.runs_2025.tar.gz .\nCheck the files are there:\nls -lh\n\n\n\nsnippy-context.copy-git.ls\n\n\n(Note that there is an existing file in our directory called `old.snippy.runs.2023.tar.gz - we will not be using this).\n\nFirst, let‚Äôs extract the old snippy runs from our archive file\ntar -zxf old.snippy.runs_2025.tar.gz\nWe can see that we now have a new directory old.snippy.runs_2024\nls -lh\n\n\n\nsnippy-context.tar.ls1\n\n\nls -lh old.snippy.runs_2024\n\n\n\nsnippy-context.tar.ls2\n\n\n\nNow lets use snippy-core to summarise all these genomes along with the new ones and create a multiple sequence alignment\nsnippy-core --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa old.snippy.runs_2024/* new-sample-1 new-sample-2 CTMA_1441.short CTMA_1441.long_polish CTMA_1441.long_nopolish\n\n\n\nsnippy-core.run1\n\n\nSnippy has now created a number of files, including a ‚Äòcore SNP alignment‚Äô\nls -l core.*\n\n\n\nsnippy-core.run2\n\n\nWe have various files that summarise our variants, e.g.\nhead core.tab \n\n\n\nsnippy-core.run3\n\n\nAnd our multiple sequence alignment containing all genomes:\nhead core.full.aln\n\n\n\nsnippy-core.run4\n\n\nThis file masks sequences with low confidence in different ways, but for some applications we want everything masked in the same way. Let‚Äôs change that so anything uncertain is marked as ‚ÄôN‚Äô using the snippy-clean_full_aln script that comes with snippy.\nsnippy-clean_full_aln core.full.aln &gt; clean.full.aln\n\n\n\nsnippy-core.run5"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#phylogenetics",
    "href": "Day3/images/Mapping+Phylo.html#phylogenetics",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Phylogenetics",
    "text": "Phylogenetics\nNow that we have a clean multiple sequence alignment, we are now going to use IQ-TREE to build a maximum likelihood phylogeny.\n\n\nMake a SNP-only alignment using snp-sites\nCalculating a phylogeny on whole genome sequences can be very time consuming. We can speed this up by only using the variable sites (SNPs). However, we need to be aware that only including variable sites can affect the evolutionary rate estimates made by phylogenetics software - therefore, we need to account for the sites we remove in our analysis.\nWe will use snp-sites to do this. You can view the options for snp-sites\nsnp-sites\n\n\n\nsnp-sites.1\n\n\nFirst, remove all the invariant sites and create a SNP-only multiple sequence alignment.\nsnp-sites -o clean.full.SNPs.aln clean.full.aln\nWe can also use snp-sites to find out how many invariant sites were removed (and what proportion of A, T, G, C they were). Use snp-sites -C to do this for the file above.\n\n\n\nsnp-sites.2\n\n\n\n\n\nCalculate a phylogenetic tree from the SNPs using IQ-TREE2\nWe can look at the options for IQ-TREE below\niqtree -h\n\n\n\niqtree.1\n\n\n\nIn the command below, we: - specify the multiple sequence alignment using -s clean.full.SNPs.aln - ask IQ-TREE to take account of missing invariant sites using -fconst $(snp-sites -C clean.full.aln) - specify an evolutionary model we want IQ-TREE to use -m GTR+F+I - tell IQ-TREE to use a maximum of 2 CPUs (threads) and 2GB memory -T 2 -mem 2G - perform 1000 ultrafast bootstraps -B 1000 - use sample M66 as an outgroup -o M66\niqtree -s clean.full.SNPs.aln -fconst $( snp-sites -C clean.full.aln ) -m GTR+F+I -T 2 -mem 2G -B 1000 -o M66\n\n\n\niqtree.2\n\n\n\nLook at folder\nls -lh clean.full.*\n\n\n\niqtree.3\n\n\n\nOur maximum likelihood tree is labelled clean.full.SNPs.aln.treefile. The treefile suffix is not always correctly identified by many tools, so we‚Äôll relabel this as something else:\ncp clean.full.SNPs.aln.treefile clean.full.SNPs.aln.tre\n\n\n\nVisualise a phylogenetic tree\nTo examine our tree, we can look at the text file:\ncat clean.full.SNPs.aln.tre\n\n\n\niqtree.4\n\n\nBut this is not very helpful - it‚Äôs just raw text in ‚Äònewick‚Äô format.\nInstead, we can visualise this using figtree\nfigtree clean.full.SNPs.aln.tre &\n(Note: Remember to include the & symbol at the end - this allows figtree to run in the background, and allows you to still use the command prompt)\n\n\n\n\nfigtree.1\n\n\nIgnore the java error messages. The popup box is asking you how to describe the ‚Äòbootstrap values‚Äô. You can click ‚ÄòOK‚Äô here.\nYou should now have a visualisation of the tree we just generated\n\n\n\nfigtree.2\n\n\n\nExamine the tree: - How are new-sample-1 and new-sample-2 related to each other? - What do you notice about the different versions of the assembled genomes CTMA_1441? - What does this tell you about the different assembly methods?"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#adjusting-a-dataset-to-remove-a-poor-quality-genome",
    "href": "Day3/images/Mapping+Phylo.html#adjusting-a-dataset-to-remove-a-poor-quality-genome",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Adjusting a dataset to remove a poor quality genome",
    "text": "Adjusting a dataset to remove a poor quality genome\nThe unpolished long-read assembly CTMA_1441.long_nopolish is filled with uncorrected errors. We therefore should not use it in our analysis. We will also remove the duplicate examples of CTMA_1441, so we are only using a single example. We can rerun snippy-core to rebuild the alignment without the unnecessary samples:\nsnippy-core --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa old.snippy.runs_2024/* new-sample-1 new-sample-2\nsnippy-clean_full_aln core.full.aln &gt; clean2.full.aln\n\nAnd now build a new phylogeny:\n\nRun snp-sites to create a file called clean2.full.SNPs.aln\nRun iqtree to calculate a phylogeny for clean2.full.SNPs.aln\nRename the final treefile to clean2.full.SNPs.aln.tre\n\n\nYou can recheck the tree using figtree."
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#tree-visualisation-using-microreact",
    "href": "Day3/images/Mapping+Phylo.html#tree-visualisation-using-microreact",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Tree Visualisation using Microreact",
    "text": "Tree Visualisation using Microreact\nWe can also visualise our tree using a webtool called Microreact\n\nOpen Firefox and navigate to https://microreact.org/\nClick ‚Äòupload‚Äô\n\n\n\n\nmicroreact.1\n\n\n\n\nSelect the file upload, or drag the tree file clean2.full.SNPs.aln.tre onto the upload screen\n\n\n\n\nmicroreact.2\n\n\n\nMicroreact may not recognise the file, so let‚Äôs tell micoreact this this is a tree in newick format. Note that we can also use this screen to add metadata in csv format.\n\n\n\nmicroreact.3\n\n\n\nClick continue to view the tree\n\n\n\nmicroreact.4"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#accounting-for-recombination-with-gubbins",
    "href": "Day3/images/Mapping+Phylo.html#accounting-for-recombination-with-gubbins",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Accounting for recombination with gubbins",
    "text": "Accounting for recombination with gubbins\nWe can use gubbins to infer recombining sites by looking for increased SNP density that occurs in specific ancestral nodes\ngubbins has been installed using conda. We must activate the relavent conda environment before running any commands using gubbins.\n\nconda activate gubbins-env\n\nNow we can run gubbins\nrun_gubbins.py -h\n\n\n\ngubbins.1\n\n\n\nThe following command runs gubbins on standard settings, with 4 CPUs.\nNote: the -c option tells the program to use 4 CPUs. Note: the -p option tells the program to name all files with the prefix gubbins This command can take a few minutes to run.\nrun_gubbins.py -c 4 -p gubbins clean2.full.aln\n\n\n\nrun_gubbins_hanging\n\n\n\n\n\n\nrun_gubbins_hanging\n\n\n\nNote: gubbins can take a long time to run on some computers. If gubbins takes more than 10 mins to complete, we have already run it for you - the files are available in the GitHub folder at ~/github_repository/Modules/Mapping_and_Phylo/gubbins_backup_2024.tar.gz.\nls -lh ~/github_repository/course_data_2025/mapping_and_phylo/\ncp ~/github_repository/course_data_2025/mapping_and_phylo/gubbins_backup_2024.tar.gz .\ntar -zxf gubbins_backup_2024.tar.gz\nNote: You may notice there is a directory called gubbins_backups - this is from a previous year, and will not be used today.\n \nLets look at what gubbins has done\nls -l gubbins.*\n\n\n\ngubbins.backup.files\n\n\n\nYou can explore these files For example gubbins.recombination_predictions.gff is a gff file that contains a record of each recombination block identified, how many SNPs it contains, and what samples are affected.\nhead gubbins.recombination_predictions.gff\n\n\n\ngubbins.3\n\n\n\ngubbins.filtered_polymorphic_sites.fasta is a fasta file containing only SNPs, where recombining sites have been ‚Äòmasked to N‚Äô.\nhead gubbins.filtered_polymorphic_sites.fasta\n\n\n\ngubbins.4\n\n\n\ngubbins.final_tree.tre is a phylogeny in which recombination has already been accounted for.\nYou can visualise this in figtree or microreact as above.\n\n\nRoot a phylogeny\nThe trees outputted by iqtree and gubbins are unrooted, but we may want to apply some evolutionary direction to them. One approach that is commonly used for bacterial datasets is midpoint rooting. Midpoint rooting involves locating the midpoint of the longest path between any two tips and putting the root in that location. Note that this does not necessarily infer the true root, and this should be used with caution.\nTo midpoint root our tree, we will use a simple script written in python that uses the ete package. You can examine the code:\nless midpoint.root.py\nTo make the code work in the VM, we need to load another conda environment:\nconda activate ete3-env\n\n\n\nmidpoint.script\n\n\n\nWe can use this script to midpoint root our tree\npython midpoint.root.py gubbins.final_tree.tre &gt; gubbins.final_tree.midpoint.tre\nYou can visualise this in figtree or microreact as above. How does it compare to the unrooted version?\n \n\n\nVisualising recombination blocks using phandango\nWe can also visualise the tree together with the recombination blocks inferred by gubbins using a webtool called phandango\n\nUsing your browser, navigate to (https://jameshadfield.github.io/phandango/#/)\n\n\n\nphandango.1\n\n\n\nOn your VM desktop, go to the file manager, and navigate to /home/manager/Module_4_Mapping_Phylogeny\nDrag and drop the tree file gubbins.final_tree.midpoint.tre into the phandango browser window\nDrag and drop the recombination gff file gubbins.recombination_predictions.gff into the phandango browser window\nPhandango should automatically display blocks of recombination in  red  (ancestral) and  blue (specific to a sample)\n\n\n\nphandango.1"
  },
  {
    "objectID": "Day3/images/Mapping+Phylo.html#clustering-genomes-using-fastbaps",
    "href": "Day3/images/Mapping+Phylo.html#clustering-genomes-using-fastbaps",
    "title": "Mapping and Phylogenetics - Costa Rica 2025 ",
    "section": "Clustering genomes using fastBAPS",
    "text": "Clustering genomes using fastBAPS\nWe can cluster genomes for epidemiology in a variety of ways, depending on the goal and genetic distances involved. Here will use fastBAPS, which is an optimised implementation of the original hierBAPS algorithm for hierarchical partitioning and Bayesian clustering of genomes. fastBAPS can be run in R as well as from the command line.\nIn the command below, we: * specify a set of SNPs/alleles for use by the model. * -i gubbins.filtered_polymorphic_sites.fasta * specify the name of the output file. * -o fastbaps.clusters.csv * specify that we want hierarchical clustering at two different levels (i.e.¬†BAPS will attempt to subdivide ‚Äúlevel 1 clusters‚Äù to form ‚Äúlevel 2 clusters‚Äù). * --levels 2 * specify a model for delineating genomes - we will use a relaxed clustering model which should maximise the number of clusters we get at level 2. * -p optimise.baps * specify an existing phylogenetic tree that we want to use for conditioning the statistical model. This option is not essential, but it ensures that the tree and the clusters are consistent with one another. * --phylogeny gubbins.final_tree.tre\nrun_fastbaps -i gubbins.filtered_polymorphic_sites.fasta -o fastbaps.clusters --levels 2 -p optimise.baps --phylogeny gubbins.final_tree.tre\nWe can view the output using head or cat\n\n\n\nfastbaps.command\n\n\n\nNow that we have clusters, let‚Äôs see how they look against the phylogenetic tree we made earlier.\n\nFirst, we‚Äôll need to make the csv file with our clusters compatible with microreact. We need to change the header of the first column from Isolates to ID. We‚Äôll use some simple code on the command line to do this:\nsed s/Isolates/ID/ fastbaps.clusters &gt; fastbaps.clusters.fixid.csv\nhead fastbaps.clusters.fixid.csv\n\n\n\nfastbaps.fixid.code\n\n\n \nGo to the microreact webpage, and try uploading the new gubbins filtered tree and the fastbaps clusters\n\n\n\nmicroreact.fastbaps.upload\n\n\n\n\n\n\nmicroreact.fastbaps.upload2\n\n\nYou can see that the first column has been correctly identified as the ‚ÄòID‚Äô - this will be linked to the taxa in the tree\nClick Continue\nYou should now see a tree coloured by your fastbaps groups. You can use the toggles to change the colouring and to add metadata blocks.\n\n\n\nmicroreact.fastbaps.final\n\n\n\n \nEND\n&lt;&lt;&lt; Go back to Manual Contents Page"
  },
  {
    "objectID": "Day3/Genomic_epidemiology.html",
    "href": "Day3/Genomic_epidemiology.html",
    "title": "Genomic Epidemiology for Outbreak Investigation",
    "section": "",
    "text": "Based on a exercise developed by Julio Diaz Caballero & Caterina Guzm√°n Verri for the Genomic Epidemiology course, Costa Rica, 2025"
  },
  {
    "objectID": "Day3/Genomic_epidemiology.html#analyse-genomes-with-pathogenwatch",
    "href": "Day3/Genomic_epidemiology.html#analyse-genomes-with-pathogenwatch",
    "title": "Genomic Epidemiology for Outbreak Investigation",
    "section": "Analyse genomes with PathogenWatch",
    "text": "Analyse genomes with PathogenWatch\nThe reference lab has sent you the culture results in an Excel file called (lab_results.xlsx). The lab reported that 2 out of 13 water samples from the CIC campus were positive for K. pneumoniae. This immediately prompted the closure of the 2 water sources and internal investigation.\n\nDoes this confirm that the source of the outbreak can be found in the campus of CIC?\n\nOne colony from each source was sequenced on Illumina MiSeq by the reference lab and the bioinformatics team has assembled each of them.\nWe will use https://pathogen.watch to identify the sequence type (ST) of these genomes using the Pasteur scheme.\nClick on the upload link at the top right\n\nClick on Single Genome FASTAs\n\nDrag and Drop provided fasta files (file1 and file2) into the browser and wait for analysis to finish\n\nClick on VIEW GENOMES\n\nSelect the two analysed genomes\n\nClick on Selected Genomes\n\nClick on Download data\n\nClick on MLST (Pasteur). This will download a csv file with the results.\n\nYou should now be able to find the mlst-Pasteur.csv in your Downloads folder."
  },
  {
    "objectID": "Day3/Genomic_epidemiology.html#merge-data-with-data-flo",
    "href": "Day3/Genomic_epidemiology.html#merge-data-with-data-flo",
    "title": "Genomic Epidemiology for Outbreak Investigation",
    "section": "Merge data with Data-flo",
    "text": "Merge data with Data-flo\nhttps://data-flo.io/\nNote: you need to sign-up for data-flo and Microreact. Creating your own account will allow you to manage and edit your projects.\nA maximum likelihood phylogenetic tree (tree.nwk) was inferred from the genomes of the 34 clinical samples and 2 environmental (water) samples. Six genomes from a previous outbreak (Mar-Apr 2022) were also included in the tree inference and their associated data added to the epi_data.csv file.\nThe disease detectives now have the information needed for the investigation in the following formats:\n\nepi_data.csv Epi data from 34 clinical cases and 6 cases from previous outbreak\nepi_data_new.csv Metadata of 2 positive water sources from the CIC campus\nlab_results.xlsx Culture and serotyping results for all the samples from the CIC campus\nmlst-Pasteur.csv MLST results from PathogenWatch\ntree.nwk Phylogenetic tree of 34 clinical cases, 2 culture-positive water samples, and 6 cases from previous outbreak\n\nThe files are located in this link.\nWe will combine data from these different sources with a data-flo workflow that takes the files above as input, and creates as an output a Microreact project where the data can be visualised.\nOpen the data-flo workflow (https://www.data-flo.io/editor/ai9jZdWD5KBoVDzuAaYCub)\nCopy the workflow to your own dataflo account.\n\nThis will open a copy of this workflow in your dataflo account.\n\nOn a different browser tab, get your microreact API access token at https://microreact.org/my-account/settings (you must already have created your microreact account).\n\nEdit the workflow to include your own microreact API access token.\n\nClick on *access token in the Create microreact project box\nFrom the options on the right, select Bind to value\nPaste your API access token in the VALUE box\n\n\nNow lets go to the implementation page by clicking on the RUN option.\n\nClick on Run. The Outputs box now shows the url of a Microreact project created by data-flo.\n\n\nBefore you open the Microreact project answer the following questions:\n\n\nIf later on we wanted to add additional entries (i.e.¬†water samples from CIC) , could we use the same data-flo? If so, how would this be beneficial?\nDo you use/know of any other methods to join data? If so, how do they compare with data-flo?"
  },
  {
    "objectID": "Day3/Genomic_epidemiology.html#explore-phylogenetic-and-epidemiological-data-with-microreact",
    "href": "Day3/Genomic_epidemiology.html#explore-phylogenetic-and-epidemiological-data-with-microreact",
    "title": "Genomic Epidemiology for Outbreak Investigation",
    "section": "Explore phylogenetic and epidemiological data with Microreact",
    "text": "Explore phylogenetic and epidemiological data with Microreact\nhttps://microreact.org/\nOpen the Microreact link in the output from data-flo. It should look something like this:\n\nNow change the colour column to ‚ÄúHospital‚Äù. Click on the eye icon at the top right of the page. Select ‚ÄúHospital from the Colour Column dropdown list.\n\nScale the markers on the map by clicking on the sliders icon, then in the opening options click on Markers. Finally, toggle the Scale markers option on.\n\n\nAnswer the following questions:\n\n\nWhat do the markers on the map represent?\nWhich hospital reported the index (first) case? Tip: Click on the earliest sample on the timeline.\n\n\nChange the colour column to ‚ÄúSource‚Äù. Click on the eye icon at the top left of the page. Then select ‚ÄúSource‚Äù from the Colour Column dropdown list.\n\nMicroreact assigns colours automatically to every column in your metadata table, but you can customise them. Let‚Äôs change the colour column to ‚ÄúInfection Type‚Äù. Click on the eye icon at the top left of the page. Then select ‚ÄúInfection Type‚Äù from the Colour Column dropdown list.\n\nNow Select ‚ÄúCategorical‚Äù from the Colour Palette option. Next Select ‚Äúqualitative‚Äù from the Palette type dropdown list and ‚Äú5‚Äù from the Number of colours dropdown list. Finally, select the second option from the list of palettes.\n\n\n\nAdd the columns ‚ÄúWorkplace‚Äù and ‚ÄúWorkplace details‚Äù as metadata blocks. For this, select the slider icon from top right of the tree panel, then, click on the Metadata blocks button. Finally, select ‚ÄúWorkplace‚Äù and ‚ÄúWorkplace details‚Äù from the list of options.\nNow explore the microreact you just created and try to answer the following questions:"
  },
  {
    "objectID": "Day3/Genomic_epidemiology.html#insights",
    "href": "Day3/Genomic_epidemiology.html#insights",
    "title": "Genomic Epidemiology for Outbreak Investigation",
    "section": "Insights",
    "text": "Insights\n\nCan you confirm that the K. pneumoniae isolated from the CIC campus are closely related to the patient isolates?\nWas it useful to include genomes from a previous outbreak in the analysis? Is the current outbreak related to the past outbreak from Mar-Apr 2022?\nIs the CIC Campus the origin of the outbreak?\nShould we investigate another location in the city?"
  },
  {
    "objectID": "Day3/Genomic_epidemiology.html#sign-in-to-data-flo",
    "href": "Day3/Genomic_epidemiology.html#sign-in-to-data-flo",
    "title": "Genomic Epidemiology for Outbreak Investigation",
    "section": "Sign-in to data-flo",
    "text": "Sign-in to data-flo\nhttps://docs.data-flo.io/introduction/getting-started-sign-in"
  },
  {
    "objectID": "Day3/Genomic_epidemiology.html#sign-in-to-microreact",
    "href": "Day3/Genomic_epidemiology.html#sign-in-to-microreact",
    "title": "Genomic Epidemiology for Outbreak Investigation",
    "section": "Sign-in to Microreact",
    "text": "Sign-in to Microreact\nHead over to https://microreact.org/my-account, and sign up using your email address or any other authenticating option"
  },
  {
    "objectID": "Day3/Phylogenetics.html#mapping-to-reference-genome",
    "href": "Day3/Phylogenetics.html#mapping-to-reference-genome",
    "title": "Phylogenetics",
    "section": "Mapping to reference genome",
    "text": "Mapping to reference genome\n\n\n\nThis is an image"
  },
  {
    "objectID": "Day3/Phylogenetics.html#sequencing-data-formats",
    "href": "Day3/Phylogenetics.html#sequencing-data-formats",
    "title": "Phylogenetics",
    "section": "Sequencing data formats",
    "text": "Sequencing data formats\nRaw data directly from the sequencer may be in sequencer-specific formats, but will always be converted to fastq format for downstream analysis. fastq is a plain text format that is standard within bioinformatics."
  },
  {
    "objectID": "Day3/Phylogenetics.html#fastq-format",
    "href": "Day3/Phylogenetics.html#fastq-format",
    "title": "Phylogenetics",
    "section": "Fastq format",
    "text": "Fastq format\n\n\n\nfastq multiline\n\n\nIn a fastq file, each sequencing read is represented by 4 different rows\n\nRead ID\n\nalways begins with ‚Äò@‚Äô\nOften includes information about sequencer and location on flow cell\n\nSequence info\n\nThe DNA sequence (A, T, G, C, N) at each position in read\n\nComment Line\n\nBegins ‚Äò+‚Äô, allows for additional information to be included ‚Äì rarely used\n\nQuality scores\n\nScore for each base position in read ‚Äì in ASCII code.\n\n\n\n\n\nfastq explainer"
  },
  {
    "objectID": "Day3/Phylogenetics.html#map-to-reference---sam-format",
    "href": "Day3/Phylogenetics.html#map-to-reference---sam-format",
    "title": "Phylogenetics",
    "section": "Map to reference - SAM format",
    "text": "Map to reference - SAM format\nEach FASTQ contains thousands to millions of sequencing reads\nMatching each read to the correct place in the genome (allowing for changes and errors) is computationally challenging\nWe use a ‚Äòread aligner‚Äô to map the position each read belongs to on a reference genome\n\nShort Reads - BWA, Bowtie2\nLong Reads - minimap2\n\nMapping produces a SAM file (or the compressed binary version BAM file)\nWe often use a tool called samtools to manage and view these large and complex files\n\n\n\nSamtool Image"
  },
  {
    "objectID": "Day3/Phylogenetics.html#variant-calling",
    "href": "Day3/Phylogenetics.html#variant-calling",
    "title": "Phylogenetics",
    "section": "Variant Calling",
    "text": "Variant Calling\nWhen reads are aligned to a reference genome, point mutations (SNPs) between the sequencing reads and the reference can be identified. In the plot below: * Reads are inblue/green. * Changes from the reference are highlighted for each read in red.\n\n\n\nBAM-pileup\n\n\n\nSome variants can be very easy to determine However, sequencing error can lead to changes that are technical artefacts\nSome variants are therefore more difficult to determine (e.g.¬†a low coverage region with very few reads)\nWhat if some reads correspond to a SNP and others do not?\nVariant callers (e.g.¬†bcftools, GATK, FreeBayes) apply statistical models to determine true variants, accounting for:\nReads counts/proportions supporting each allele\nQuality scores for base, reads, mapping, position in the read a variant occurs\nVariant callers output their analysis in a ‚Äòvariant call format‚Äô ‚Äì VCF file\n\n\n\n\nvcf"
  },
  {
    "objectID": "Day3/Phylogenetics.html#consensus-calling",
    "href": "Day3/Phylogenetics.html#consensus-calling",
    "title": "Phylogenetics",
    "section": "Consensus calling",
    "text": "Consensus calling\n\nVariant calls will be filtered and assessed (generally in an automated way).\nOnce we have a filtered variant set, variants can be applied to the reference genome.\nThis generates a consensus ‚Äòpseudosequence‚Äô in FASTA format.\u000b\n\n\n\n\nconsensus.fa\n\n\n\nNote, that one important step that can be overlooked is false negative variant calls\n\nVCF format only highlights changes from the reference (i.e.¬†positive calls)\nIf a base position has low coverage and no SNP is called, how do you know if there is a SNP or not?\nSome methods will simply assume that the site is ‚Äòreference‚Äô, without evidence to the contrary\nThis can give misleading results for some analyses (e.g.¬†phylogenetics)\nImportant to ensure all sites without support are tested, and marked as ‚ÄòN‚Äô.\n\n\nWe can use integrated pipelines to automate parts of these processes\n\n\n\nworkflow-tools"
  },
  {
    "objectID": "Day3/Phylogenetics.html#finding-the-data",
    "href": "Day3/Phylogenetics.html#finding-the-data",
    "title": "Phylogenetics",
    "section": "Finding the data",
    "text": "Finding the data\nNavigate to the Phylogenetics directory\ncd Phylogenetics\nWe can confirm where we are\npwd\nWe can also examine the contents of this file\nls -l\n\n\n\ndirectory_contents\n\n\nThe folder contains:\n\ntwo pairs of sequencing reads :\n\nnew-sample-1_1.fastq.gz, new-sample-1_2.fastq.gz\nnew-sample-2_1.fastq.gz, new-sample-2_2.fastq.gz\n\na folder containing indexed reference genomes\nan archive file containing previous snippy runs (snippy.runs.1.tar.gz)"
  },
  {
    "objectID": "Day3/Phylogenetics.html#perform-mapping-of-reads-to-reference-genome-using-snippy",
    "href": "Day3/Phylogenetics.html#perform-mapping-of-reads-to-reference-genome-using-snippy",
    "title": "Phylogenetics",
    "section": "Perform mapping of reads to reference genome using snippy",
    "text": "Perform mapping of reads to reference genome using snippy\nSnippy is an integrated pipeline that maps reads to a reference genome and produces a range of outputs. For the purposes of this tutorial, we will use snippy to organise analyse our genomes.\nFirst, activate the snippy environment:\nconda activate snippy_env\nYou can view the options for snippy using the following code:\nsnippy -h\n\n\n\nsnippy.h\n\n\nNow run snippy on the pair of fastqs (for this exercise we‚Äôll call this ‚Äònew-sample-1‚Äô)\nsnippy --outdir new-sample-1 --R1 new-sample-1_1.fastq.gz --R2 new-sample-1_2.fastq.gz --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa --cpus 4 --ram 4 --force --quiet\nWait for snippy to finish\n\n\n\nsnippy.run1\n\n\nExamine results of snippy\nls -lh new-sample-1/\n\n\n\nsnippy.run1.output\n\n\nYou can examine the log file to see exactly what snippy has done\nless new-sample-1/snps.log\n\nExamine snippy logs\nWe‚Äôll use grep to retrieve the relevant lines for each command from the log file.\nFirst, here is the command we used to set snippy running\ngrep \"outdir\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep1\n\n\nAfter ensuring the reference genome is indexed, snippy maps the reads to the reference genome using bwa mem\ngrep \"bwa mem\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep2\n\n\nWithin that command, snippy also marks duplicate sequencing reads using samtools markdup (we extract only the first part of that command here, but you can look for it with less)\ngrep \"COMMAND: samtools\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep3\n\n\nSnippy then uses FreeBayes to call variants against the reference genome, producing a variant call file (.vcf)\ngrep \"freebayes\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep4\n\n\nSnippy then applies some filters to assess the quality of those variants. It then applies the high quality variants to the reference genome to create a ‚Äòpseudosequence consensus‚Äô representation of our new genome\ngrep \"bcftools consensus\" new-sample-1/snps.log\n\n\n\nsnippy.run1.grep5\n\n\nSnippy creates two versions of the pseudosequence consensus: - snps.consensus.fa contains all high quality variants - snps.consensus.subs.fa contains only high quality SNPs (no INDELS)\n\n\nExamine snippy outputs\nThe bam file contains all the mapping positions on the genome for each individual read, along with metrics around mapping quality. This is a binary (machine readable) file, but we can view it using samtools view\nsamtools view new-sample-1/snps.bam | head -2\n\n\n\nsnippy.run1.samtools-view\n\n\nThe VCF file contains all the variants that have been called in our new genome compared to the reference genome\nhead -35 new-sample-1/snps.vcf \n\n\n\nsnippy.run1.vcf\n\n\nThe first ~28 lines here are ‚Äòheaders‚Äô and contain information about what has been done to call the variants, and helps you to interpret what different columns mean.\nThe last ~7 lines are individual variants, one per line. Variants columns are labelled\n#CHROM POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  new-sample\nWe can view a slightly easier summary of these variants in the snps.tab file\nhead -5 new-sample-1/snps.tab\n\n\n\nsnippy.run1.snps\n\n\nIn this file, we have not provided gene information, so only the first 6 columns are relevant\nSnippy has also created a fasta file with our pseudosequence genome\nhead new-sample-1/snps.consensus.fa\n\n\n\nsnippy.run1.consensus.fa"
  },
  {
    "objectID": "Day3/Phylogenetics.html#map-a-second-genome-to-reference",
    "href": "Day3/Phylogenetics.html#map-a-second-genome-to-reference",
    "title": "Phylogenetics",
    "section": "Map a second genome to reference",
    "text": "Map a second genome to reference\nWe now need to map the second genome (new-sample-2) to our reference\nAdapt the code examples provided above to run snippy on the second set of fastq files\nExamine the results of snippy for new-sample-2 using grep, ls and less"
  },
  {
    "objectID": "Day3/Phylogenetics.html#use-an-existing-assembly-as-input-to-snippy",
    "href": "Day3/Phylogenetics.html#use-an-existing-assembly-as-input-to-snippy",
    "title": "Phylogenetics",
    "section": "Use an existing assembly as input to snippy",
    "text": "Use an existing assembly as input to snippy\nSometimes we only have assemblies available, or we want to integrate long-read data into our analysis of short-read genomes. Snippy allows us to do this using the --ctgs argument. This takes an existing assembly file and simulates short reads, before calling variants against the reference.\nWe are going to integrate assemblies from a genome called ‚ÄúCTMA_1441‚Äù into our mapping analysis analysis. This genome was published in 2020 (https://pubmed.ncbi.nlm.nih.gov/32586863/), and used both short and long reads. For the purposes of this exercise, we have assembled it using three different approaches:\n\nShort read assembly (SPAdes) from Illumina reads\nLong read assembly (Dragonflye) from Oxford Nanopore reads\nHybrid assembly from long reads (Dragonflye), followed by short read polishing\n\nNote that we will explore the differences between these assembly methods more in a later module.\n\nLocate existing assemblies\nThe reads are hosted in the course GitHub repository. If that has been updated on your local VM, the files should be available to access. Let‚Äôs check:\nls -lh ~/github_repository/course_data_2025/mapping_and_phylo/\n\n\n\ngithub.repo.ls\n\n\nIf the files are missing, check with the instructors.\nAssuming they are available, we can copy the assemblies to our current working directory to make things easier:\ncp ~/github_repository/course_data_2025/mapping_and_phylo/CTMA_1441.* .\n\n\nMap an existing short-read assembly\nThe short read assembly is called CTMA_1441.unicycler-short.fasta. We can add the short-read assembly to snippy like this:\nsnippy --outdir CTMA_1441.short --ctgs CTMA_1441.unicycler-short.fasta --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa --cpus 4 --ram 4 --force --quiet\n\n\n\nsnippy_assembly_example\n\n\n\n\nMap a long-read assembly to reference\nWe now need to map the two long read assemblies to our analysis: - CTMA_1441.filtreads.dragonflye.long_4.1_polish.contigs.fa - CTMA_1441.filtreads.dragonflye.long_4.1_nopolish.contigs.fa\nAdapt the code examples provided above to run snippy on the long-read assemblies using the output directories CTMA_1441.long_polish and CTMA_1441.long_nopolish.\nExamine the results of snippy for each assembly using grep, ls and less.\nWhat do you notice about the number of variable sites (reported at the end of the snippy run) identified with the different assemblies?"
  },
  {
    "objectID": "Day3/Phylogenetics.html#contextualise-new-genomes-with-a-collection",
    "href": "Day3/Phylogenetics.html#contextualise-new-genomes-with-a-collection",
    "title": "Phylogenetics",
    "section": "Contextualise new genomes with a collection",
    "text": "Contextualise new genomes with a collection\nNow lets look at this new genome in context with a collection.\nWe can add this genome to a collection of genomes mapped using snippy-core.\nFor this exercise, we will again retrieve our data from the github folder:\ncp ~/github_repository/course_data_2025/mapping_and_phylo/old.snippy.runs_2025.tar.gz .\nCheck the files are there:\nls -lh\n\n\n\nsnippy-context.copy-git.ls\n\n\n(Note that there is an existing file in our directory called `old.snippy.runs.2023.tar.gz - we will not be using this).\nFirst, let‚Äôs extract the old snippy runs from our archive file\ntar -zxf old.snippy.runs_2025.tar.gz\nWe can see that we now have a new directory old.snippy.runs_2024\nls -lh\n\n\n\nsnippy-context.tar.ls1\n\n\nls -lh old.snippy.runs_2024\n\n\n\nsnippy-context.tar.ls2\n\n\nNow lets use snippy-core to summarise all these genomes along with the new ones and create a multiple sequence alignment\nsnippy-core --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa old.snippy.runs_2024/* new-sample-1 new-sample-2 CTMA_1441.short CTMA_1441.long_polish CTMA_1441.long_nopolish\n\n\n\nsnippy-core.run1\n\n\nSnippy has now created a number of files, including a ‚Äòcore SNP alignment‚Äô\nls -l core.*\n\n\n\nsnippy-core.run2\n\n\nWe have various files that summarise our variants, e.g.\nhead core.tab \n\n\n\nsnippy-core.run3\n\n\nAnd our multiple sequence alignment containing all genomes:\nhead core.full.aln\n\n\n\nsnippy-core.run4\n\n\nThis file masks sequences with low confidence in different ways, but for some applications we want everything masked in the same way. Let‚Äôs change that so anything uncertain is marked as ‚ÄôN‚Äô using the snippy-clean_full_aln script that comes with snippy.\nsnippy-clean_full_aln core.full.aln &gt; clean.full.aln\n\n\n\nsnippy-core.run5"
  },
  {
    "objectID": "Day3/Phylogenetics.html#phylogenetics",
    "href": "Day3/Phylogenetics.html#phylogenetics",
    "title": "Phylogenetics",
    "section": "Phylogenetics",
    "text": "Phylogenetics\nNow that we have a clean multiple sequence alignment, we are now going to use IQ-TREE to build a maximum likelihood phylogeny.\n\nMake a SNP-only alignment using snp-sites\nCalculating a phylogeny on whole genome sequences can be very time consuming. We can speed this up by only using the variable sites (SNPs). However, we need to be aware that only including variable sites can affect the evolutionary rate estimates made by phylogenetics software - therefore, we need to account for the sites we remove in our analysis.\nWe will use snp-sites to do this. You can view the options for snp-sites\nsnp-sites\n\n\n\nsnp-sites.1\n\n\nFirst, remove all the invariant sites and create a SNP-only multiple sequence alignment.\nsnp-sites -o clean.full.SNPs.aln clean.full.aln\nWe can also use snp-sites to find out how many invariant sites were removed (and what proportion of A, T, G, C they were). Use snp-sites -C to do this for the file above.\n\n\n\nsnp-sites.2\n\n\n\n\nCalculate a phylogenetic tree from the SNPs using IQ-TREE2\nWe can look at the options for IQ-TREE below\niqtree -h\n\n\n\niqtree.1\n\n\nIn the command below, we: - specify the multiple sequence alignment using -s clean.full.SNPs.aln - ask IQ-TREE to take account of missing invariant sites using -fconst $(snp-sites -C clean.full.aln) - specify an evolutionary model we want IQ-TREE to use -m GTR+F+I - tell IQ-TREE to use a maximum of 2 CPUs (threads) and 2GB memory -T 2 -mem 2G - perform 1000 ultrafast bootstraps -B 1000 - use sample M66 as an outgroup -o M66\niqtree -s clean.full.SNPs.aln -fconst $( snp-sites -C clean.full.aln ) -m GTR+F+I -T 2 -mem 2G -B 1000 -o M66\n\n\n\niqtree.2\n\n\nLook at folder\nls -lh clean.full.*\n\n\n\niqtree.3\n\n\nOur maximum likelihood tree is labelled clean.full.SNPs.aln.treefile. The treefile suffix is not always correctly identified by many tools, so we‚Äôll relabel this as something else:\ncp clean.full.SNPs.aln.treefile clean.full.SNPs.aln.tre\n\n\nVisualise a phylogenetic tree\nTo examine our tree, we can look at the text file:\ncat clean.full.SNPs.aln.tre\n\n\n\niqtree.4\n\n\nBut this is not very helpful - it‚Äôs just raw text in ‚Äònewick‚Äô format.\nInstead, we can visualise this using figtree\nfigtree clean.full.SNPs.aln.tre &\n(Note: Remember to include the & symbol at the end - this allows figtree to run in the background, and allows you to still use the command prompt)\n\n\n\n\nfigtree.1\n\n\nIgnore the java error messages. The popup box is asking you how to describe the ‚Äòbootstrap values‚Äô. You can click ‚ÄòOK‚Äô here.\nYou should now have a visualisation of the tree we just generated\n\n\n\nfigtree.2\n\n\nExamine the tree: - How are new-sample-1 and new-sample-2 related to each other? - What do you notice about the different versions of the assembled genomes CTMA_1441? - What does this tell you about the different assembly methods?"
  },
  {
    "objectID": "Day3/Phylogenetics.html#adjusting-a-dataset-to-remove-a-poor-quality-genome",
    "href": "Day3/Phylogenetics.html#adjusting-a-dataset-to-remove-a-poor-quality-genome",
    "title": "Phylogenetics",
    "section": "Adjusting a dataset to remove a poor quality genome",
    "text": "Adjusting a dataset to remove a poor quality genome\nThe unpolished long-read assembly CTMA_1441.long_nopolish is filled with uncorrected errors. We therefore should not use it in our analysis. We will also remove the duplicate examples of CTMA_1441, so we are only using a single example. We can rerun snippy-core to rebuild the alignment without the unnecessary samples:\nsnippy-core --ref references/Vibrio_cholerae_O1_biovar_eltor_str_N16961_v2.fa old.snippy.runs_2024/* new-sample-1 new-sample-2\nsnippy-clean_full_aln core.full.aln &gt; clean2.full.aln\nAnd now build a new phylogeny:\n\nRun snp-sites to create a file called clean2.full.SNPs.aln\nRun iqtree to calculate a phylogeny for clean2.full.SNPs.aln\nRename the final treefile to clean2.full.SNPs.aln.tre\n\nYou can recheck the tree using figtree."
  },
  {
    "objectID": "Day3/Phylogenetics.html#tree-visualisation-using-microreact",
    "href": "Day3/Phylogenetics.html#tree-visualisation-using-microreact",
    "title": "Phylogenetics",
    "section": "Tree Visualisation using Microreact",
    "text": "Tree Visualisation using Microreact\nWe can also visualise our tree using a webtool called Microreact\n\nOpen Firefox and navigate to https://microreact.org/\nClick ‚Äòupload‚Äô\n\n![microreact.1]images/(Microreact-front-page.png)\n\nSelect the file upload, or drag the tree file clean2.full.SNPs.aln.tre onto the upload screen\n\n\n\n\nmicroreact.2\n\n\nMicroreact may not recognise the file, so let‚Äôs tell micoreact this this is a tree in newick format. Note that we can also use this screen to add metadata in csv format.\n\n\n\nmicroreact.3\n\n\nClick continue to view the tree\n\n\n\nmicroreact.4"
  },
  {
    "objectID": "Day3/Phylogenetics.html#accounting-for-recombination-with-gubbins",
    "href": "Day3/Phylogenetics.html#accounting-for-recombination-with-gubbins",
    "title": "Phylogenetics",
    "section": "Accounting for recombination with gubbins",
    "text": "Accounting for recombination with gubbins\nWe can use gubbins to infer recombining sites by looking for increased SNP density that occurs in specific ancestral nodes\ngubbins has been installed using conda. We must activate the relavent conda environment before running any commands using gubbins.\nconda activate gubbins-env\nNow we can run gubbins\nrun_gubbins.py -h\n\n\n\ngubbins.1\n\n\nThe following command runs gubbins on standard settings, with 4 CPUs.\nNote: the -c option tells the program to use 4 CPUs. Note: the -p option tells the program to name all files with the prefix gubbins This command can take a few minutes to run.\nrun_gubbins.py -c 4 -p gubbins clean2.full.aln\n\n\n\nrun_gubbins_hanging\n\n\n\n\n\nrun_gubbins_hanging\n\n\nNote: gubbins can take a long time to run on some computers. If gubbins takes more than 10 mins to complete, we have already run it for you - the files are available in the GitHub folder at ~/github_repository/Modules/Mapping_and_Phylo/gubbins_backup_2024.tar.gz.\nls -lh ~/github_repository/course_data_2025/mapping_and_phylo/\ncp ~/github_repository/course_data_2025/mapping_and_phylo/gubbins_backup_2024.tar.gz .\ntar -zxf gubbins_backup_2024.tar.gz\nNote: You may notice there is a directory called gubbins_backups - this is from a previous year, and will not be used today.\nLets look at what gubbins has done\nls -l gubbins.*\n\n\n\ngubbins.backup.files\n\n\nYou can explore these files For example gubbins.recombination_predictions.gff is a gff file that contains a record of each recombination block identified, how many SNPs it contains, and what samples are affected.\nhead gubbins.recombination_predictions.gff\n\n\n\ngubbins.3\n\n\ngubbins.filtered_polymorphic_sites.fasta is a fasta file containing only SNPs, where recombining sites have been ‚Äòmasked to N‚Äô.\nhead gubbins.filtered_polymorphic_sites.fasta\n\n\n\ngubbins.4\n\n\ngubbins.final_tree.tre is a phylogeny in which recombination has already been accounted for.\nYou can visualise this in figtree or microreact as above.\n\n\nRoot a phylogeny\nThe trees outputted by iqtree and gubbins are unrooted, but we may want to apply some evolutionary direction to them. One approach that is commonly used for bacterial datasets is midpoint rooting. Midpoint rooting involves locating the midpoint of the longest path between any two tips and putting the root in that location. Note that this does not necessarily infer the true root, and this should be used with caution.\nTo midpoint root our tree, we will use a simple script written in python that uses the ete package. You can examine the code:\nless midpoint.root.py\nTo make the code work in the VM, we need to load another conda environment:\nconda activate ete3-env\n\n\n\nmidpoint.script\n\n\nWe can use this script to midpoint root our tree\npython midpoint.root.py gubbins.final_tree.tre &gt; gubbins.final_tree.midpoint.tre\nYou can visualise this in figtree or microreact as above. How does it compare to the unrooted version?\n\n\nVisualising recombination blocks using phandango\nWe can also visualise the tree together with the recombination blocks inferred by gubbins using a webtool called phandango\nUsing your browser, navigate to (https://jameshadfield.github.io/phandango/#/)\n\n\n\nphandango.1\n\n\nOn your VM desktop, go to the file manager, and navigate to /home/manager/Module_4_Mapping_Phylogeny\nDrag and drop the tree file gubbins.final_tree.midpoint.tre into the phandango browser window\nDrag and drop the recombination gff file gubbins.recombination_predictions.gff into the phandango browser window\nPhandango should automatically display blocks of recombination in red (ancestral) and blue(specific to a sample)\n![phandango.1](images/gubbins-phandango-plot__2024.png"
  },
  {
    "objectID": "Day2/Annotation.html",
    "href": "Day2/Annotation.html",
    "title": "Annotation",
    "section": "",
    "text": "There are a number of ways you can generate annotation for a novel sequence. You can manually annotate a sequence by curating the results of bioinformatic analyses of the sequence, but this is time consuming and prone to human bias. If there is a closely related reference sequence and annotation, you can transfer annotation by similarity matching, but this relies on there being a suitable reference. The fastest and most consistant way to generate annotation for novel sequence is to use an automatic annotation software such as prokka (Seemann T. (2014) Prokka: rapid prokaryotic genome annotation. Bioinformatics. 30:2068-9. doi: 10.1093/bioinformatics/btu153) or bakta (Schwengers O et al., (2021). Bakta: rapid and standardized annotation of bacterial genomes via alignment-free sequence identification. Microbial Genomics, 7(11). https://doi.org/10.1099/mgen.0.000685)\nIn this example, we are going to use bakta as this is a new tool that generates standardized, taxonomy-independent, high-throughput annotation.\n\n\nbakta has been installed using conda. We must activate the relavent conda environment before running any commands using bakta.\nconda activate bakta-env\nbakta uses a number of databases to annotate a genome. These databases are located in bakta_database/db-light. These have been updated for you.\nRun bakta with the following commands to anotate the ordered assembly for 16B:\n\nSpecify the database directory\n\n--db bakta_database/db-light\n\nSpecify the multifasta file to be annotated\n\n16B.ordered.fasta\n\n\nbakta --db bakta_database/db-light 16B.ordered.fasta\nThe first step of bakta is to annotate non-protein encoding regions including tRNAs, tmRNA, rRNA, ncRNA. It then predicts protein coding sequences and annotates these from match to proteins with predicted function, and includes annotation of hypothetical proteins with matches to protein domains. Matches to plasmids origins of replication are included where found and provides a summary of the genomic annotation.\n\n\n\nbakta_output\n\n\nYou should now see 16B.ordered.embl in your directory\nFor more information on the annotation generated by bakta, the run options and the output it generates see here: https://github.com/oschwengers/bakta\nWe must now deactivate the bakta conda environment.\nconda deactivate\n\n\n\nbakta has generated a number of output files in different formats that contain the annotation for the 16B ordered assembly. We are going to use the EMBL format file and view it in act.\nIn act, open the 16B.ordered.embl file into the 16B.ordered.fasta entry by going to the File menu, and selecting the 16B.ordered.fasta option, and right clicking onto the Read An Entry option.\n\n\n\nACT_3_regions\n\n\n\n\n\n\n\nACT_region_1\n\n\nIn this region on the left hand side of the reference chromosome and near the origin of replication, you can see that the first contig spans the origin of replication and therefore matches two separate regions of the reference (left and right ends of the MSSA476 chromosome).\n\n\n\nACT_region_1_ori_rep\n\n\n\n\n\nIn region 2, you can see a prophage in pink which has inserted into the MSSA476 genome which is not present in the genome of 16B. Next the this, there is a gap annotated by bakta on the 16B genome. This is a scaffolding error by abacas.\n\n\n\nACT_region_2\n\n\n\n\n\n\n\n\nACT_region_3\n\n\nIn this region near at the right hand side of the assembly, we have the non-mapping contigs (yellow). Previously we have seen that the two largest contigs are likely to be separate plasmids.\nHave a look at the annotation generated by bakta of the CDSs of the contigs in this region.\nWhat sort of functions do the proteins in this encode?\nDoes the annotation confirm them as plasmids?\nCan you find any genes of interest for antibiotic resistance that ariba identified?"
  },
  {
    "objectID": "Day2/Annotation.html#genomic-annotation-using-bakta",
    "href": "Day2/Annotation.html#genomic-annotation-using-bakta",
    "title": "Annotation",
    "section": "",
    "text": "bakta has been installed using conda. We must activate the relavent conda environment before running any commands using bakta.\nconda activate bakta-env\nbakta uses a number of databases to annotate a genome. These databases are located in bakta_database/db-light. These have been updated for you.\nRun bakta with the following commands to anotate the ordered assembly for 16B:\n\nSpecify the database directory\n\n--db bakta_database/db-light\n\nSpecify the multifasta file to be annotated\n\n16B.ordered.fasta\n\n\nbakta --db bakta_database/db-light 16B.ordered.fasta\nThe first step of bakta is to annotate non-protein encoding regions including tRNAs, tmRNA, rRNA, ncRNA. It then predicts protein coding sequences and annotates these from match to proteins with predicted function, and includes annotation of hypothetical proteins with matches to protein domains. Matches to plasmids origins of replication are included where found and provides a summary of the genomic annotation.\n\n\n\nbakta_output\n\n\nYou should now see 16B.ordered.embl in your directory\nFor more information on the annotation generated by bakta, the run options and the output it generates see here: https://github.com/oschwengers/bakta\nWe must now deactivate the bakta conda environment.\nconda deactivate"
  },
  {
    "objectID": "Day2/Annotation.html#visualizing-the-bakta-annotation-in-act",
    "href": "Day2/Annotation.html#visualizing-the-bakta-annotation-in-act",
    "title": "Annotation",
    "section": "",
    "text": "bakta has generated a number of output files in different formats that contain the annotation for the 16B ordered assembly. We are going to use the EMBL format file and view it in act.\nIn act, open the 16B.ordered.embl file into the 16B.ordered.fasta entry by going to the File menu, and selecting the 16B.ordered.fasta option, and right clicking onto the Read An Entry option.\n\n\n\nACT_3_regions\n\n\n\n\n\n\n\nACT_region_1\n\n\nIn this region on the left hand side of the reference chromosome and near the origin of replication, you can see that the first contig spans the origin of replication and therefore matches two separate regions of the reference (left and right ends of the MSSA476 chromosome).\n\n\n\nACT_region_1_ori_rep\n\n\n\n\n\nIn region 2, you can see a prophage in pink which has inserted into the MSSA476 genome which is not present in the genome of 16B. Next the this, there is a gap annotated by bakta on the 16B genome. This is a scaffolding error by abacas.\n\n\n\nACT_region_2\n\n\n\n\n\n\n\n\nACT_region_3\n\n\nIn this region near at the right hand side of the assembly, we have the non-mapping contigs (yellow). Previously we have seen that the two largest contigs are likely to be separate plasmids.\nHave a look at the annotation generated by bakta of the CDSs of the contigs in this region.\nWhat sort of functions do the proteins in this encode?\nDoes the annotation confirm them as plasmids?\nCan you find any genes of interest for antibiotic resistance that ariba identified?"
  },
  {
    "objectID": "Day2/Annotation.html#identify-highly-similar-regions-using-blastn-mw2-vs-16b",
    "href": "Day2/Annotation.html#identify-highly-similar-regions-using-blastn-mw2-vs-16b",
    "title": "Annotation",
    "section": "Identify highly similar regions using blastn (MW2 vs 16B)",
    "text": "Identify highly similar regions using blastn (MW2 vs 16B)\nIn order to examine the regions of difference in the 16B assembly with MW2 we are going generate a comparison file that we can load in ACT, as we did previously for MSSA476.\nAs before, run makeblastdb to format one of the sequences as a blast database, this time specifying our ordered asssembly as the input:\nmakeblastdb -in 16B.ordered.fasta -dbtype nucl -out 16B.ordered\nNext we run blastn with MW2.dna as the query, against the 16B.ordered database:\nblastn -query MW2.dna -db 16B.ordered -out 16B.ordered.fasta_vs_MW2.dna.tsv -outfmt 6"
  },
  {
    "objectID": "Day2/Annotation.html#comparing-annotations-of-16b-vs-mssa476-vs-mw2-in-act",
    "href": "Day2/Annotation.html#comparing-annotations-of-16b-vs-mssa476-vs-mw2-in-act",
    "title": "Annotation",
    "section": "Comparing annotations of 16B vs MSSA476 vs MW2 in act",
    "text": "Comparing annotations of 16B vs MSSA476 vs MW2 in act\nWe are now going to load up the three sequences and relevant comparison files into act. You can do this either from the command line or by clicking on the ACT icon. If you prefer to do it from the command line you can type:\nact MSSA476.embl MSSA476.dna_vs_16B.ordered.fasta.tsv 16B.ordered.embl 16B.ordered.fasta_vs_MW2.dna.tsv MW2.embl &\nNow that you have included the MW2 sequence to the comparison you should see an act view with three DNA panels and two comparison panels separating them. In this zoomed out view, MSSA476 is on the top, 16B is in the middle and MW2 on the bottom. You will also notice that in the act menu at the top there are now three entry options.\n\n\n\nACT_3way_1\n\n\nTo help you with your investigations, we have also provided two additional annotation files that contain misc_features which mark the extent of MGEs identified in the MSSA476 and MW2 chromosomes. These can be loaded into the appropriate entry (from the menu click File, the entry you want, then Read An Entry). The misc_features are colour coded in the act view according to the type of MGE (see legend on on the circular diagram of MSSA476).\n\n\n\nACT_3way_2\n\n\nGo back to region 1, this time looking at the regions of difference between the three genomes. You shouod be able to find some MGEs highlighted in yellow on the MW2 and MSSA476 genomes.\nWhat sort of functions do the proteins in this region encode?\nCan you find any genes of interest for antibiotic resistance that ariba identified?\nDoes this region of MW2 share identity with regions of 16B located elsewhere in the assembly? Hint: Look at the transversion in the region which shares similarity to an unmapped contig\nWhat has occurred in this region of the 16B chromosome that could explain the structure of this region in comparison to the other strains?\n\n\n\nACT_mecA_16B_vs_MW2\n\n\nCompare the other regions containing MGEs.\nHow do these regions vary in the three strains, and what do they encode?\nDoes this explain the differences in the antibiotics phenotypes of the isolates?\nCan you find any other important genes associated with MGEs that are vary in the isolates that are clinical relevant? (clue, think toxins)"
  },
  {
    "objectID": "module3/module3_answers.html",
    "href": "module3/module3_answers.html",
    "title": "Bioinformatic methods for bacterial genomics, Cluj-Napoca, October 2025",
    "section": "",
    "text": "Run ARIBA on all samples\nExample script: create the following bash file: mkdir /home/ubuntu/Data/ARIBA_run/run_ariba.sh\nAdd the following lines of code:\nfor sample in `ls /home/ubuntu/Data/all_fastqs/*_1.fastq.gz | sed 's/\\_1.fastq.gz//'`\ndo\necho $sample\noutput=$(echo $sample | sed -E 's#.*/([^/]+)$#\\1#')\nif [ -d /home/ubuntu/Data/ARIBA_output/${output}.ariba ]; then\nrm -r /home/ubuntu/Data/ARIBA_output/${output}.ariba\nfi\nariba run /home/ubuntu/Data/ARIBA_dbs/out.resfinder.prepareref ${sample}_1.fastq.gz ${sample}_2.fastq.gz /home/ubuntu/Data/ARIBA_output/${output}.ariba\ndone"
  },
  {
    "objectID": "data-locations.html",
    "href": "data-locations.html",
    "title": "data-locations",
    "section": "",
    "text": "Data Locations /home/ubuntu/ ‚îú‚îÄ‚îÄ Data/ ‚îÇ ‚îú‚îÄ‚îÄ samples/ # Raw FASTQ files ‚îÇ ‚îî‚îÄ‚îÄ Reference/ # Reference genomes ‚îú‚îÄ‚îÄ analysis/ ‚îÇ ‚îú‚îÄ‚îÄ qc/ # FastQC outputs ‚îÇ ‚îú‚îÄ‚îÄ assembly/ # Assembly results ‚îÇ ‚îú‚îÄ‚îÄ annotation/ # Prokka outputs ‚îÇ ‚îú‚îÄ‚îÄ amr/ # ARIBA results ‚îÇ ‚îú‚îÄ‚îÄ pangenome/ # Roary outputs ‚îÇ ‚îî‚îÄ‚îÄ snippy/ # SNP analysis ‚îî‚îÄ‚îÄ Software/ # Pre-installed tools"
  },
  {
    "objectID": "module2/module2_answers.html",
    "href": "module2/module2_answers.html",
    "title": "Bioinformatic methods for bacterial genomics, Cluj-Napoca, October 2025",
    "section": "",
    "text": "How to run Prokka on all samples from one folder\nExample script: create the following bash file: mkdir /home/ubuntu/Data/PROKKA_run/run_prokka.sh\nAdd the following lines of code:\nfor sample in `ls /home/ubuntu/Data/all_fastas/*.fasta | sed 's/\\.fasta//'`\ndo\necho $sample\noutput=$(echo $sample | sed -E 's#.*/([^/]+)$#\\1#')\necho $output\ndocker run -v $(pwd):$(pwd) -w $(pwd) staphb/prokka:latest prokka --outdir /home/ubuntu/Data/annotations/${output} --prefix ${output} /home/ubuntu/Data/all_fastas/${output}.fasta\ndone"
  },
  {
    "objectID": "module5/module5.html",
    "href": "module5/module5.html",
    "title": "Module 5",
    "section": "",
    "text": "How to setup your Amazon EC machine\nIn your browser navigate to: https://aws.amazon.com/\nCreate an AWS account\nOnce your virtual machine is up and running, login via ssh, using your key.pem file.\nchmod 400 key.pem\nssh -i key.pem ubuntu@xxx.xxx.xxx.xxx\nWhere xxx.xxx.xxx.xxx is the IP of the virtual machine.\nOnce logged in, change the password for the username ubuntu:\nsudo passwd ubuntu\nThen, try to log in via the visual interface, by navigating to the IP address (xxx.xxx.xxx.xxx) in your browser."
  },
  {
    "objectID": "module5/module5.html#create-an-image",
    "href": "module5/module5.html#create-an-image",
    "title": "Module 5",
    "section": "Create an image",
    "text": "Create an image"
  },
  {
    "objectID": "module5/module5.html#launch-instances-from-an-image",
    "href": "module5/module5.html#launch-instances-from-an-image",
    "title": "Module 5",
    "section": "Launch instances from an image",
    "text": "Launch instances from an image"
  },
  {
    "objectID": "module5/module5.html#delete-the-aws-account",
    "href": "module5/module5.html#delete-the-aws-account",
    "title": "Module 5",
    "section": "Delete the AWS account",
    "text": "Delete the AWS account\nHow to close your AWS account\nhttps://docs.aws.amazon.com/accounts/latest/reference/manage-acct-closing.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "This page is a repository for training resources designed and developed for the short course on Bacterial Genomics Bioinformatics, Babes-Bolyai University, Cluj-Napoca, Romania, November 2025, by Monica Abrudan.\nThe materials provided in this repository are FREE to use. This work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement.\nFind out more about the author here"
  },
  {
    "objectID": "microreact/microreact_doc.html",
    "href": "microreact/microreact_doc.html",
    "title": "Microreact documentation",
    "section": "",
    "text": "https://docs.microreact.org/"
  },
  {
    "objectID": "data-flo/data-flo_doc.html",
    "href": "data-flo/data-flo_doc.html",
    "title": "Data-flo documentation",
    "section": "",
    "text": "https://docs.data-flo.io/introduction/readme"
  },
  {
    "objectID": "Syllabus.html",
    "href": "Syllabus.html",
    "title": "Training Syllabus",
    "section": "",
    "text": "Topics to be addressed during the workshop\n\n\n\nTraining Objectives\nLearning objectives. By the end of the workshop, the participants will be able to\n\n\nTargeted competencies and Knowledge, Skills and Attitudes\n\n\nTraining Methods and Instructional Strategies\n\n\nDuration of training\n\n\nSchedule of sessions\n\n\nAssessment and Evaluation\n\n\nResources\n\n\nRecommended background knowledge"
  },
  {
    "objectID": "Day1/QC.html",
    "href": "Day1/QC.html",
    "title": "QC",
    "section": "",
    "text": "pwd\n\n\n\ncd /home/ubuntu/Data\n\n\n\nls -lh\n\n\n\nmkdir -p analysis/qc analysis/assembly\n\n\n\ndf -h"
  },
  {
    "objectID": "Day1/QC.html#reads-qc",
    "href": "Day1/QC.html#reads-qc",
    "title": "QC",
    "section": "Reads QC",
    "text": "Reads QC\nIn this part of the exercise, we will use a programme called FastQC.\nFastQC aims to provide a simple way to do some quality control checks on raw sequence Data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your Data has any problems of which you should be aware before doing any further analysis.\nThe main functions of FastQC are\n\nImport of Data from BAM, SAM or FastQ files (any variant)\nProviding a quick overview to tell you in which areas there may be problems\nSummary graphs and tables to quickly assess your Data\nExport of results to an HTML based permanent report\n\n\nRun FastQC programatically\nTo run non-interactively you simply have to specify a list of files to process on the command line.¬†\nEg:\nfastqc somefile.txt someotherfile.txt\nfastqc /home/ubuntu/Data/*/*/*\n\n\nQC your fastq reads through the visual interface\nNavigate to /home/ubuntu/Software/FastQC and double click on the FastQC icon.\nIn the visual interface, open all files produced by FastQC and assess the results, eg ERR4635696_1_fastqc.html\nVisualize and interpret the FastQC results. Compare the QC results for Nanopore and Illumina sequence reads\nOptional: Unzip the files /home/ubuntu/Data/*/*/*_*_fastqc.zip and produce a script that extracts the information on reads lengths, GC contents etc; plot the results from all samples.\n\n\n\nFastQC Illumina reads\n\n\n\n\n\nFastQC Nanopore reads\n\n\n\n\nGenerate MultiQC report (aggregates all FastQC results)\nMultiQC looks for the .zip files produced by FastQC (not the .html files)\ncd /home/ubuntu/Data/QC\nmultiqc ERR4784794\nOpen in browser:\nNavigate to /home/ubuntu/Data/QC and double-click multiqc_report.html\nKey metrics to check:\nPer base sequence quality (should be &gt;28 for most bases)\nPer sequence quality scores (peak should be &gt;30)\nAdapter content (should be minimal)\nGC content (should match expected ~33% for S. aureus)\n\n\nRead Trimming/Filtering\n\n\nUsing fastp (fast and comprehensive)\nEg:\nfastp -i ERR4635696_1.fastq.gz -I ERR4635696_2.fastq.gz -o ERR4635696_1.clean.fastq.gz -O ERR4635696_2.clean.fastq.gz --qualified_quality_phred 20 --length_required 50 --detect_adapter_for_pe --html sample1_fastp.html --json sample1_fastp.json --thread 4\n\n\nRe-run MultiQC to confirm improvement\ncd /home/ubuntu/Data/QC\nmultiqc ERR4635696\nmultiqc ERR4635696_clean\nDecision point: If &gt;80% reads pass filtering and mean quality &gt;Q30, proceed to assembly."
  },
  {
    "objectID": "module8/module8.html",
    "href": "module8/module8.html",
    "title": "Module 8",
    "section": "",
    "text": "This exercise used tools developed by The Centre for Genomic Pathogen Surveillance (CGPS) at University of Oxford\nSimilar exercises have been developed as part of T3Connect https://wcscourses.github.io/T3connectResources/\n\nFor Microreact documentation, go to https://docs.microreact.org/\n\n\nIn this series of exercises, you will use Microreact to gain knowledge regarding a large collection of Klebsiella pneumoniae isolates, sampled from Colombia, between 2013 and 2019. At the end of this series of exercises, you will be able to identify a high risk clone circulating in the country.\n\n\n\nTask 1: Create an editable project.\nTask 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.¬†\nTask 3: What are the dominating sequence types (STs) in Colombia?\nTask 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\nTask 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nThis is a screenshot of a Microreact project called ‚ÄúKlebsiella pneumoniae Colombia‚Äù. Notice the main panels in this project: the map, the tree, the timeline and the saved views."
  },
  {
    "objectID": "module8/module8.html#data-interpretation",
    "href": "module8/module8.html#data-interpretation",
    "title": "Module 8",
    "section": "",
    "text": "This exercise used tools developed by The Centre for Genomic Pathogen Surveillance (CGPS) at University of Oxford\nSimilar exercises have been developed as part of T3Connect https://wcscourses.github.io/T3connectResources/\n\nFor Microreact documentation, go to https://docs.microreact.org/\n\n\nIn this series of exercises, you will use Microreact to gain knowledge regarding a large collection of Klebsiella pneumoniae isolates, sampled from Colombia, between 2013 and 2019. At the end of this series of exercises, you will be able to identify a high risk clone circulating in the country.\n\n\n\nTask 1: Create an editable project.\nTask 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.¬†\nTask 3: What are the dominating sequence types (STs) in Colombia?\nTask 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\nTask 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nThis is a screenshot of a Microreact project called ‚ÄúKlebsiella pneumoniae Colombia‚Äù. Notice the main panels in this project: the map, the tree, the timeline and the saved views."
  },
  {
    "objectID": "module8/module8.html#task-1-create-an-editable-project.",
    "href": "module8/module8.html#task-1-create-an-editable-project.",
    "title": "Module 8",
    "section": "Task 1: Create an editable project.",
    "text": "Task 1: Create an editable project.\n\n\n\nTask 1: Create an editable project. Step 1: Notice the crossed out pen on right top corner of your screen. This indicates that you cannot edit the current Microreact project.\n\n\n\n\n\nTask 1: Create an editable project. Step 2: Click the crossed out ‚ÄúPen‚Äù symbol in the top right of the screen. A window appears asking you to ‚ÄúSIGN IN TO EDIT‚Äù.\n\n\n\n\n\nTask 1: Create an editable project. Step 3: Once you sign in, the message changes, and you are invited to ‚ÄúMAKE A COPY‚Äù of the current project. Make a copy, to proceed.\n\n\n\n\n\nTask 1: Create an editable project. Step 4: Notice that, once you made a copy of the project, the crossed out ‚Äúpen‚Äù symbol will change to a ‚Äúnormal pen‚Äù, and you will be able to edit and save the project."
  },
  {
    "objectID": "module8/module8.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "href": "module8/module8.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "title": "Module 8",
    "section": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.",
    "text": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 1: Select the ‚ÄúKpn Colombia‚Äù view. Click on the ‚ÄúPen‚Äù symbol on the top right menu. Click on the ‚ÄúCreate New Chart‚Äù\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 2: Drag the new chart to overlap with the tree.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 3: An empty panel for the new chart will be shown on top of the tree panel.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 4: In the Chart Type dropdown list select ‚ÄúBar Chart‚Äù.\n\n\n\n\n\nTask2: Present WGS QC statistics in a chart. Step 5: A new window appears. In the X Axis Column, select ‚ÄúWGS_QC_no_contigs‚Äù and for ‚ÄúMaximum number of bins‚Äù select 10.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 6: The bar chart will look like above. Observe that most genomes have less the 100 contigs.\n\n\n\nTask 3: What are the dominating sequence types (STs) in Colombia?\n\n\n\nTask 3: What are the dominating STs in Colombia? Now that you‚Äôve created one chart, you can create another one! Step 1: Go to the ‚ÄúPen: symbol on the right hand side and click on the‚ÄùCreate New Chart‚Äù.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 2: The new chart can stay right on top of the previously created one.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 3: Notice a white canvas on top of the previously generated chart.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 4: Once again, from the Chart Type dropdown menu, select ‚ÄúBar Chart‚Äù, and when the new view shows up on the ‚ÄúX Axis Column‚Äù, select ST.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 5: A new chart will appear. The labels on the x-axis appear squished and they are hard to read. Drag the panel divider on the left hand side of the chart, to increase the width of the panel.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 6: The information on the x-axis should be readable now. The 3 most abundant STs are ST11, ST258 and ST512.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 7: Click the ‚ÄúViews‚Äù panel on the left hand side, hover over ‚ÄúKpn Colombia‚Äù, click on the three dots on the corner of the view and hit ‚ÄúUpdate View‚Äù\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 8: Go to the Save icon on the right corner, press the icon and choose ‚ÄúUpdate This Project‚Äù\n\n\n\n\nTask 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 1: Press the icon on the top right of the tree panel.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 2: In the Metadata blocks dropdown list, tick all boxes containing KPC, NDM, VIM, OXA and CTX-M-15 genes.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 3: The tree panel will show metadata columns. Yellow indicates presence of a certain gene and green indicates absence. In the panel shown here, it appears that the most common genes present are KPC-2, KPC-3, NDM-1 and CTX-M-15.\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 1: Go to the ‚ÄúMetadata blocks‚Äù and check the ST box.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 2: Observe the new metadata column next to the tree, with the header ‚ÄúST‚Äù.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? In the tree with added metadata blocks, we can observe a large brown block of isolates belonging to ST258. You will need to check the Legend on the very right of Microreact.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? On a close look, we can observe that ST258 is associated with the presence of the carbapenamase gene KPC-3.\n\n\nMicroreact demo link\nData taken from¬†a series of articles published in 2021 in the journal Clinical Infectious Diseases:\n\nGlobal collection article\n\nPhilippines article\nIndia article\nNigeria article\nColombia article"
  },
  {
    "objectID": "module1/module1.html",
    "href": "module1/module1.html",
    "title": "Module 1",
    "section": "",
    "text": "Module 1: Set up your bioinformatics working environment\n\n\nPart 1: Connect to your Amazon EC2 instance via ssh\nBefore you start, make sure you received the IPv4 address of the virtual machine and your private key to connect to the machine from your course coordinator.\nThe virtual machines provided during this course are intended exclusively for use with the course material.\n\nUsing a Mac\nOpen your Terminal application on your local machine.\nThen type\nchmod 400 /local/path/to/student.pem\nssh -i /local/path/to/key.pem¬† ubuntu@xxx.xxx.xxx.xxx\nReplace the ‚Äúxxx.xxx.xxx.xxx‚Äù with the Public IPv4 of your Amazon instance and the /local/path/to/student.pem with the local path to your student.pem file that was communicated to you previously.\nFor more information on how to connect to your virtual machine, access https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-linux-inst-ssh.html .\n\n\nUsing a Windows machine\nSee https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-linux-inst-from-windows.html\n\n\nHow to connect to your Amazon EC2 instance via your browser (during the course)\nType the Public IPv4 in your browser address bar, eg: https://xxx.xxx.xxx.xxx/\nType in the username ubuntu, and the password provided to you by the course instructor.\nOnce you log in, you should see the welcome screen.\nPlease reject any invitation to update the system.\n\n\nHow to modify your $PATH variable\nOnce you are connected to your Amazon EC2 Ubuntu instance, try to view and edit your configuration files.¬†\nCheck the configuration files:\nls -a ~/.\nEdit the .profile file and add the paths in the PATH variable\nEg:\nvi ~/.profile\nPATH=\"/home/ubuntu/Software/mash-Linux64-v2.3:$PATH\"\n\n\n\nPart 2: Configure your Amazon EC2 Ubuntu instance\nYour Amazon EC2 Ubuntu instance should be ready to go right away. However, in order to go through the next modules of this course, you will need to install the list of tools below.\nFirst, familiarise yourselves with the tools. Understand what is their role in an NGS pipeline, what is the required input and the expected output. Who developed them and when? Do they have dependencies?\nFor each tool that you attempt to install, write down the steps you took to achieve that in the Shared student observations file, which you have received from your course coordinator.\n\nFastQC\nhttps://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n\n\nBactinspector\nhttps://gitlab.com/antunderwood/bactinspector\n\n\nVelvet assembler\nhttps://github.com/dzerbino/velvet\n\n\n\nACT and Artemis\nhttps://www.sanger.ac.uk/tool/artemis-comparison-tool-act/\n\n\n\nUnicycler¬†\nhttps://github.com/rrwick/Unicycler?tab=readme-ov-file#build-and-run-without-installation\n\n\n\nSPADES\nhttps://github.com/ablab/spades\n\n\n\nQuast\nhttps://github.com/ablab/quast\n\n\n\nbrew\nhttps://docs.brew.sh/Homebrew-on-Linux\n\n\n\nmakeblastdb and tblastn\nhttps://www.ncbi.nlm.nih.gov/books/NBK569861/\nprokka\nhttps://github.com/tseemann/prokka\n\n\n\nresfinder\nhttps://github.com/cadms/resfinder\n\nARIBA\nhttps://sanger-pathogens.github.io/ariba/ \nProkka\nhttps://github.com/tseemann/prokka\ndocker pull staphb/prokka:latest\ndocker run staphb/prokka:latest prokka -h"
  }
]