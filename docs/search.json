[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "The materials provided in this repository are FREE to use.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement.\n\nStarts 25 November 2024\nFree online course: ùóßùóøùóÆùó∂ùóª ùòÅùóµùó≤ ùóßùóøùóÆùó∂ùóªùó≤ùóø: ùóóùó≤ùòÄùó∂ùó¥ùóª ùóöùó≤ùóªùóºùó∫ùó∂ùó∞ùòÄ ùóÆùóªùó± ùóïùó∂ùóºùó∂ùóªùó≥ùóºùóøùó∫ùóÆùòÅùó∂ùó∞ùòÄ ùóßùóøùóÆùó∂ùóªùó∂ùóªùó¥ https://www.futurelearn.com/courses/train-the-trainer-design-genomics-training Interactive exercises delivered via FutureLearn."
  },
  {
    "objectID": "module6/module6.html",
    "href": "module6/module6.html",
    "title": "Module 6",
    "section": "",
    "text": "ChatGPT for bioinformatics\nThis will work with a Pro subscription.\nThe purpose of this demo is to show how to merge CSV files produced by ARIBA Resfinder and Pathogenwatch, using ChatGPT.\nThe input files can be found here: https://github.com/monicaiabrudan/bacterial-genomics/tree/main/module6/input\nAnd the output files can be found here: https://github.com/monicaiabrudan/bacterial-genomics/tree/main/module6/output\nUpload the CSV files\n\nPrompt ChatGPT to merge the three files.\n\nThe three files have different number of rows!\n\nSelect only isolates that are present in all files, and are resistance to Ciprofloxacin.\n\nThe selection of Ciprofloxacin resistant isolates is based on the column ‚ÄúCiprofloxacin‚Äù in the AMR profiles files.\nChatGPT attempts to explain the genes that are responsible to Ciprofloxacin resistance, however, these are all wrong"
  },
  {
    "objectID": "module7/module7.html",
    "href": "module7/module7.html",
    "title": "Module 7",
    "section": "",
    "text": "What is MLST?\n\nProposed by Maiden et al.¬†in 1998.\nPopular genotypic method in taxonomy classification.\nIdentifies allelic variants to characterize, subtype, and classify the members of a haploid bacterial population.\n\n\n\nHow does MLST work?\n\nConsist of sequencing several different housekeeping genes (generally 7) from an organism and comparing their sequences with the sequences of the same genes from different strains of the same organism.\nFor each gene, an approximately 45‚Äì500 bp sequence is amplified using PCR and then sequenced by Sanger‚Äôs method.\nEach nucleotide along the sequence is compared, and differences are noted.\nEach difference or sequence variant is called an allele and is assigned a number. The strain being studied is then assigned a series of numbers as its allelic profile or multilocus sequence types.\nStrains with identical sequences for a given gene have the same allele for that gene, and two strains with identical sequences for all the genes have the same allelic profile.\nThe relationship between each allelic profile is expressed in a dendrogram of linkage distance that varies.\nCombination of variants = Sequence Type (ST)\nNumber of differences is ignored; a new identifier is given for every possible variant\nWith 30 alleles per locus: ~20 billion possibilities\nDatabase with observed combinations\n\n\n\n\n\n\nUsing MLST to Study Bacterial Variation: Prospects in the Genomic Era, 2014, Future Microbiology\n\n\nMore general information on MLST can be found here\n\n\nMLST with ARIBA\nThe manual can be found here: https://github.com/sanger-pathogens/ariba/wiki/MLST-calling-with-ARIBA\nARIBA can be used for MLST using the typing schemes from PubMLST. A list of available species can be obtained by running\nariba pubmlstspecies\nDownload the data (in this example, Staphylococcus aureus) using pubmlstget:\nariba pubmlstget \"Staphylococcus aureus\" get_mlst\nHow to run ARIBA for one sample\nariba run get_mlst/ref_db /home/ubuntu/Data/all_fastqs/ERR017200_1.fastq.gz /home/ubuntu/Data/all_fastqs/ERR017200_2.fastq.gz MLST\n\nWrite a script to run ARIBA MLST on all samples."
  },
  {
    "objectID": "Day1/Assembly.html",
    "href": "Day1/Assembly.html",
    "title": "Assembly",
    "section": "",
    "text": "SPAdes Assembly (Single K-mer)\n\nBasic assembly with automatic k-mer selection\nspades.py-1 sample1_1.clean.fastq.gz-2 sample1_2.clean.fastq.gz-o /home/ubuntu/analysis/assembly/sample1_spades_auto--careful-t 4-m 8\n\n\nView assembly statistics\ncat /home/ubuntu/analysis/assembly/sample1_spades_auto/scaffolds.fasta\n\n\n\nSPAdes Assembly (Specific K-mer)\n\nAssembly with specific k-mer value\nspades.py-1 sample1_1.clean.fastq.gz-2 sample1_2.clean.fastq.gz-k 21,33,55,77-o /home/ubuntu/analysis/assembly/sample1_spades_k77--careful-t 4-m 8\nK-mer selection guidelines:\n\nSmaller k-mers (21-33): better for low coverage, shorter contigs\nLarger k-mers (55-99): better for high coverage, more contiguous\nMultiple k-mers: SPAdes uses all and picks best\n\n\n\n\nUnicycler Hybrid Assembly\n\nHybrid assembly (short + long reads)\nunicycler-1 sample1_1.clean.fastq.gz-2 sample1_2.clean.fastq.gz-l sample1_nanopore.fastq.gz-o /home/ubuntu/analysis/assembly/sample1_unicycler-t 4\n\n\nShort-read only mode (for comparison)\nunicycler-1 sample1_1.clean.fastq.gz-2 sample1_2.clean.fastq.gz-o /home/ubuntu/analysis/assembly/sample1_unicycler_short-t 4\n\n\n\nAssembly Quality Assessment\n\nQUAST - comprehensive quality metrics\nquast.py/home/ubuntu/analysis/assembly/sample1_spades_auto/scaffolds.fasta/home/ubuntu/analysis/assembly/sample1_spades_k77/scaffolds.fasta/home/ubuntu/analysis/assembly/sample1_unicycler/assembly.fasta-o /home/ubuntu/analysis/assembly/quast_comparison-r /home/ubuntu/Reference/Saureus_reference.fasta-g /home/ubuntu/Reference/Saureus_reference.gff--threads 4\n\n\nView report\nfirefox /home/ubuntu/analysis/assembly/quast_comparison/report.html\nKey QUAST metrics:\nNumber of contigs: Lower is better (fewer breaks)\nLargest contig: Should be ~2-3 Mb for chromosome\nTotal length: Should be ~2.8 Mb for S. aureus\nN50: Higher is better (longer contigs)\nL50: Lower is better (fewer contigs contain 50% of assembly)\n\n\nBUSCO - check genome completeness\nbusco-i /home/ubuntu/analysis/assembly/sample1_unicycler/assembly.fasta-o sample1_busco-m genome-l bacteria_odb10--cpu 4\nBUSCO interpretation:\nComplete: &gt;95% is excellent\nFragmented: &lt;5% is good\nMissing: &lt;5% is acceptable\n\n\nQuick Assembly Statistics\n\n\nSimple assembly stats script\nassembly_stats() { echo \"Assembly: $1\" echo \"Number of contigs:\" grep -c \"&gt;\" $1 echo \"Total length:\" grep -v \"&gt;\" $1 | tr -d '\\n' | wc -c echo \"Largest contig:\" grep -v \"&gt;\" $1 | awk '{print length}' | sort -rn | head -1 }\n\n\nUse the function\nassembly_stats /home/ubuntu/analysis/assembly/sample1_spades_auto/scaffolds.fasta\n\n\n\nVisualize Assembly Graph (Optional)\n\nView assembly graph in Bandage\nBandage load /home/ubuntu/analysis/assembly/sample1_spades_auto/assembly_graph.fastg\nComparing Multiple Assemblies\nCreate a comparison table\necho -e \"Assembly\\tContigs\\tTotal_Length\\tN50\\tLargest_Contig\" &gt; assembly_comparison.txt\nfor assembly in /home/ubuntu/analysis/assembly/*/scaffolds.fasta;\ndo name=$(basename $(dirname $assembly))     contigs=$(grep -c \"&gt;\" $assembly)     total=$(grep -v \"&gt;\" $assembly | tr -d '\\n' | wc -c)     echo -e \"$name\\t$contigs\\t$total\\t-\\t-\" &gt;&gt; assembly_comparison.txt done\ncolumn -t assembly_comparison.txt\n\n\nTroubleshooting\nIssue: SPAdes fails with memory error: Reduce memory usage by using ‚Äìmemory flag\nspades.py -1 R1.fastq.gz -2 R2.fastq.gz -o output --careful -t 2 -m 4\nIssue: Too many contigs (&gt;200)\nCoverage might be too low; Try larger k-mer values; Check for contamination Issue:\nAssembly too small/large\nCheck reference genome size\nVerify species identification with Mash Look for contamination"
  },
  {
    "objectID": "Compare_AMR_files.html",
    "href": "Compare_AMR_files.html",
    "title": "Bioinformatic methods for bacterial genomics, Cluj-Napoca, October 2025",
    "section": "",
    "text": "import pandas as pd\nfrom google.colab import files\nimport io\n\n# Upload files\nresfinder_file = files.upload()\n\n# Load the files\n# Check the uploaded file name\nprint(\"Uploaded files:\", resfinder_file.keys())\n\n# Use the correct file name from the printed keys\nfilename = list(resfinder_file.keys())[0]  # Automatically use the first uploaded file\ndf_res = pd.read_csv(io.StringIO(resfinder_file[filename].decode('utf-8')))\n\n# Display the first few rows\nprint(df_res.head())\n\n\npw_amr_file = files.upload()\n\n# Load the files\n# Check the uploaded file name\nprint(\"Uploaded files:\", pw_amr_file.keys())\n\n# Use the correct file name from the printed keys\nfilename = list(pw_amr_file.keys())[0]  # Automatically use the first uploaded file\ndf_pw = pd.read_csv(io.StringIO(pw_amr_file[filename].decode('utf-8')))\n\n# Display the first few rows\nprint(df_pw.head())\n\n\n# Clean Resfinder gene names\nresfinder_genes = {gene.replace('.match', '') for gene in df_res.columns}\nresfinder_genes2 = [w.replace('_', '') for w in resfinder_genes]\nprint (resfinder_genes2)\n# Get PW AMR gene names\npw_amr_genes = set(df_pw .columns)\n\n# Find substrings\nsubstr_in_resfinder = {gene for gene in pw_amr_genes if any(gene in res_gene for res_gene in resfinder_genes2)}\n\nprint(\"PW AMR genes that are substrings of Resfinder genes:\", substr_in_resfinder)\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving ARIBA_resfinder.csv to ARIBA_resfinder (6).csv\nUploaded files: dict_keys(['ARIBA_resfinder (6).csv'])\n        name aadD.match aph_2____Ia+.match blaZ_1.match bleO.match  \\\n0  ERR017200         no                 no          yes         no   \n1  ERR030279         no                 no          yes         no   \n2  ERR031664         no                 no          yes         no   \n3  ERR033465         no                 no          yes         no   \n4  ERR033475         no                 no          yes         no   \n\n  cat_pC221_.match erm_C_.match mecA-.match  \n0               no           no         yes  \n1               no          yes         yes  \n2               no          yes         yes  \n3               no           no         yes  \n4               no          yes         yes  \n\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving PW_AMR_genes.csv to PW_AMR_genes (5).csv\nUploaded files: dict_keys(['PW_AMR_genes (5).csv'])\n         NAME  aadD  aacA-aphD  mecA  blaZ  ermC  ermC_LP  dfrA\n0  SRR7653488     0          0     1     1     1        1     1\n1  SRR7653497     0          1     1     1     0        0     0\n2  ERR4635593     1          1     1     1     1        1     1\n3  ERR4783604     0          1     1     1     0        0     1\n4   ERR033465     0          0     1     1     0        0     0\n['ermC', 'name', 'mecA-', 'bleO', 'catpC221', 'aadD', 'blaZ1', 'aph2Ia+']\nPW AMR genes that are substrings of Resfinder genes: {'blaZ', 'aadD', 'ermC', 'mecA'}"
  },
  {
    "objectID": "agenda.html",
    "href": "agenda.html",
    "title": "Agenda",
    "section": "",
    "text": "Prerequisites for following this course.\n\nYou are expected to be comfortable with bash and also with a text editor such as vi or nano. If you are not, try to practice a few commands before you attempt to follow the steps below.\nThere are numerous online introductory tutorials to the UNIX/Linux operating system and command line, including:\nhttp://www.ee.surrey.ac.uk/Teaching/Unix\nhttp://swcarpentry.github.io/shell-novice/\nPlease make sure you have received from your course instructor the Public IPv4 of your Amazon EC2 instance, the name of the machine you are going to work on and also a public key to access the machine via ssh."
  },
  {
    "objectID": "data-flo/data-flo.html",
    "href": "data-flo/data-flo.html",
    "title": "Data-flo tutorial",
    "section": "",
    "text": "Data-flo (https://data-flo.io/) is a system for customised integration and manipulation of diverse data via a simple drag and drop interface.\nFor Data-flo documentation, go to https://docs.data-flo.io/introduction/readme\nData-flo can easily combine epidemiological data, genomic data, laboratory data, and various metadata from disparate sources (i.e., different data systems) and formats.\nData-flo provides a visual method to design a reusable pipeline to integrate, clean, and manipulate data in a multitude of ways, eliminating the need for continuous manual intervention (e.g., coding, formatting, spreadsheet formulas, manual copy-pasting).\nData-flo pipelines are combinations of ready-to-use data adaptors that can be tailored, modularised and shared for reuse and reproducibility. Once a Data-flo pipeline has been created, it can be run anytime, by anyone with access, to enable push-button data extraction and transformation. This saves significant time by removing the bulk of the manual repetitive workflows that require multiple sequential or tedious steps, enabling practitioners to focus on analysis and interpretation.\n\n\n‚ÄúSteps‚Äù in Data-flo are called ADAPTORS. There are three main types of adaptors, which serve different functions.\n\nImporting data\nManipulating & transforming data\nExporting data\n\n\n\n\nData-flo editing view. Examples of Data-flo adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo that imports data from Google spreadsheets, through ‚ÄúGoogle spreadsheet‚Äù adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo where two datatables are merged using a ‚ÄúJoin datatables‚Äù adaptor.\n\n\n\n\n\nData-flo editing view. Using various adaptors, columns in a datatable can be removed or renamed, specific strings or blank values can be replaced, dates can be reformatted, distinct lists can be generated.\n\n\n\n\n\nData-flo editing view. In this example, a Data-flo pushes updated data to a Microreact project and supplies the URL for that project using the ‚ÄúUpdate Microreact project‚Äù adaptor.\n\n\n\n\n\n\n\n\nHow to run a Data-flo, Step 1: Go to Data-flo transformations: data-flo.io/transformations/dataflows\n\n\n\n\n\nHow to run a Data-flo, Step 2: Select your favourite Data-flo. The Data-flo shown in this example is ‚ÄúLab to bioinformatics‚Äù\n\n\n\n\n\nHow to run a Data-flo, Step 3: Click on the RUN tab to get to the RUN page.\n\n\n\n\n\nHow to run a Data-flo, Step 4: Hit the RUN button.\n\n\n\n\n\nHow to run a Data-flo, Step 5: Check out the output. In this example, the output is a Microreact link. You can choose to RUN AGAIN the Data-flo, and this will update your results (in case the input had been changed, of course).\n\n\n\n\n\nCreate a Data-flo from scratch, Step 1: Go to the + sign on the bottom right of your screen.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 2: Select ‚ÄúNEW DATAFLOW‚Äù.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 3: Select your preferred ACCESS CONTROL.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 4: Add adaptors to your Data-flo. On the left-hand side of Data-flo, you can find the list of available adaptors.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 5: Try to retrieve data from a spreadsheet. Click on ‚ÄúSpreadsheet file‚Äù on the list of adaptors. Hover over the ‚Äúi‚Äù button next to ‚ÄúSpreadsheet file‚Äù to see basic information about this adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 6: Clicking on ‚ÄúSpreadsheet file‚Äù on the list of adaptors, will add an adaptor (which looks like a box), with the title ‚ÄúSpreadsheet file‚Äù on your canvas. When you click on *file on the left side of the adaptor, the view on the right hand side of your screen will change, and you will be able to see a list of options under ‚ÄúBINDING TYPE‚Äù, such as ‚ÄúBind to a Dataflow input‚Äù and ‚ÄúBind to another transformation‚Äù.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 7: From the list of ‚ÄúBINDING TYPEs‚Äù, Select ‚ÄúBind to a Dataflow input‚Äù and press the ‚Äú+‚Äù next to INPUT ARGUMENT.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 8: A box with ‚Äúfile‚Äù will appear on the canvas, linked to the ‚ÄúSpreadsheet file‚Äù adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 9: To run a Data-flo, press the little ‚Äúbug icon‚Äù on top, on the right hand side of the Save button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 10: Pressing the ‚Äúbug icon‚Äù will trigger the Dataflow Debugger and you will be shown a ‚ÄúChoose file‚Äù button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 11: Press the ‚ÄúChoose file‚Äù button and select a spreadsheet from your local computer. Click the ‚Äúdata‚Äù on the right hand side of the adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 12: Clicking on the ‚Äúdata‚Äù on the right hand side of the adaptor, will retrieve a view of the file selected. In this case, the Dataflow Debugger is showing the first 3 rows of a file containing epidemiological data.\n\n\n\n\n\n\n\n\nHow to share your Data-flo, Step 1: Press the downward facing arrow shown here on the top right corner. This will trigger the export of a .json file, which can be sent and shared.\n\n\n\n\n\nHow to share your Data-flo, Step 2: If you have received a .json Data-flo file, press the + sign at the bottom right of your screen\n\n\n\n\n\nHow to share your Data-flo, Step 3: Press the IMPORT button and then select your .json file containing the Data-flo."
  },
  {
    "objectID": "data-flo/data-flo.html#a-short-introduction",
    "href": "data-flo/data-flo.html#a-short-introduction",
    "title": "Data-flo tutorial",
    "section": "",
    "text": "Data-flo (https://data-flo.io/) is a system for customised integration and manipulation of diverse data via a simple drag and drop interface.\nFor Data-flo documentation, go to https://docs.data-flo.io/introduction/readme\nData-flo can easily combine epidemiological data, genomic data, laboratory data, and various metadata from disparate sources (i.e., different data systems) and formats.\nData-flo provides a visual method to design a reusable pipeline to integrate, clean, and manipulate data in a multitude of ways, eliminating the need for continuous manual intervention (e.g., coding, formatting, spreadsheet formulas, manual copy-pasting).\nData-flo pipelines are combinations of ready-to-use data adaptors that can be tailored, modularised and shared for reuse and reproducibility. Once a Data-flo pipeline has been created, it can be run anytime, by anyone with access, to enable push-button data extraction and transformation. This saves significant time by removing the bulk of the manual repetitive workflows that require multiple sequential or tedious steps, enabling practitioners to focus on analysis and interpretation.\n\n\n‚ÄúSteps‚Äù in Data-flo are called ADAPTORS. There are three main types of adaptors, which serve different functions.\n\nImporting data\nManipulating & transforming data\nExporting data\n\n\n\n\nData-flo editing view. Examples of Data-flo adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo that imports data from Google spreadsheets, through ‚ÄúGoogle spreadsheet‚Äù adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo where two datatables are merged using a ‚ÄúJoin datatables‚Äù adaptor.\n\n\n\n\n\nData-flo editing view. Using various adaptors, columns in a datatable can be removed or renamed, specific strings or blank values can be replaced, dates can be reformatted, distinct lists can be generated.\n\n\n\n\n\nData-flo editing view. In this example, a Data-flo pushes updated data to a Microreact project and supplies the URL for that project using the ‚ÄúUpdate Microreact project‚Äù adaptor.\n\n\n\n\n\n\n\n\nHow to run a Data-flo, Step 1: Go to Data-flo transformations: data-flo.io/transformations/dataflows\n\n\n\n\n\nHow to run a Data-flo, Step 2: Select your favourite Data-flo. The Data-flo shown in this example is ‚ÄúLab to bioinformatics‚Äù\n\n\n\n\n\nHow to run a Data-flo, Step 3: Click on the RUN tab to get to the RUN page.\n\n\n\n\n\nHow to run a Data-flo, Step 4: Hit the RUN button.\n\n\n\n\n\nHow to run a Data-flo, Step 5: Check out the output. In this example, the output is a Microreact link. You can choose to RUN AGAIN the Data-flo, and this will update your results (in case the input had been changed, of course).\n\n\n\n\n\nCreate a Data-flo from scratch, Step 1: Go to the + sign on the bottom right of your screen.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 2: Select ‚ÄúNEW DATAFLOW‚Äù.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 3: Select your preferred ACCESS CONTROL.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 4: Add adaptors to your Data-flo. On the left-hand side of Data-flo, you can find the list of available adaptors.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 5: Try to retrieve data from a spreadsheet. Click on ‚ÄúSpreadsheet file‚Äù on the list of adaptors. Hover over the ‚Äúi‚Äù button next to ‚ÄúSpreadsheet file‚Äù to see basic information about this adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 6: Clicking on ‚ÄúSpreadsheet file‚Äù on the list of adaptors, will add an adaptor (which looks like a box), with the title ‚ÄúSpreadsheet file‚Äù on your canvas. When you click on *file on the left side of the adaptor, the view on the right hand side of your screen will change, and you will be able to see a list of options under ‚ÄúBINDING TYPE‚Äù, such as ‚ÄúBind to a Dataflow input‚Äù and ‚ÄúBind to another transformation‚Äù.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 7: From the list of ‚ÄúBINDING TYPEs‚Äù, Select ‚ÄúBind to a Dataflow input‚Äù and press the ‚Äú+‚Äù next to INPUT ARGUMENT.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 8: A box with ‚Äúfile‚Äù will appear on the canvas, linked to the ‚ÄúSpreadsheet file‚Äù adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 9: To run a Data-flo, press the little ‚Äúbug icon‚Äù on top, on the right hand side of the Save button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 10: Pressing the ‚Äúbug icon‚Äù will trigger the Dataflow Debugger and you will be shown a ‚ÄúChoose file‚Äù button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 11: Press the ‚ÄúChoose file‚Äù button and select a spreadsheet from your local computer. Click the ‚Äúdata‚Äù on the right hand side of the adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 12: Clicking on the ‚Äúdata‚Äù on the right hand side of the adaptor, will retrieve a view of the file selected. In this case, the Dataflow Debugger is showing the first 3 rows of a file containing epidemiological data.\n\n\n\n\n\n\n\n\nHow to share your Data-flo, Step 1: Press the downward facing arrow shown here on the top right corner. This will trigger the export of a .json file, which can be sent and shared.\n\n\n\n\n\nHow to share your Data-flo, Step 2: If you have received a .json Data-flo file, press the + sign at the bottom right of your screen\n\n\n\n\n\nHow to share your Data-flo, Step 3: Press the IMPORT button and then select your .json file containing the Data-flo."
  },
  {
    "objectID": "microreact/microreact.html",
    "href": "microreact/microreact.html",
    "title": "Microreact tutorial",
    "section": "",
    "text": "For Microreact documentation, go to https://docs.microreact.org/"
  },
  {
    "objectID": "microreact/microreact.html#task-1-create-an-editable-project.",
    "href": "microreact/microreact.html#task-1-create-an-editable-project.",
    "title": "Microreact tutorial",
    "section": "Task 1: Create an editable project.",
    "text": "Task 1: Create an editable project.\n\n\n\nTask 1: Create an editable project. Step 1: Notice the crossed out pen on right top corner of your screen. This indicates that you cannot edit the current Microreact project.\n\n\n\n\n\nTask 1: Create an editable project. Step 2: Click the crossed out ‚ÄúPen‚Äù symbol in the top right of the screen. A window appears asking you to ‚ÄúSIGN IN TO EDIT‚Äù.\n\n\n\n\n\nTask 1: Create an editable project. Step 3: Once you sign in, the message changes, and you are invited to ‚ÄúMAKE A COPY‚Äù of the current project. Make a copy, to proceed.\n\n\n\n\n\nTask 1: Create an editable project. Step 4: Notice that, once you made a copy of the project, the crossed out ‚Äúpen‚Äù symbol will change to a ‚Äúnormal pen‚Äù, and you will be able to edit and save the project."
  },
  {
    "objectID": "microreact/microreact.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "href": "microreact/microreact.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "title": "Microreact tutorial",
    "section": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.",
    "text": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 1: Select the ‚ÄúKpn Colombia‚Äù view. Click on the ‚ÄúPen‚Äù symbol on the top right menu. Click on the ‚ÄúCreate New Chart‚Äù\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 2: Drag the new chart to overlap with the tree.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 3: An empty panel for the new chart will be shown on top of the tree panel.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 4: In the Chart Type dropdown list select ‚ÄúBar Chart‚Äù.\n\n\n\n\n\nTask2: Present WGS QC statistics in a chart. Step 5: A new window appears. In the X Axis Column, select ‚ÄúWGS_QC_no_contigs‚Äù and for ‚ÄúMaximum number of bins‚Äù select 10.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 6: The bar chart will look like above. Observe that most genomes have less the 100 contigs."
  },
  {
    "objectID": "microreact/microreact.html#task-3-what-are-the-dominating-sequence-types-sts-in-colombia",
    "href": "microreact/microreact.html#task-3-what-are-the-dominating-sequence-types-sts-in-colombia",
    "title": "Microreact tutorial",
    "section": "Task 3: What are the dominating sequence types (STs) in Colombia?",
    "text": "Task 3: What are the dominating sequence types (STs) in Colombia?\n\n\n\nTask 3: What are the dominating STs in Colombia? Now that you‚Äôve created one chart, you can create another one! Step 1: Go to the ‚ÄúPen: symbol on the right hand side and click on the‚ÄùCreate New Chart‚Äù.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 2: The new chart can stay right on top of the previously created one.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 3: Notice a white canvas on top of the previously generated chart.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 4: Once again, from the Chart Type dropdown menu, select ‚ÄúBar Chart‚Äù, and when the new view shows up on the ‚ÄúX Axis Column‚Äù, select ST.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 5: A new chart will appear. The labels on the x-axis appear squished and they are hard to read. Drag the panel divider on the left hand side of the chart, to increase the width of the panel.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 6: The information on the x-axis should be readable now. The 3 most abundant STs are ST11, ST258 and ST512.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 7: Click the ‚ÄúViews‚Äù panel on the left hand side, hover over ‚ÄúKpn Colombia‚Äù, click on the three dots on the corner of the view and hit ‚ÄúUpdate View‚Äù\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 8: Go to the Save icon on the right corner, press the icon and choose ‚ÄúUpdate This Project‚Äù"
  },
  {
    "objectID": "microreact/microreact.html#task-4-plot-metadata-blocks-for-the-carbapenamase-genes-ctx-m-15-ndm-1-kpc-and-oxa.-what-are-the-prevalent-amr-mechanisms-detected",
    "href": "microreact/microreact.html#task-4-plot-metadata-blocks-for-the-carbapenamase-genes-ctx-m-15-ndm-1-kpc-and-oxa.-what-are-the-prevalent-amr-mechanisms-detected",
    "title": "Microreact tutorial",
    "section": "Task 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?",
    "text": "Task 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 1: Press the icon on the top right of the tree panel.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 2: In the Metadata blocks dropdown list, tick all boxes containing KPC, NDM, VIM, OXA and CTX-M-15 genes.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 3: The tree panel will show metadata columns. Yellow indicates presence of a certain gene and green indicates absence. In the panel shown here, it appears that the most common genes present are KPC-2, KPC-3, NDM-1 and CTX-M-15."
  },
  {
    "objectID": "microreact/microreact.html#task-5-which-sts-are-associated-with-the-presence-of-carbapenamase-genes",
    "href": "microreact/microreact.html#task-5-which-sts-are-associated-with-the-presence-of-carbapenamase-genes",
    "title": "Microreact tutorial",
    "section": "Task 5: Which STs are associated with the presence of carbapenamase genes?",
    "text": "Task 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 1: Go to the ‚ÄúMetadata blocks‚Äù and check the ST box.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 2: Observe the new metadata column next to the tree, with the header ‚ÄúST‚Äù.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? In the tree with added metadata blocks, we can observe a large brown block of isolates belonging to ST258. You will need to check the Legend on the very right of Microreact.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? On a close look, we can observe that ST258 is associated with the presence of the carbapenamase gene KPC-3."
  },
  {
    "objectID": "Assessment/assessment.html",
    "href": "Assessment/assessment.html",
    "title": "Assessment",
    "section": "",
    "text": "More details will be communicated to you via email."
  },
  {
    "objectID": "module2/module2.html",
    "href": "module2/module2.html",
    "title": "Module 2",
    "section": "",
    "text": "Read more here:\nhttps://genome.ucsc.edu/FAQ/FAQformat.html\n\n\n\nFollow the steps in Module 1 to connect to your remote virtual machine.\nThe next exercise is broadly based on the study https://journals.asm.org/doi/full/10.1128/msphere.00185-23 by Abrudan and Shamanna, 2023\n\n\n\nThe European Nucleotide Archive (ENA) provides a comprehensive record of the world‚Äôs nucleotide sequencing information, covering raw sequencing Data, sequence assembly information and functional annotation.\nAccess to ENA Data is provided through the browser, through search tools, through large scale file download and through the API."
  },
  {
    "objectID": "module2/module2.html#reads-qc",
    "href": "module2/module2.html#reads-qc",
    "title": "Module 2",
    "section": "Reads QC",
    "text": "Reads QC\nIn this part of the exercise, we will use a programme called FastQC.\nFastQC aims to provide a simple way to do some quality control checks on raw sequence Data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your Data has any problems of which you should be aware before doing any further analysis.\nThe main functions of FastQC are\n\nImport of Data from BAM, SAM or FastQ files (any variant)\nProviding a quick overview to tell you in which areas there may be problems\nSummary graphs and tables to quickly assess your Data\nExport of results to an HTML based permanent report\n\n\nRun FastQC programatically\nTo run non-interactively you simply have to specify a list of files to process on the command line.¬†\nfastqc somefile.txt someotherfile.txt\nfastqc /home/ubuntu/Data/*/*/*\n\n\nQC your fastq reads through the visual interface\nNavigate to /home/ubuntu/Software/FastQC and double click on the FastQC icon.\nIn the visual interface, open all files produced by FastQC and assess the results, eg ERR4635696_1_fastqc.html\nVisualize and interpret the FastQC results. Compare the QC results for Nanopore and Illumina sequence reads\nOptional: Unzip the files /home/ubuntu/Data/*/*/*_*_fastqc.zip and produce a script that extracts the information on reads lengths, GC contents etc; plot the results from all samples.\n\n\n\nFastQC Illumina reads\n\n\n\n\n\nFastQC Nanopore reads"
  },
  {
    "objectID": "module2/module2.html#speciation",
    "href": "module2/module2.html#speciation",
    "title": "Module 2",
    "section": "Speciation",
    "text": "Speciation\nBactinspector is a package to a) determine the most probable species based on sequence in fasta/fastq files using refseq and Mash (https://mash.readthedocs.io/en/latest/index.html) and b) determine the closest reference in refseq to a set of fasta/fastq files.\n\nGo to sample files /home/ubuntu/Data/ and run Bactinspector using the following command\n\nbactinspector closest_match -fq \"*_2.fastq.gz\"\n\n\nTidy up the output *.tsv file\n\ntail -n +2 *.tsv| column -t | less -S\ncat *.tsv | tr \"\\t\" \"~\" | cut -d\"~\" -f2\nEg of a result:\nASM289538v1\nGCF_002895385\n\n\nHow do you interpret the results? Look up your results in ENA.\nIn your browser, type\nhttps://www.ebi.ac.uk/ena/browser/text-search?query=ASM289538v1\n\n\nCheck fields such as:\n\nAssembly Level\nGenome Representation\nStrain\nCount Contig\nContig N50\nTotal Length\n\nDownload the reference genome from the visual reference\n\nOr from the command line.\nwget https://www.ebi.ac.uk/ena/browser/api/fasta/CP018629.1 -O - &gt;&gt; GCA_002895385-chromosomes.fasta"
  },
  {
    "objectID": "module2/module2.html#genome-assembly",
    "href": "module2/module2.html#genome-assembly",
    "title": "Module 2",
    "section": "Genome assembly",
    "text": "Genome assembly\n\n\n\nAn example of the N50 metric\n\n\nThe Velvet assembler\nVelvet is an algorithm package that has been designed to deal with de novo genome assembly and short read sequencing alignments. This is achieved through the manipulation of de Bruijn graphs for genomic sequence assembly via the removal of errors and the simplification of repeated regions.\nRun Velvet\nFind a value of k (between 21 and 99) to start with, and record your choice.\nvelveth G18252308 KMER -short -separate -fastq /home/ubuntu/Data/G18252308/illumina/ERR4635696_1.fastq.gz /home/ubuntu/Data/G18252308/illumina/ERR4635696_2.fastq.gz\nAfter velveth is finished, look in the new folder that has the name you chose. You should see the following files:\nLog\nRoadmaps\nSequences\nThe ‚ÄôLog‚Äò file has a useful reminder of what commands you typed to get this assembly result, for reproducing results later on. ‚ÄôSequences‚Äò contains the sequences we put in, and ‚ÄôRoadmaps‚Äò contains the index you just created.\nNow we will run the assembly with default parameters:\nvelvetg G18252308\n\nVelvet will end with a text like this:\nFinal graph has ... nodes and n50 of ..., max ..., total ..., using .../... reads\n\nThe number of nodes represents the number of nodes in the graph, which (more or less) is the number of contigs. Velvet reports its N50 (as well as everything else) in ‚Äòkmer‚Äô space. The conversion to ‚Äòbasespace‚Äô is as simple as adding k-1 to the reported length.\nLook again at the folder asm_name, you should see the following extra files:\ncontigs.faGraphLastGraphPreGraphstats.txt\nThe important files are:\ncontigs.fa - the assembly itselfGraph - a textual representation of the contig graphstats.txt - a file containing statistics on each contig\nQuestions\n\nWhat k-mer did you use?\nWhat is the N50 of the assembly?\nWhat is the size of the largest contig?\nHow many contigs are there in the contigs.fa file? Use grep -c NODE contigs.fa\n\nRun again velveth and velvetg with another KMER size.\n\nIn the classroom, decide you got the best N50 and the lowest number of contigs!\nVisualise the assembles using ACT and Artemis\nACT is a Java application for displaying pairwise comparisons between two or more DNA sequences.\nACT can be used to identify and analyse regions of similarity and difference between genomes and to explore conservation of synteny, in the context of the entire sequences and their annotation. It can read complete EMBL, GENBANK and GFF entries or sequences in FASTA or raw format.\nTip: ACT and Artemis are installed here: /usr/share/miniconda/pkgs/artemis-18.2.0-hdfd78af_0/share/artemis-18.2.0-0¬†\nGo the the visual interface of your virtual machine.\nIn the Terminal, type\nart &\nArtemis should open. Load your contigs.fa file.\n\n\n\n\n\nTry View-&gt;Overview in the Artemis menu\n\n\nFrom the Graph menu, open GC Deviation (G-C)/(G+C) by clicking on the button next to it.\nRescale the plot for to a more appropriate window size for this zoomed out view: Right click on the graph, and click Maximum Window Size, and select 20000. Then move the graph slider of the right hand side of the screen down to the bottom of the bar.\n\nFrom the graph you can see that plot generally varies about an upper level and a lower level across the assembly, with shifts occurring at contig boundaries."
  },
  {
    "objectID": "module2/module2.html#assembly-qc-with-quast",
    "href": "module2/module2.html#assembly-qc-with-quast",
    "title": "Module 2",
    "section": "Assembly QC with Quast",
    "text": "Assembly QC with Quast\nQUAST stands for QUality ASsessment Tool. It evaluates genome/metagenome assemblies by computing various metrics. The current QUAST toolkit includes the general QUAST tool for genome assemblies, MetaQUAST, the extension for metagenomic Datasets, QUAST-LG, the extension for large genomes (e.g., mammalians), and Icarus, the interactive visualizer for these tools.\n\nIn your virtual machine, go to the Chrome browser, and navigate to:\nfile:///home/ubuntu/Data/G18252308/illumina/G18252308_quast/report.html\nAlternatively, navigate to /home/ubuntu/Data/G18252308/illumina/G18252308_quast and double click on report.html\n\nWhat is the cumulative length of the assembly? Can you check if this expected for a S. aureus genome? Hint: go back to the reference genome you found previously! What was the length of that assembly?"
  },
  {
    "objectID": "module2/module2.html#assembly-methods-comparison",
    "href": "module2/module2.html#assembly-methods-comparison",
    "title": "Module 2",
    "section": "Assembly methods comparison",
    "text": "Assembly methods comparison\nFor assembly of short (Illumina) reads, we will use Unicycler. With short read data, Unicycler acts as wrapper script for the SPAdes assembler, and will produce an assembly with settings optimised for bacteria.\nIn the command below we:\n\nAllocate 4 CPUs to the assembler\n\n-t 4\n\nSpecify the Illumina forward and reverse reads to use\n\n-1 &lt;fastq&gt; and -2 &lt;fastq&gt;\n\nSpecify a directory to output files to\n\n-o &lt;dir&gt;\n\n\nunicycler -t 4 -1 /home/ubuntu/Data/G18252308/illumina/ERR4635696_1.fastq.gz -2 /home/ubuntu/Data/G18252308/illumina/ERR4635696_2.fastq.gz -o ERR4635696.short\nFor assembly of long reads, Unicycler switches to the long read assembler miniasm - this is a very rapid assembler, but is not particularly accurate, and can often introduce errors and mistakes into its outputs.\n\nIn the command below we:\n\nAllocate 4 CPUs to the assembler\n\n-t 4\n\nSpecify the ONT long reads to use\n\n-l &lt;fastq&gt;\n\nSpecify a directory to output files to\n\n-o &lt;dir&gt;\n\n\nunicycler -t 4 -l /home/ubuntu/Data/G18252308/nanopore/ERR8187245.fastq.gz -o S_aureus_JKD6159.s100.unicycler.long\nFor assembly of both long and short reads, Unicycler uses SPAdes for an initial ‚Äòshort read‚Äô assembly, and then uses the long reads to attempt to bridge the gaps between contigs from the initial SPAdes assembly. This can work well when the short read data is of high quality/depth but the ONT data is of low depth. However, the ‚Äòbridging step‚Äô can be time consuming, and if the short read assembly is poor, the final assembly may also be. If you have high quality and depth long reads, this may not be the best option.\nIn the command below we:\n\nAllocate 4 CPUs to the assembler\n\n-t 4\n\nSpecify the Illumina forward and reverse reads to use\n\n-1 &lt;fastq&gt; and -2 &lt;fastq&gt;\n\nSpecify the ONT long reads to use\n\n-l &lt;fastq&gt;\n\nSpecify a directory to output files to\n\n-o &lt;dir&gt;\n\n\nNote, this method may take over an hour to run on the VMs - check with an instructor before running.\nunicycler -t 4 -1/home/ubuntu/Data/G18252308/illumina/ERR4635696_1.fastq.gz -2 /home/ubuntu/Data/G18252308/illumina/ERR4635696_2.fastq.gz -l /home/ubuntu/Data/G18252308/nanopore/ERR8187245.fastq.gz -o ERR8187245.unicycler.hybrid"
  },
  {
    "objectID": "module2/module2.html#compare-velvet-and-spades-assemblers",
    "href": "module2/module2.html#compare-velvet-and-spades-assemblers",
    "title": "Module 2",
    "section": "Compare Velvet and SPADES assemblers",
    "text": "Compare Velvet and SPADES assemblers\nRun SPADES (with Unicycler)\nunicycler -t 4 -1 /home/ubuntu/Data/G18255819/illumina/ERR4784794_1.fastq.gz -2 /home/ubuntu/Data/G18255819/illumina/ERR4784794_2.fastq.gz -o ERR4784794.short\nView the SPADES assembly metrics in Quast\n\nHow do these metrics compare with the results you got from the Velvet assembler?"
  },
  {
    "objectID": "module2/module2.html#genome-annotation-with-prokka",
    "href": "module2/module2.html#genome-annotation-with-prokka",
    "title": "Module 2",
    "section": "Genome annotation with Prokka",
    "text": "Genome annotation with Prokka\n\nRun Prokka for one sample\ncd /home/ubuntu/Data/\ndocker run -v $(pwd):$(pwd) -w $(pwd) staphb/prokka:latest prokka --outdir /home/ubuntu/Data/annotations/G18252308 --prefix G18252308 /home/ubuntu/Data/G18252308/illumina/G18252308_kmer51/contigs.fa\nCheck if the mecA gene was found in the annotated genome.\n\nOnce prokka is done, open Artemis and load the .gff file and locate the mecA gene. How many nucleotides does it have?\n\n\n\nHow to run Prokka on all samples from one folder\nExample script: create the following bash file: /home/ubuntu/Data/PROKKA_run/run_prokka.sh\nAdd the following lines of code:\nfor sample in `ls /home/ubuntu/Data/all_fastas/*.fasta | sed ‚Äòs/\\.fasta//‚Äô`\ndo\n¬†echo $sample\n¬†output=$(echo $sample | sed -E ‚Äòs#.*/([^/]+)$#\\1#‚Äô)\n¬†echo $output\n¬†docker run -v $(pwd):$(pwd) -w $(pwd) staphb/prokka:latest prokka ‚Äìoutdir /home/ubuntu/Data/annotations/${output} ‚Äìprefix ${output} /home/ubuntu/Data/all_fastas/${output}.fasta\ndone"
  },
  {
    "objectID": "module2/module2.html#bonus-activity",
    "href": "module2/module2.html#bonus-activity",
    "title": "Module 2",
    "section": "Bonus activity",
    "text": "Bonus activity\nInstall Snippy, snp_sites and IQtree and run a SNP tree. See the tutorial here"
  },
  {
    "objectID": "unix-commands.html",
    "href": "unix-commands.html",
    "title": "unix-commands",
    "section": "",
    "text": "Useful General Commands File Management # Compress files gzip large_file.txt\n\nDecompress\ngunzip file.txt.gz\n\n\nArchive directory\ntar -czf archive.tar.gz directory/\n\n\nExtract archive\ntar -xzf archive.tar.gz\n\n\nDisk usage\ndu -sh /home/ubuntu/analysis/*\nText Processing # Count lines wc -l file.txt\n\n\nSearch for pattern\ngrep ‚Äúpattern‚Äù file.txt\n\n\nColumn formatting\ncolumn -t -s‚Äô,‚Äô file.csv\n\n\nSort numerically\nsort -n file.txt\n\n\nUnique entries\nsort file.txt | uniq -c\nProcess Management # Check running processes top\n\n\nKill process\nkill PID\n\n\nRun in background\ncommand &\n\n\nCheck background jobs\njobs\nTroubleshooting Common Issues Out of Memory # Check memory usage free -h\n\n\nReduce threads\n\n\nChange -t 4 to -t 2 in commands\nCommand Not Found # Check if installed which program_name\n\n\nActivate conda environment\nconda activate genomics\n\n\nAdd to PATH\nexport PATH=$PATH:/home/ubuntu/Software/tool/bin\nPermission Denied # Make script executable chmod +x script.sh\n\n\nChange ownership\nsudo chown ubuntu:ubuntu file.txt\nDisk Space Issues # Check space df -h\n\n\nFind large files\ndu -h /home/ubuntu | sort -rh | head -20\n\n\nClean up\nrm -rf /home/ubuntu/analysis/temp/*"
  },
  {
    "objectID": "module3/module3.html",
    "href": "module3/module3.html",
    "title": "Module 3",
    "section": "",
    "text": "Read about Roary here.\nRoary is a high speed stand alone pan genome pipeline, which takes annotated assemblies in GFF3 format (produced by Prokka (Seemann, 2014)) and calculates the pan genome. Using a standard desktop PC, it can analyse datasets with thousands of samples, something which is computationally infeasible with existing methods, without compromising the quality of the results. 128 samples can be analysed in under 1 hour using 1 GB of RAM and a single processor. To perform this analysis using existing methods would take weeks and hundreds of GB of RAM. Roary is not intended for meta-genomics or for comparing extremely diverse sets of genomes.\ndocker run --rm -it -v $(pwd):$(pwd) sangerpathogens/roary roary -f /home/ubuntu/Data/roary /home/ubuntu/Data/annotations/*/*.gff\nThe summary output is present in the summary_statistics.txt\nCore genes (99% &lt;= strains &lt;= 100%) 1861\nSoft core genes (95% &lt;= strains &lt; 99%) 0\nShell genes (15% &lt;= strains &lt; 95%) 962\nCloud genes (0% &lt;= strains &lt; 15%) 838\nTotal genes (0% &lt;= strains &lt;= 100%) 3661\nAdditionally, Roary produces a gene_presence_absence.csv file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.\nDrop the accessory_binary_genes.fa.newick and gene_presence_absence.csv files in Phandango\n\nWhen you analyze your own data, you will need a phylogeny that represents the evolutionary history of your isolates. The inference of a phylogenetic tree is not part of roary‚Äôs functions, but you can use the core gene alignment (file: core_gene_alignment.aln) as input to infer a tree.\nRoary comes with a python script that allows you to generate a few plots to graphically assess your analysis output. You will need the gene_presence_absence.csv file and the phylogeny at /home/ubuntu/Data/trees/tree.nwk.\nYou should get three files: a pangenome matrix, a frequency plot, and a pie chart.\npython /home/ubuntu/Software/roary_plots.py /home/ubuntu/Data/trees/tree_for_phandango.nwk /home/ubuntu/roary/gene_presence_absence.csv\n\n\n\n\n\n\n\nThe first part of this exercise is broadly based on a tutorial by Martin Hunt.\n\n\n\nARIBA is a tool that identifies antibiotic resistance genes. This tutorial will walk you through the analysis of the Staphylococcus aureus data set used in the paper:\nNovel multidrug-resistant sublineages of Staphylococcus aureus clonal complex 22 discovered in India\nAbrudan, Shamanna et al, mSphere 2023 https://doi.org/10.1128/msphere.00185-23\nLearning outcomes By the end of this tutorial you can expect to be able to:\n‚Ä¢ Download and prepare the standard AMR databases for use with ARIBA\n‚Ä¢ Prepare your own database for use with ARIBA\n‚Ä¢ Perform QC on input data and understand why QC is important\n‚Ä¢ Run ARIBA on several samples to identify antibiotic resistance\n‚Ä¢ Understand the different flags produced by ARIBA\n‚Ä¢ Summarise ARIBA results for several samples\n‚Ä¢ Query the AMR results produced by ARIBA\n‚Ä¢ Use Phandango to visualise ARIBA results\nThis module comprises the following sections:\n1. Run ARIBA on a reference database\n2. View summarized results using Phandango\n\nYou can run the commands in this module in the Amazon EC2 instance you were provided by your course instructor.\nARIBA should be already installed on this machine. If you need to use a different machine, please follow the instructions from Module 1 of this course.\n1. Run ARIBA on a reference database\nConfigure the Resfinder and CARD reference databases.\nariba getref resfinder out.resfinder\nariba prepareref -f out.resfinder.fa -m out.resfinder.tsv out.resfinder.prepareref\nariba getref card out.card\nariba prepareref -f out.card.fa -m out.card.tsv out.card.prepareref\nHow to run on one sample\nARIBA needs the database directories, for eg, out.resfinder.prepareref or out.card.prepareref, and two sequencing reads files reads.1.fastq.gz and reads.2.fastq.gz. The command to run ARIBA is:\nariba run out.resfinder.prepareref reads.1.fastq.gz reads.2.fastq.gz outdir\nThe above command will make a new directory called outdir that contains the results.\nRun ARIBA on all samples\nThe S. aureus dataset consists of many samples, and we need to run ARIBA on each sample, which can be done with a ‚Äúfor‚Äù loop. We assume that the reads files are named like this:\nname1_1.fastq.gz name1_2.fastq.gz\nname2_1.fastq.gz name2_2.fastq.gz\nname3_1.fastq.gz name3_2.fastq.gz\nThen we can run ARIBA on all samples like this (you may need to edit this command depending on how your own files are named):\nEg:\nfor sample in `ls *.1.fastq.gz | sed 's/\\.1.fastq.gz//'`\ndo\n  ariba run out.resfinder.prepareref $sample_1.fastq.gz $sample_2.fastq.gz $sample.ariba\ndone\nThe output directory of each sample is called $sample.ariba, for example name1.ariba is the output directory for sample name1.\nEXERCISE: Using the information above, write a script to fit the file names and paths in your own context and run ARIBA on all samples.\nARIBA output\nThe format of the output files are described here.\nViewing ARIBA results in Phandango\nThis section describes how to use Phandango to view a summary of ARIBA results from many samples.\nThe most important output file from ARIBA is the report called report.tsv. For this tutorial, we have all sample reports in the directory /home/ubuntu/Data/ARIBA_output\nls /home/ubuntu/Data/ARIBA_output | wc -l\nARIBA has a functon called ‚Äúsummary‚Äù that can summarise presence/absence of sequences and/or SNPs across samples. It takes at least two ariba reports as input, and makes a CSV file that can be opened in your favourite spreadsheet program, and also makes input files for Phandango. The two Phandango files (a tree and a CSV file) can be dropped straight into the Phandango page for viewing.\nThe tree that ARIBA makes is based on the CSV file, which contains results of presence/absence of sequence and SNPs, and other information such as percent identity between contigs and reference sequences. This means that it does not necessarily represent the real phylogeny of the samples. It is more accurate to provide a tree built from the sequencing data. For this reason, we will use a pre-computed tree file /home/ubuntu/Data/tree_for_phandango.tre.\nBasic usage of ariba summary\nCheck the options of ariba summaryusing the following command:\nariba summary -h\nFirst, let‚Äôs run ariba summary using the default settings, except we will skip making the tree:\nariba summary --no_tree out /home/ubuntu/Data/ARIBA_output/*.tsv\nWe can see that this made two files:\nls out.*\nThey are the same except for the first line, which has Phandango-specific information. ARIBA uses the filenames as sample names in the output:\nhead -n 2 out.phandango.csv\nThe first name is ‚Äú/home/ubuntu/Data/ARIBA_output/*/sample.tsv‚Äù, and the rest are named similarly.\nThis is not ideal, as it will look ugly in Phandango. Further, the names must exactly match the names in the tree file for Phandango to work (have a look in the tree /home/ubuntu/Data/trees/tree_for_phandango.nwk). You could do a little hacking here using the Unix command sed on the CSV file. Instead, we can supply ARIBA with a file of filenames that also tells ariba what to call the samples in its output CSV files. Instead of ‚Äú/home/ubuntu/Data/ARIBA_output/sample.tsv‚Äù, we would like to simply use ‚Äúsample‚Äù, which is cleaner and matches the tree file. It also means we can (and will) repeatedly run ariba summary with different options, and get output files that can be loaded straight into Phandango. This is one way to make the file with the naming information:\nls /home/ubuntu/Data/ARIBA_output/*/report.tsv | awk -F/ '{name=$(NF-1); sub(/\\.ariba$/, \"\", name); print $0, name}' &gt; ARIBA_output/filenames.fofn\nThe file is quite simple. Column 1 is the filename, and column 2 is the name we would like to use in the output.\n head ARIBA_output/filenames.fofn\nNow we can rerun summary using this input file. Note the use of the new option ‚Äìfofn.\nariba summary --no_tree --fofn ARIBA_output/filenames.fofn ARIBA_output/out /home/ubuntu/Data/ARIBA_output/*/report.tsv\nCheck that the renaming worked:\nhead -n 2 out.phandango.csv\nNow go to Phandango and drag and drop the files out.phandango.csv and trees/tree_for_phandango.nwk into the window. The result should like this\n\nThis a very high-level summary of the data. For each cluster, it is simply saying whether or not each sample has a ‚Äòmatch‚Äô. Green means a match, and pink means not a match. For presence/absence genes, this means that the gene must simply be there to count as a match. If it is a ‚Äúvariant only‚Äù gene, then the gene must be there and one of the variants that we told ARIBA about earlier when generating the ARIBA database.\nMore information per cluster\nIn addition to a simple ‚Äúyes‚Äù or ‚Äúno‚Äù as to whether a sample ‚Äúmatches‚Äù a given cluster (as explained above), more columns can be output for each cluster. See the ARIBA summary wiki page for a full description of the options.\nVariants In the previous screenshot, where the option ‚Äìpreset cluster_all, there are two variant columns: ‚Äúknown_var‚Äù and ‚Äúnovel_var‚Äù. Green means ‚Äúyes‚Äù and pink means ‚Äúno‚Äù.\nPart 2: Antimicrobial Resistance Identification using Pathogenwatch\nBrowse the public collection:\nhttps://pathogen.watch/collection/mxebr8oz0wjm-module2-s-aureus\nDownload the AMR genes and AMR SNPs.\nPart 3: Compare results obtained with ARIBA Resfinder versus Pathogenwatch\nOpen the ARIBA summary files and compare them with the AMR genes and AMR SNPs files you downloaded from Pathogenwatch.\n\nWhich tool produced more results? Why?\nWhich tool would you use to report AMR in your collection of S. aureus?\n\nA Python script that compares the ARIBA Resfinder summary result with the Pathogenwatch AMR genes result is available on Github: https://github.com/monicaiabrudan/bacterial-genomics/blob/main/Compare_AMR_files.ipynb\nThe input files can be found here: https://github.com/monicaiabrudan/bacterial-genomics/tree/main/Compare_AMR_files"
  },
  {
    "objectID": "module3/module3.html#core-and-accessory-genomes-with-roary",
    "href": "module3/module3.html#core-and-accessory-genomes-with-roary",
    "title": "Module 3",
    "section": "",
    "text": "Read about Roary here.\nRoary is a high speed stand alone pan genome pipeline, which takes annotated assemblies in GFF3 format (produced by Prokka (Seemann, 2014)) and calculates the pan genome. Using a standard desktop PC, it can analyse datasets with thousands of samples, something which is computationally infeasible with existing methods, without compromising the quality of the results. 128 samples can be analysed in under 1 hour using 1 GB of RAM and a single processor. To perform this analysis using existing methods would take weeks and hundreds of GB of RAM. Roary is not intended for meta-genomics or for comparing extremely diverse sets of genomes.\ndocker run --rm -it -v $(pwd):$(pwd) sangerpathogens/roary roary -f /home/ubuntu/Data/roary /home/ubuntu/Data/annotations/*/*.gff\nThe summary output is present in the summary_statistics.txt\nCore genes (99% &lt;= strains &lt;= 100%) 1861\nSoft core genes (95% &lt;= strains &lt; 99%) 0\nShell genes (15% &lt;= strains &lt; 95%) 962\nCloud genes (0% &lt;= strains &lt; 15%) 838\nTotal genes (0% &lt;= strains &lt;= 100%) 3661\nAdditionally, Roary produces a gene_presence_absence.csv file that can be opened in any spreadsheet software to manually explore the results. In this file, you will find information such as gene name and gene annotation, and, of course, whether a gene is present in a genome or not.\nDrop the accessory_binary_genes.fa.newick and gene_presence_absence.csv files in Phandango\n\nWhen you analyze your own data, you will need a phylogeny that represents the evolutionary history of your isolates. The inference of a phylogenetic tree is not part of roary‚Äôs functions, but you can use the core gene alignment (file: core_gene_alignment.aln) as input to infer a tree.\nRoary comes with a python script that allows you to generate a few plots to graphically assess your analysis output. You will need the gene_presence_absence.csv file and the phylogeny at /home/ubuntu/Data/trees/tree.nwk.\nYou should get three files: a pangenome matrix, a frequency plot, and a pie chart.\npython /home/ubuntu/Software/roary_plots.py /home/ubuntu/Data/trees/tree_for_phandango.nwk /home/ubuntu/roary/gene_presence_absence.csv\n\n\n\n\n\n\n\nThe first part of this exercise is broadly based on a tutorial by Martin Hunt.\n\n\n\nARIBA is a tool that identifies antibiotic resistance genes. This tutorial will walk you through the analysis of the Staphylococcus aureus data set used in the paper:\nNovel multidrug-resistant sublineages of Staphylococcus aureus clonal complex 22 discovered in India\nAbrudan, Shamanna et al, mSphere 2023 https://doi.org/10.1128/msphere.00185-23\nLearning outcomes By the end of this tutorial you can expect to be able to:\n‚Ä¢ Download and prepare the standard AMR databases for use with ARIBA\n‚Ä¢ Prepare your own database for use with ARIBA\n‚Ä¢ Perform QC on input data and understand why QC is important\n‚Ä¢ Run ARIBA on several samples to identify antibiotic resistance\n‚Ä¢ Understand the different flags produced by ARIBA\n‚Ä¢ Summarise ARIBA results for several samples\n‚Ä¢ Query the AMR results produced by ARIBA\n‚Ä¢ Use Phandango to visualise ARIBA results\nThis module comprises the following sections:\n1. Run ARIBA on a reference database\n2. View summarized results using Phandango\n\nYou can run the commands in this module in the Amazon EC2 instance you were provided by your course instructor.\nARIBA should be already installed on this machine. If you need to use a different machine, please follow the instructions from Module 1 of this course.\n1. Run ARIBA on a reference database\nConfigure the Resfinder and CARD reference databases.\nariba getref resfinder out.resfinder\nariba prepareref -f out.resfinder.fa -m out.resfinder.tsv out.resfinder.prepareref\nariba getref card out.card\nariba prepareref -f out.card.fa -m out.card.tsv out.card.prepareref\nHow to run on one sample\nARIBA needs the database directories, for eg, out.resfinder.prepareref or out.card.prepareref, and two sequencing reads files reads.1.fastq.gz and reads.2.fastq.gz. The command to run ARIBA is:\nariba run out.resfinder.prepareref reads.1.fastq.gz reads.2.fastq.gz outdir\nThe above command will make a new directory called outdir that contains the results.\nRun ARIBA on all samples\nThe S. aureus dataset consists of many samples, and we need to run ARIBA on each sample, which can be done with a ‚Äúfor‚Äù loop. We assume that the reads files are named like this:\nname1_1.fastq.gz name1_2.fastq.gz\nname2_1.fastq.gz name2_2.fastq.gz\nname3_1.fastq.gz name3_2.fastq.gz\nThen we can run ARIBA on all samples like this (you may need to edit this command depending on how your own files are named):\nEg:\nfor sample in `ls *.1.fastq.gz | sed 's/\\.1.fastq.gz//'`\ndo\n  ariba run out.resfinder.prepareref $sample_1.fastq.gz $sample_2.fastq.gz $sample.ariba\ndone\nThe output directory of each sample is called $sample.ariba, for example name1.ariba is the output directory for sample name1.\nEXERCISE: Using the information above, write a script to fit the file names and paths in your own context and run ARIBA on all samples.\nARIBA output\nThe format of the output files are described here.\nViewing ARIBA results in Phandango\nThis section describes how to use Phandango to view a summary of ARIBA results from many samples.\nThe most important output file from ARIBA is the report called report.tsv. For this tutorial, we have all sample reports in the directory /home/ubuntu/Data/ARIBA_output\nls /home/ubuntu/Data/ARIBA_output | wc -l\nARIBA has a functon called ‚Äúsummary‚Äù that can summarise presence/absence of sequences and/or SNPs across samples. It takes at least two ariba reports as input, and makes a CSV file that can be opened in your favourite spreadsheet program, and also makes input files for Phandango. The two Phandango files (a tree and a CSV file) can be dropped straight into the Phandango page for viewing.\nThe tree that ARIBA makes is based on the CSV file, which contains results of presence/absence of sequence and SNPs, and other information such as percent identity between contigs and reference sequences. This means that it does not necessarily represent the real phylogeny of the samples. It is more accurate to provide a tree built from the sequencing data. For this reason, we will use a pre-computed tree file /home/ubuntu/Data/tree_for_phandango.tre.\nBasic usage of ariba summary\nCheck the options of ariba summaryusing the following command:\nariba summary -h\nFirst, let‚Äôs run ariba summary using the default settings, except we will skip making the tree:\nariba summary --no_tree out /home/ubuntu/Data/ARIBA_output/*.tsv\nWe can see that this made two files:\nls out.*\nThey are the same except for the first line, which has Phandango-specific information. ARIBA uses the filenames as sample names in the output:\nhead -n 2 out.phandango.csv\nThe first name is ‚Äú/home/ubuntu/Data/ARIBA_output/*/sample.tsv‚Äù, and the rest are named similarly.\nThis is not ideal, as it will look ugly in Phandango. Further, the names must exactly match the names in the tree file for Phandango to work (have a look in the tree /home/ubuntu/Data/trees/tree_for_phandango.nwk). You could do a little hacking here using the Unix command sed on the CSV file. Instead, we can supply ARIBA with a file of filenames that also tells ariba what to call the samples in its output CSV files. Instead of ‚Äú/home/ubuntu/Data/ARIBA_output/sample.tsv‚Äù, we would like to simply use ‚Äúsample‚Äù, which is cleaner and matches the tree file. It also means we can (and will) repeatedly run ariba summary with different options, and get output files that can be loaded straight into Phandango. This is one way to make the file with the naming information:\nls /home/ubuntu/Data/ARIBA_output/*/report.tsv | awk -F/ '{name=$(NF-1); sub(/\\.ariba$/, \"\", name); print $0, name}' &gt; ARIBA_output/filenames.fofn\nThe file is quite simple. Column 1 is the filename, and column 2 is the name we would like to use in the output.\n head ARIBA_output/filenames.fofn\nNow we can rerun summary using this input file. Note the use of the new option ‚Äìfofn.\nariba summary --no_tree --fofn ARIBA_output/filenames.fofn ARIBA_output/out /home/ubuntu/Data/ARIBA_output/*/report.tsv\nCheck that the renaming worked:\nhead -n 2 out.phandango.csv\nNow go to Phandango and drag and drop the files out.phandango.csv and trees/tree_for_phandango.nwk into the window. The result should like this\n\nThis a very high-level summary of the data. For each cluster, it is simply saying whether or not each sample has a ‚Äòmatch‚Äô. Green means a match, and pink means not a match. For presence/absence genes, this means that the gene must simply be there to count as a match. If it is a ‚Äúvariant only‚Äù gene, then the gene must be there and one of the variants that we told ARIBA about earlier when generating the ARIBA database.\nMore information per cluster\nIn addition to a simple ‚Äúyes‚Äù or ‚Äúno‚Äù as to whether a sample ‚Äúmatches‚Äù a given cluster (as explained above), more columns can be output for each cluster. See the ARIBA summary wiki page for a full description of the options.\nVariants In the previous screenshot, where the option ‚Äìpreset cluster_all, there are two variant columns: ‚Äúknown_var‚Äù and ‚Äúnovel_var‚Äù. Green means ‚Äúyes‚Äù and pink means ‚Äúno‚Äù.\nPart 2: Antimicrobial Resistance Identification using Pathogenwatch\nBrowse the public collection:\nhttps://pathogen.watch/collection/mxebr8oz0wjm-module2-s-aureus\nDownload the AMR genes and AMR SNPs.\nPart 3: Compare results obtained with ARIBA Resfinder versus Pathogenwatch\nOpen the ARIBA summary files and compare them with the AMR genes and AMR SNPs files you downloaded from Pathogenwatch.\n\nWhich tool produced more results? Why?\nWhich tool would you use to report AMR in your collection of S. aureus?\n\nA Python script that compares the ARIBA Resfinder summary result with the Pathogenwatch AMR genes result is available on Github: https://github.com/monicaiabrudan/bacterial-genomics/blob/main/Compare_AMR_files.ipynb\nThe input files can be found here: https://github.com/monicaiabrudan/bacterial-genomics/tree/main/Compare_AMR_files"
  },
  {
    "objectID": "module4/module4.html",
    "href": "module4/module4.html",
    "title": "Module 4",
    "section": "",
    "text": "SARS-CoV-2 surveillance in Romania\nGenomic surveillance of pathogens aims to understand the emergence and dissemination of pathogens or their lineages of risk with the ultimate goal of implementing evidence-based interventions to protect public health. Epidemiological data from patients is collected by healthcare professionals. Data on species identification and any phenotypic or molecular characterization of the isolates is often generated by the microbiology laboratories linked to healthcare facilities and/or the reference laboratory. In an ideal scenario, the different sources of laboratory data are stored in a centralised surveillance system and database (such as WHONET). However, these systems rarely also incorporate genomic data produced by bioinformaticians.\nThe Centre for Genomic Pathogen Surveillance develops free web applications for integration and visualisation of surveillance (and other) data, called data-flo and Microreact. The aim of this module is to highlight the role of the various analytics and sources of data in pathogen surveillance, and how streamlined data integration is essential for real-time decision making, while also introducing resources that can facilitate the inclusion of this topic in teaching curricula."
  },
  {
    "objectID": "module4/module4.html#input-files",
    "href": "module4/module4.html#input-files",
    "title": "Module 4",
    "section": "Input files:",
    "text": "Input files:\nhttps://drive.google.com/drive/folders/1yP-ectAQ0plj8G1rcd_UNqPOyFIC7axa?usp=sharing"
  },
  {
    "objectID": "module4/module4.html#resources-needed",
    "href": "module4/module4.html#resources-needed",
    "title": "Module 4",
    "section": "Resources needed",
    "text": "Resources needed\nInternet access, Projector & screen, Participants should bring their laptops equipped with Google Chrome or Mozilla Firefox.\nAccess to: https://microreact.org/, https://pathogen.watch/, https://data-flo.io/"
  },
  {
    "objectID": "module4/module4.html#introduction",
    "href": "module4/module4.html#introduction",
    "title": "Module 4",
    "section": "Introduction",
    "text": "Introduction\nNote: This case-study is based on real sequence data from¬†the article ‚ÄúMolecular epidemiology analysis of SARS-CoV-2 strains circulating in Romania during the first months of the pandemic.‚Äù by Surleac, Marius, et al., https://doi.org/10.3390/life10080152 , but it has been modified for the purposes of teaching this course.¬†\n\n\n\nThe workflow of this exercise\n\n\nExplore the epidemiological information related to 24 SARS-CoV-2 samples from Romania.\n\n\n\nCountry\nTown\nNAME\nDate\n\n\n\n\nRomania\nBucuresti\nEPI_ISL_468134\n21/03/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468135\n22/03/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468136\n22/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468137\n22/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468138\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468139\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468140\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468141\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468142\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468143\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468144\n25/03/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468145\n28/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468146\n08/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468147\n11/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468148\n11/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468149\n12/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468150\n12/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468151\n15/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468152\n16/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468153\n17/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468154\n17/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468155\n18/04/2020\n\n\nRomania\nSuceava\nEPI_ISL_468157\n29/05/2020\n\n\nRomania\nSuceava\nEPI_ISL_468158\n29/05/2020\n\n\n\nThe genomes of the isolates presented in the table above have been sequenced, assembled and the assemblies have been uploaded to Pathogenwatch for further analyses.\nActivity 1. Data analysis of 24 SARS-CoV-2 genomes from Romania, collected during the early months of the pandemic.\nExplore a collection of 24 SARS-CoV-2 genomes in Pathogenwatch.\nhttps://pathogen.watch/collection/5hpqvjar7gfa-sars-cov-2-romania-v1\nQ1: In the statistics tab, check the genome lengths. Why do you think these have variable values?\nQ2: Why are the values of the N50 equal to the values of the genome lengths?\nQ3: What do you think of the QC statistics of the genome assemblies overall?\nQ4: Go to the Typing tab. What do you think is a ‚Äúlineage‚Äù of SARS-CoV-2? Why were ‚Äúlineages‚Äù defined early in the pandemic? What lineages do these genomes belong to?\nQ5: Go to the ‚ÄúNotable mutations‚Äù tab. Observe that all the genomes in this collection belong to the same lineage and they all share the same notable mutations. What does this tell you about the diversity of this collection?\nQ6: Click on one of the genomes and see the ‚ÄúGenome card‚Äù. Check the details of this lineage in a global context, by pressing View B.1.1 in the COG-UK Global Microreact. Where did this lineage spread and when?\n\n\n\nPathogenwatch screenshot showing a genome card for a SARS-CoV-2 genome from the lineage B1.1 . Notice the View B.1.1 in the COG-UK Global Microreact link\n\n\n\n\n\nMicroreact view showing isolates from the B1.1 lineage.\n\n\nActivity 2. Data integration\nIn this activity, you will merge the epidemiological information related to the isolates, the bioinformatics analysis files produced by Pathogenwatch and with the tree in Newick format, to produce a Microreact.\nDownload all the files used in this exercise from https://drive.google.com/drive/folders/1yP-ectAQ0plj8G1rcd_UNqPOyFIC7axa\nGo to Data-flo: https://data-flo.io/run?4LxncBtNr69TmfzTGj9Szm\nClick on the Run button. Load the input files. Notice that you need an ‚Äúaccess token‚Äù from Microreact.\n\n\n\nThe RUN button in Data-flo\n\n\n\n\n\nYou will find your access token in your Microreact account, under ‚ÄúAccount settings‚Äù\n\n\n\n\n\nThe output of data-flo is a link to a microreact\n\n\n\n\n\nThe microreact produced by the data-flo\n\n\nCopy the data-flo to your account.\n\n\n\nOnce you are logged in to your data-flo account, you will be able to copy a public data-flo to your own account. Press on the symbol showing two overlapping papers on the right top corner to copy the current data-flo to your account\n\n\nRun the data-flo in debug mode and explore how data is being transformed between steps.\n\n\n\nPress the bug symbol at the top left corner to trigger the Debug mode. Notice the Data-flo debugger loading at the bottom of the screen.\n\n\nQ1: Which adaptors are responsible for data entry?\nQ2: What does the ‚ÄúForward geocoding‚Äù adaptor do?\nQ3: Check the Microreact link produced by the data-flo.\nActivity 3. Data visualisation with Microreact\nExplore the Microreact produced by the Data-flo in the previous step.\nColour the leaves of the tree by ‚ÄúTown‚Äù and display the metadata columns for ‚ÄúNotable mutations‚Äù and the ‚ÄúPangolin lineage‚Äù.\n\n\n\nIn order to set the colour of the leaves of a tree in Microreact, go to the eye symbol on top right and choose ‚ÄúTown‚Äù in the ‚ÄúColour Column‚Äù\n\n\n\n\n\nSelect which columns you would like to display in the metadata blocks\n\n\nYour personal Microreact should like this one: https://microreact.org/project/sars-cov-2-romania1\nQ1: Look at the timelime. Which is the first isolate sampled from this collection? When was it sampled and where?\nQ2: When was the first isolate from the Bucharest sampled and when was the first isolate from Suceava sampled?\nActivity 4. Data analysis with context genomes\nThe bioinformatician in your team has found another 26 SARS-CoV-2 genomes from Romania in a public database and wants to include them in your study.\nView the whole collection of 51 genomes in Pathogenwatch https://pathogen.watch/collection/gqhf7ndghsim-test2\nSelect one of the genomes that belong to the lineage B.1.408 and open the ‚ÄúGenome card‚Äù.\nThen click on the View B.1.408 in the COG-UK Global Microreact\nQ1: How many entries of this sublineage are found in the database?\nQ2: In what countries was this sub-lineage found ? What does this tell you about the transmission links between Romania and those countries?\n\n\n\nLineage B.1.408 shown in Microreact\n\n\nSelect the genome that belongs to the B.1.520 lineage, open its ‚ÄúGenome card‚Äù and click on the View B.1.520 in the COG-UK Global Microreact.\nQ3: In what countries was this sub-lineage found ? What does this tell you about the transmission links between Romania and those countries?\n\n\n\nLineage B.1.520 shown in Microreact\n\n\nActivity 5. Data integration for the collection of 51 genomes.\nLoad the same Data-flo as in Activity 2: https://data-flo.io/run?4LxncBtNr69TmfzTGj9Szm\nRun the data-flo with the new set of input files and load the new Microreact.\nActivity 6. Explore the new Microreact.\nColour the nodes of the tree by ‚ÄúTown‚Äù and display the mutations in the metadata blocks.\nYour microreact should look like this: https://microreact.org/project/sars-cov-2-romania-2\nQ1: According to this study, where did SARS-CoV-2 spread first outside of Suceava and Bucuresti?\nQ2: Where was the isolate belonging to the B.1.520 lineage found in Romania?\nActivity 7. Discuss the Romanian sequences in a global context.\n\n\n\nFigure from the original study published at https://doi.org/10.3390/life10080152\n\n\nQ1: How many introductions of SARS-CoV-2 into Romania can you identify based on this figure?\nQ2: Why do you think that the isolates from Suceava are interleaved with isolates from abroad?"
  },
  {
    "objectID": "module4/module4.html#appendix",
    "href": "module4/module4.html#appendix",
    "title": "Module 4",
    "section": "Appendix",
    "text": "Appendix\nData-flo Documentation: https://docs.data-flo.io/introduction/readme\nIf you found data-flo useful and would like to continue using it, start by creating an account https://data-flo.io/signin. There are a few public workflows with useful transformations such as left join two CSVs (https://data-flo.io/run?bzYR3DtxRJsBty5ezb5LoJ) or the Geocoder (https://data-flo.io/run?kvpsi3T8V) that you can try.\n\nA similar workflow to the one used on this course can be found in the documentation https://docs.data-flo.io/tutorials/prep-outbreak-data-for-microreact. In this workflow, one of the inputs is a SNP matrix.¬†\nData-flo also allows you to import data from a Google Spreadsheet (https://docs.data-flo.io/using-data-flo/specific-adaptors/google-spreadsheet) or from a database (https://docs.data-flo.io/tutorials/common-use-cases-solved/connect-directly-to-a-database) (as opposed to downloading data from the spreadsheet/database into a file).¬†\n\nMicroreact documentation: https://docs.microreact.org/\nTo be able to edit and manage projects, please create your own Microreact Account https://microreact.org/api/auth/signin.¬†\nYou can customise the colours of your visualisations by assigning colours to the different variables before you create the Microreact project and/or after the project was created https://docs.microreact.org/instructions/labels-colours-and-shapes.\nEveryone with access to a Microreact project has access to the data the project uses. Privacy and permissions can be configured to change who can access the project and its data https://docs.microreact.org/instructions/access-control-and-project-sharing."
  },
  {
    "objectID": "Day2/AMR-detection.html",
    "href": "Day2/AMR-detection.html",
    "title": "AMR-detection",
    "section": "",
    "text": "Antimicrobial Resistance Detection\n\n1. Prepare ARIBA Databases\n\n\nDownload ResFinder database\nariba getref resfinder /home/ubuntu/Reference/ARIBA/resfinder\n\n\nPrepare database for ARIBA\nariba prepareref\n-f /home/ubuntu/Reference/ARIBA/resfinder.fa\n-m /home/ubuntu/Reference/ARIBA/resfinder.tsv\n/home/ubuntu/Reference/ARIBA/resfinder.prepareref\n\n\nDownload CARD database\nariba getref card /home/ubuntu/Reference/ARIBA/card\n\n\nPrepare CARD database\nariba prepareref\n-f /home/ubuntu/Reference/ARIBA/card.fa\n-m /home/ubuntu/Reference/ARIBA/card.tsv\n/home/ubuntu/Reference/ARIBA/card.prepareref\n\n\n\n2. Run ARIBA on Single Sample\n\nRun ARIBA with ResFinder\nariba run\n/home/ubuntu/Reference/ARIBA/resfinder.prepareref\nsample1_1.clean.fastq.gz\nsample1_2.clean.fastq.gz\n/home/ubuntu/analysis/amr/sample1_resfinder\n‚Äìthreads 4\n\n\nView results\ncat /home/ubuntu/analysis/amr/sample1_resfinder/report.tsv | column -t | less -S\nUnderstanding ARIBA report columns: ref_name: Gene name flag: Quality flags (see below) pc_ident: Percent identity to reference ref_len: Reference gene length ref_base_assembled: Bases of reference assembled known_var: Known variant detected ARIBA flags: 19: Match without issues (good!) 27: Partial match 147: Assembly problems 403: Low coverage\n\n\n\n3. Run ARIBA on Multiple Samples\n\nCreate list of samples\nls /home/ubuntu/Data/samples/*_1.clean.fastq.gz | sed ‚Äòs/_1.clean.fastq.gz//‚Äô &gt; sample_list.txt\n\n\nLoop through samples\nwhile read sample; do sample_name=$(basename $sample) ariba run\n/home/ubuntu/Reference/ARIBA/resfinder.prepareref\n${sample}_1.clean.fastq.gz\n\\({sample}_2.clean.fastq.gz \\\n    /home/ubuntu/analysis/amr/\\){sample_name}_resfinder\n‚Äìthreads 4 done &lt; sample_list.txt\n\n\n\n4. Summarize ARIBA Results\n\nCreate summary across all samples\nariba summary\n/home/ubuntu/analysis/amr/summary_resfinder\n/home/ubuntu/analysis/amr/*/report.tsv\n\n\nView summary\ncat /home/ubuntu/analysis/amr/summary_resfinder.csv | column -t -s‚Äô,‚Äô | less -S\nOutput files: summary_resfinder.csv: Presence/absence matrix summary_resfinder.phandango.csv: Formatted for Phandango summary_resfinder.phandango.tre: Simple tree for Phandango # 5. Visualize in Phandango ### Open Phandango in browser firefox https://phandango.net/\n\n\nDrag and drop these files:\n\nsummary_resfinder.phandango.tre\nsummary_resfinder.phandango.csv\n\nPhandango interpretation: Tree shows relationships between samples Colored cells show AMR gene presence Darker colors = higher confidence matches White = gene absent\n\n\n\n6. Alternative: AMRFinderPlus\n\nRun AMRFinderPlus on protein sequences\namrfinder\n‚Äìprotein /home/ubuntu/analysis/annotation/sample1_prokka/sample1.faa\n‚Äìorganism Staphylococcus_aureus\n‚Äìplus\n‚Äìoutput /home/ubuntu/analysis/amr/sample1_amrfinder.txt\n‚Äìthreads 4\n\n\nView results\ncat /home/ubuntu/analysis/amr/sample1_amrfinder.txt | column -t | less -S\n\n\nClinical Interpretation Exercise\n\n\nCreate simple AMR profile summary\necho ‚ÄúSample,Beta-lactam,Aminoglycoside,Fluoroquinolone,MRSA‚Äù &gt; amr_profile.csv\nfor dir in /home/ubuntu/analysis/amr/*_resfinder; do sample=$(basename $dir | sed ‚Äòs/_resfinder//‚Äô)\n# Check for specific resistance genes beta=$(grep -c ‚Äúbla‚Äù \\(dir/report.tsv || echo 0)\n  amino=\\)(grep -c ‚Äúaac|aph|ant‚Äù \\(dir/report.tsv || echo 0)\n  fluoro=\\)(grep -c ‚Äúgyr|par‚Äù \\(dir/report.tsv || echo 0)\n  mrsa=\\)(grep -c ‚ÄúmecA‚Äù $dir/report.tsv || echo 0)\necho ‚Äú\\(sample,\\)beta,\\(amino,\\)fluoro,$mrsa‚Äù &gt;&gt; amr_profile.csv done\ncolumn -t -s‚Äô,‚Äô amr_profile.csv"
  },
  {
    "objectID": "Day3/Comparative-Genomics.html",
    "href": "Day3/Comparative-Genomics.html",
    "title": "Comparative-Genomics",
    "section": "",
    "text": "Pan-genome Analysis 1. Prepare Input Files # Make sure all samples are annotated with Prokka # GFF files should be in: /home/ubuntu/analysis/annotation//sample.gff\n\nCreate directory for Roary\nmkdir -p /home/ubuntu/analysis/pangenome\n\n\nCopy all GFF files to one location\ncp /home/ubuntu/analysis/annotation//.gff /home/ubuntu/analysis/pangenome/\n\nRun Roary # Basic Roary run roary\n-f /home/ubuntu/analysis/pangenome/roary_output\n-e -n -v\n-p 4\n/home/ubuntu/analysis/pangenome/*.gff\n\n\n\nOptions explained:\n\n\n-e: create multifasta alignment of core genes\n\n\n-n: fast core gene alignment (for speed)\n\n\n-v: verbose output\n\n\n-p: number of threads\nRoary outputs: gene_presence_absence.csv: Main results (open in Excel/LibreOffice) summary_statistics.txt: Pan-genome statistics core_gene_alignment.aln: Alignment of core genes accessory_binary_genes.fa.newick: Tree based on accessory genes 3. Examine Pan-genome Statistics # View summary cat /home/ubuntu/analysis/pangenome/roary_output/summary_statistics.txt\n\n\nCore genes (present in ‚â•99% of samples)\n\n\nSoft core (95-99%)\n\n\nShell (15-95%)\n\n\nCloud (0-15%)\nInterpreting pan-genome: Large core (~2000 genes): conserved essential functions Large cloud: diverse/recently acquired genes Open pan-genome: new genes with each genome added Closed pan-genome: plateau reached 4. Explore Gene Presence/Absence # Open in spreadsheet libreoffice /home/ubuntu/analysis/pangenome/roary_output/gene_presence_absence.csv &\n\n\nOr examine with command line\n\n\nCount genes per sample\nhead -1 gene_presence_absence.csv | tr ‚Äò,‚Äô ‚Äò‚Äô | tail -n +15 | nl\n\n\nFind strain-specific genes\nawk -F‚Äô,‚Äô ‚ÄòNR&gt;1 && $4==1‚Äô gene_presence_absence.csv | cut -d‚Äô,‚Äô -f1,3\n\nVisualize with Roary Plots # Generate visualization plots python /home/ubuntu/Software/roary_plots.py\n/home/ubuntu/analysis/trees/core_genome.newick\n/home/ubuntu/analysis/pangenome/roary_output/gene_presence_absence.csv\n\n\n\nOutputs:\n\n\n- pangenome_matrix.png: presence/absence heatmap\n\n\n- pangenome_pie.png: core/accessory/unique genes\n\n\n- pangenome_frequency.png: gene frequency distribution\n\nPhandango Visualization # Open Phandango firefox https://phandango.net/\n\n\n\nDrag and drop:\n\n\n1. accessory_binary_genes.fa.newick\n\n\n2. gene_presence_absence.csv\n\nIdentify Interesting Genes # Find AMR genes in accessory genome grep -i ‚Äúresist|beta-lactam|penicillin|methicillin‚Äù\ngene_presence_absence.csv | cut -d‚Äô,‚Äô -f1,3,4\n\n\n\nFind virulence factors\ngrep -i ‚Äútoxin|hemolysin|leukocidin|protease‚Äù\ngene_presence_absence.csv | cut -d‚Äô,‚Äô -f1,3,4\n\n\nFind mobile elements\ngrep -i ‚Äútranspos|integrase|plasmid‚Äù\ngene_presence_absence.csv | cut -d‚Äô,‚Äô -f1,3,4"
  },
  {
    "objectID": "Day3/Phylogenetics.html",
    "href": "Day3/Phylogenetics.html",
    "title": "Phylogenetics",
    "section": "",
    "text": "SNP Calling with Snippy # Run Snippy on each sample against reference for sample in sample1 sample2 sample3; do snippy\n‚Äìoutdir /home/ubuntu/analysis/snippy/\\({sample} \\\n    --ref /home/ubuntu/Reference/Saureus_reference.gbk \\\n    --R1 /home/ubuntu/Data/samples/\\){sample}_1.clean.fastq.gz\n‚ÄìR2 /home/ubuntu/Data/samples/${sample}_2.clean.fastq.gz\n‚Äìcpus 4 done\n\nCore SNP Alignment # Combine all Snippy results into core SNP alignment snippy-core\n‚Äìref /home/ubuntu/Reference/Saureus_reference.gbk\n‚Äìprefix core\n/home/ubuntu/analysis/snippy/*\n\n\nOutput: core.aln (alignment of variant sites)\n\nSNP Distance Matrix # Calculate pairwise SNP distances snp-dists /home/ubuntu/analysis/snippy/core.aln &gt; snp_distances.txt\n\n\n\nView matrix\ncolumn -t snp_distances.txt | less -S\n\n\nFind closely related pairs (&lt;10 SNPs)\nawk ‚ÄòNR&gt;1{for(i=2;i&lt;=NF;i++) if($i&lt;10 && $i&gt;0) print $1, $(i), $i}‚Äô snp_distances.txt\nOutbreak threshold: ‚â§10 SNPs: Likely direct transmission or recent common ancestor 10-20 SNPs: Possibly related 20 SNPs: Likely unrelated (for S. aureus)\n\nBuild Phylogenetic Tree # Using IQ-TREE (fast maximum likelihood) iqtree\n-s /home/ubuntu/analysis/snippy/core.aln\n-m GTR+G\n-bb 1000\n-nt 4\n-pre core_tree\n\n\n\nOptions:\n\n\n-s: input alignment\n\n\n-m: substitution model\n\n\n-bb: ultrafast bootstrap (1000 replicates)\n\n\n-nt: number of threads\n\n\n-pre: output prefix\n\n\nOutput: core_tree.treefile\nAlternative: RAxML raxmlHPC-PTHREADS\n-T 4\n-m GTRGAMMA\n-p 12345\n-x 12345\n-# 100\n-s core.aln\n-n core_tree\n\nVisualize Tree with Metadata Option A: iTOL (Interactive Tree of Life) # Upload to iTOL: https://itol.embl.de/ # 1. Upload core_tree.treefile # 2. Upload metadata files (see Metadata Preparation below)\n\nOption B: Microreact # Open Microreact: https://microreact.org/ # Upload: # 1. Tree file (core_tree.treefile) # 2. Metadata CSV # 3. ARIBA summary (optional)\n\nMetadata Preparation # Create metadata file for visualization cat &gt; metadata.csv &lt;&lt; ‚ÄòEOF‚Äô sample,collection_date,ward,patient_id,amr_profile,outbreak sample1,2024-01-15,ICU,P001,MRSA,Yes sample2,2024-01-18,ICU,P002,MRSA,Yes sample3,2024-01-20,ICU,P003,MRSA,Yes sample4,2024-01-22,Surgery,P004,MSSA,No sample5,2024-02-01,ICU,P005,MRSA,Yes sample6,2024-02-03,ICU,P006,MRSA,Yes sample7,2024-02-10,Medicine,P007,MSSA,No sample8,2024-02-15,ICU,P008,MRSA,Yes sample9,2024-03-01,Outpatient,P009,MSSA,No sample10,2024-03-05,Community,P010,MSSA,No EOF\n\nOutbreak Investigation Analysis Questions to Answer: Which isolates cluster together? # Check SNP distances awk ‚Äò$3&lt;10‚Äô snp_distances.txt | column -t\nDo genomic clusters match epidemiological links? # Compare SNP distances with ward/date information paste snp_distances.txt metadata.csv | column -t | less -S\nCan you identify the index case? # Look for earliest collection date in outbreak cluster grep ‚ÄúYes‚Äù metadata.csv | sort -t‚Äô,‚Äô -k2\nAre there unexpected connections? # Find samples from different wards with low SNP distance # This might indicate unknown transmission routes"
  },
  {
    "objectID": "Day2/Annotation.html",
    "href": "Day2/Annotation.html",
    "title": "Annotation",
    "section": "",
    "text": "Genome Annotation\n\n\n1. Prokka Annotation\n\nBasic Prokka run\nprokka\n‚Äìoutdir /home/ubuntu/analysis/annotation/sample1_prokka\n‚Äìprefix sample1\n‚Äìgenus Staphylococcus\n‚Äìspecies aureus\n‚Äìstrain sample1\n‚Äìcpus 4\n/home/ubuntu/analysis/assembly/sample1_unicycler/assembly.fasta\n\n\nView summary\ncat /home/ubuntu/analysis/annotation/sample1_prokka/sample1.txt\nProkka outputs: .gff: Annotation in GFF3 format .faa: Protein sequences (amino acids) .ffn: Gene sequences (nucleotides) .gbk: GenBank format .txt: Summary statistics\n#2. Explore Annotation Files ### Count features grep -v ‚Äú#‚Äù sample1.gff | cut -f3 | sort | uniq -c\n\n\nCount CDS\ngrep -c ‚ÄúCDS‚Äù sample1.gff\n\n\nFind specific genes (e.g., beta-lactamase)\ngrep -i ‚Äúbeta-lactam‚Äù sample1.gff\n\n\nExtract all gene product descriptions\ngrep ‚Äúproduct=‚Äù sample1.gff | sed ‚Äòs/.product=([^;]).*/\\1/‚Äô | sort | uniq -c | sort -rn | head -20\n\n\n\n3. Visualize in Artemis\n\n\nLaunch Artemis from terminal\nart /home/ubuntu/analysis/annotation/sample1_prokka/sample1.gbk &\n\n\nOr double-click the sample1.gbk file in file browser\nArtemis tips: View ‚Üí Overview for whole genome view Right-click genes for details Graph ‚Üí GC Content to see compositional bias Use ‚ÄúGoto‚Äù to jump to specific positions # 4. BLAST Specific Genes # Extract a gene of interest grep ‚ÄúmecA‚Äù sample1.gff | cut -f1,4,5\n\n\nExtract sequence and BLAST\n\n\n(coordinates from above, e.g., 100000-102000)\nsamtools faidx assembly.fasta contig1:100000-102000 &gt; mecA_gene.fasta\n\n\nBLAST against nr database (if available)\nblastn -query mecA_gene.fasta -db nt -remote -outfmt 6 | head -5\n\n\nOr BLAST locally against CARD database\nblastn -query mecA_gene.fasta\n-db /home/ubuntu/Reference/CARD/nucleotide\n-outfmt ‚Äú6 qseqid sseqid pident length qcovs evalue bitscore stitle‚Äù\n-num_threads 4 | head -10"
  },
  {
    "objectID": "module3/module3_answers.html",
    "href": "module3/module3_answers.html",
    "title": "Bioinformatic methods for bacterial genomics, Cluj-Napoca, October 2025",
    "section": "",
    "text": "Run ARIBA on all samples\nExample script: create the following bash file: mkdir /home/ubuntu/Data/ARIBA_run/run_ariba.sh\nAdd the following lines of code:\nfor sample in `ls /home/ubuntu/Data/all_fastqs/*_1.fastq.gz | sed 's/\\_1.fastq.gz//'`\ndo\necho $sample\noutput=$(echo $sample | sed -E 's#.*/([^/]+)$#\\1#')\nif [ -d /home/ubuntu/Data/ARIBA_output/${output}.ariba ]; then\nrm -r /home/ubuntu/Data/ARIBA_output/${output}.ariba\nfi\nariba run /home/ubuntu/Data/ARIBA_dbs/out.resfinder.prepareref ${sample}_1.fastq.gz ${sample}_2.fastq.gz /home/ubuntu/Data/ARIBA_output/${output}.ariba\ndone"
  },
  {
    "objectID": "data-locations.html",
    "href": "data-locations.html",
    "title": "data-locations",
    "section": "",
    "text": "Data Locations /home/ubuntu/ ‚îú‚îÄ‚îÄ Data/ ‚îÇ ‚îú‚îÄ‚îÄ samples/ # Raw FASTQ files ‚îÇ ‚îî‚îÄ‚îÄ Reference/ # Reference genomes ‚îú‚îÄ‚îÄ analysis/ ‚îÇ ‚îú‚îÄ‚îÄ qc/ # FastQC outputs ‚îÇ ‚îú‚îÄ‚îÄ assembly/ # Assembly results ‚îÇ ‚îú‚îÄ‚îÄ annotation/ # Prokka outputs ‚îÇ ‚îú‚îÄ‚îÄ amr/ # ARIBA results ‚îÇ ‚îú‚îÄ‚îÄ pangenome/ # Roary outputs ‚îÇ ‚îî‚îÄ‚îÄ snippy/ # SNP analysis ‚îî‚îÄ‚îÄ Software/ # Pre-installed tools"
  },
  {
    "objectID": "module2/module2_answers.html",
    "href": "module2/module2_answers.html",
    "title": "Bioinformatic methods for bacterial genomics, Cluj-Napoca, October 2025",
    "section": "",
    "text": "How to run Prokka on all samples from one folder\nExample script: create the following bash file: mkdir /home/ubuntu/Data/PROKKA_run/run_prokka.sh\nAdd the following lines of code:\nfor sample in `ls /home/ubuntu/Data/all_fastas/*.fasta | sed 's/\\.fasta//'`\ndo\necho $sample\noutput=$(echo $sample | sed -E 's#.*/([^/]+)$#\\1#')\necho $output\ndocker run -v $(pwd):$(pwd) -w $(pwd) staphb/prokka:latest prokka --outdir /home/ubuntu/Data/annotations/${output} --prefix ${output} /home/ubuntu/Data/all_fastas/${output}.fasta\ndone"
  },
  {
    "objectID": "module5/module5.html",
    "href": "module5/module5.html",
    "title": "Module 5",
    "section": "",
    "text": "How to setup your Amazon EC machine\nIn your browser navigate to: https://aws.amazon.com/\nCreate an AWS account\nOnce your virtual machine is up and running, login via ssh, using your key.pem file.\nchmod 400 key.pem\nssh -i key.pem ubuntu@xxx.xxx.xxx.xxx\nWhere xxx.xxx.xxx.xxx is the IP of the virtual machine.\nOnce logged in, change the password for the username ubuntu:\nsudo passwd ubuntu\nThen, try to log in via the visual interface, by navigating to the IP address (xxx.xxx.xxx.xxx) in your browser."
  },
  {
    "objectID": "module5/module5.html#create-an-image",
    "href": "module5/module5.html#create-an-image",
    "title": "Module 5",
    "section": "Create an image",
    "text": "Create an image"
  },
  {
    "objectID": "module5/module5.html#launch-instances-from-an-image",
    "href": "module5/module5.html#launch-instances-from-an-image",
    "title": "Module 5",
    "section": "Launch instances from an image",
    "text": "Launch instances from an image"
  },
  {
    "objectID": "module5/module5.html#delete-the-aws-account",
    "href": "module5/module5.html#delete-the-aws-account",
    "title": "Module 5",
    "section": "Delete the AWS account",
    "text": "Delete the AWS account\nHow to close your AWS account\nhttps://docs.aws.amazon.com/accounts/latest/reference/manage-acct-closing.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "This page is a repository for training resources designed and developed for the short course on Bacterial Genomics Bioinformatics, Babes-Bolyai University, Cluj-Napoca, Romania, November 2025, by Monica Abrudan.\nThe materials provided in this repository are FREE to use. This work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement.\nFind out more about the author here"
  },
  {
    "objectID": "microreact/microreact_doc.html",
    "href": "microreact/microreact_doc.html",
    "title": "Microreact documentation",
    "section": "",
    "text": "https://docs.microreact.org/"
  },
  {
    "objectID": "data-flo/data-flo_doc.html",
    "href": "data-flo/data-flo_doc.html",
    "title": "Data-flo documentation",
    "section": "",
    "text": "https://docs.data-flo.io/introduction/readme"
  },
  {
    "objectID": "Syllabus.html",
    "href": "Syllabus.html",
    "title": "Training Syllabus",
    "section": "",
    "text": "Topics to be addressed during the workshop\n\n\n\nTraining Objectives\nLearning objectives. By the end of the workshop, the participants will be able to\n\n\nTargeted competencies and Knowledge, Skills and Attitudes\n\n\nTraining Methods and Instructional Strategies\n\n\nDuration of training\n\n\nSchedule of sessions\n\n\nAssessment and Evaluation\n\n\nResources\n\n\nRecommended background knowledge"
  },
  {
    "objectID": "Day1/QC.html",
    "href": "Day1/QC.html",
    "title": "QC",
    "section": "",
    "text": "pwd\n\n\n\ncd /home/ubuntu/Data\n\n\n\nls -lh\n\n\n\nmkdir -p analysis/qc analysis/assembly\n\n\n\ndf -h"
  },
  {
    "objectID": "Day1/QC.html#reads-qc",
    "href": "Day1/QC.html#reads-qc",
    "title": "QC",
    "section": "Reads QC",
    "text": "Reads QC\nIn this part of the exercise, we will use a programme called FastQC.\nFastQC aims to provide a simple way to do some quality control checks on raw sequence Data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your Data has any problems of which you should be aware before doing any further analysis.\nThe main functions of FastQC are\n\nImport of Data from BAM, SAM or FastQ files (any variant)\nProviding a quick overview to tell you in which areas there may be problems\nSummary graphs and tables to quickly assess your Data\nExport of results to an HTML based permanent report\n\n\nRun FastQC programatically\nTo run non-interactively you simply have to specify a list of files to process on the command line.¬†\nEg:\nfastqc somefile.txt someotherfile.txt\nfastqc /home/ubuntu/Data/*/*/*\n\n\nQC your fastq reads through the visual interface\nNavigate to /home/ubuntu/Software/FastQC and double click on the FastQC icon.\nIn the visual interface, open all files produced by FastQC and assess the results, eg ERR4635696_1_fastqc.html\nVisualize and interpret the FastQC results. Compare the QC results for Nanopore and Illumina sequence reads\nOptional: Unzip the files /home/ubuntu/Data/*/*/*_*_fastqc.zip and produce a script that extracts the information on reads lengths, GC contents etc; plot the results from all samples.\n\n\n\nFastQC Illumina reads\n\n\n\n\n\nFastQC Nanopore reads\n\n\n\n\nGenerate MultiQC report (aggregates all FastQC results)\nMultiQC looks for the .zip files produced by FastQC (not the .html files)\ncd /home/ubuntu/Data/QC\nmultiqc ERR4784794\nOpen in browser:\nNavigate to /home/ubuntu/Data/QC and double-click multiqc_report.html\nKey metrics to check:\nPer base sequence quality (should be &gt;28 for most bases)\nPer sequence quality scores (peak should be &gt;30)\nAdapter content (should be minimal)\nGC content (should match expected ~33% for S. aureus)\n\n\nRead Trimming/Filtering\n\n\nUsing fastp (fast and comprehensive)\nEg:\nfastp -i ERR4635696_1.fastq.gz -I ERR4635696_2.fastq.gz -o ERR4635696_1.clean.fastq.gz -O ERR4635696_2.clean.fastq.gz --qualified_quality_phred 20 --length_required 50 --detect_adapter_for_pe --html sample1_fastp.html --json sample1_fastp.json --thread 4\n\n\nRe-run MultiQC to confirm improvement\ncd /home/ubuntu/Data/QC\nmultiqc ERR4635696\nmultiqc ERR4635696_clean\nDecision point: If &gt;80% reads pass filtering and mean quality &gt;Q30, proceed to assembly."
  },
  {
    "objectID": "module8/module8.html",
    "href": "module8/module8.html",
    "title": "Module 8",
    "section": "",
    "text": "This exercise used tools developed by The Centre for Genomic Pathogen Surveillance (CGPS) at University of Oxford\nSimilar exercises have been developed as part of T3Connect https://wcscourses.github.io/T3connectResources/\n\nFor Microreact documentation, go to https://docs.microreact.org/\n\n\nIn this series of exercises, you will use Microreact to gain knowledge regarding a large collection of Klebsiella pneumoniae isolates, sampled from Colombia, between 2013 and 2019. At the end of this series of exercises, you will be able to identify a high risk clone circulating in the country.\n\n\n\nTask 1: Create an editable project.\nTask 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.¬†\nTask 3: What are the dominating sequence types (STs) in Colombia?\nTask 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\nTask 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nThis is a screenshot of a Microreact project called ‚ÄúKlebsiella pneumoniae Colombia‚Äù. Notice the main panels in this project: the map, the tree, the timeline and the saved views."
  },
  {
    "objectID": "module8/module8.html#data-interpretation",
    "href": "module8/module8.html#data-interpretation",
    "title": "Module 8",
    "section": "",
    "text": "This exercise used tools developed by The Centre for Genomic Pathogen Surveillance (CGPS) at University of Oxford\nSimilar exercises have been developed as part of T3Connect https://wcscourses.github.io/T3connectResources/\n\nFor Microreact documentation, go to https://docs.microreact.org/\n\n\nIn this series of exercises, you will use Microreact to gain knowledge regarding a large collection of Klebsiella pneumoniae isolates, sampled from Colombia, between 2013 and 2019. At the end of this series of exercises, you will be able to identify a high risk clone circulating in the country.\n\n\n\nTask 1: Create an editable project.\nTask 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.¬†\nTask 3: What are the dominating sequence types (STs) in Colombia?\nTask 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\nTask 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nThis is a screenshot of a Microreact project called ‚ÄúKlebsiella pneumoniae Colombia‚Äù. Notice the main panels in this project: the map, the tree, the timeline and the saved views."
  },
  {
    "objectID": "module8/module8.html#task-1-create-an-editable-project.",
    "href": "module8/module8.html#task-1-create-an-editable-project.",
    "title": "Module 8",
    "section": "Task 1: Create an editable project.",
    "text": "Task 1: Create an editable project.\n\n\n\nTask 1: Create an editable project. Step 1: Notice the crossed out pen on right top corner of your screen. This indicates that you cannot edit the current Microreact project.\n\n\n\n\n\nTask 1: Create an editable project. Step 2: Click the crossed out ‚ÄúPen‚Äù symbol in the top right of the screen. A window appears asking you to ‚ÄúSIGN IN TO EDIT‚Äù.\n\n\n\n\n\nTask 1: Create an editable project. Step 3: Once you sign in, the message changes, and you are invited to ‚ÄúMAKE A COPY‚Äù of the current project. Make a copy, to proceed.\n\n\n\n\n\nTask 1: Create an editable project. Step 4: Notice that, once you made a copy of the project, the crossed out ‚Äúpen‚Äù symbol will change to a ‚Äúnormal pen‚Äù, and you will be able to edit and save the project."
  },
  {
    "objectID": "module8/module8.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "href": "module8/module8.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "title": "Module 8",
    "section": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.",
    "text": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 1: Select the ‚ÄúKpn Colombia‚Äù view. Click on the ‚ÄúPen‚Äù symbol on the top right menu. Click on the ‚ÄúCreate New Chart‚Äù\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 2: Drag the new chart to overlap with the tree.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 3: An empty panel for the new chart will be shown on top of the tree panel.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 4: In the Chart Type dropdown list select ‚ÄúBar Chart‚Äù.\n\n\n\n\n\nTask2: Present WGS QC statistics in a chart. Step 5: A new window appears. In the X Axis Column, select ‚ÄúWGS_QC_no_contigs‚Äù and for ‚ÄúMaximum number of bins‚Äù select 10.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 6: The bar chart will look like above. Observe that most genomes have less the 100 contigs.\n\n\n\nTask 3: What are the dominating sequence types (STs) in Colombia?\n\n\n\nTask 3: What are the dominating STs in Colombia? Now that you‚Äôve created one chart, you can create another one! Step 1: Go to the ‚ÄúPen: symbol on the right hand side and click on the‚ÄùCreate New Chart‚Äù.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 2: The new chart can stay right on top of the previously created one.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 3: Notice a white canvas on top of the previously generated chart.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 4: Once again, from the Chart Type dropdown menu, select ‚ÄúBar Chart‚Äù, and when the new view shows up on the ‚ÄúX Axis Column‚Äù, select ST.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 5: A new chart will appear. The labels on the x-axis appear squished and they are hard to read. Drag the panel divider on the left hand side of the chart, to increase the width of the panel.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 6: The information on the x-axis should be readable now. The 3 most abundant STs are ST11, ST258 and ST512.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 7: Click the ‚ÄúViews‚Äù panel on the left hand side, hover over ‚ÄúKpn Colombia‚Äù, click on the three dots on the corner of the view and hit ‚ÄúUpdate View‚Äù\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 8: Go to the Save icon on the right corner, press the icon and choose ‚ÄúUpdate This Project‚Äù\n\n\n\n\nTask 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 1: Press the icon on the top right of the tree panel.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 2: In the Metadata blocks dropdown list, tick all boxes containing KPC, NDM, VIM, OXA and CTX-M-15 genes.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 3: The tree panel will show metadata columns. Yellow indicates presence of a certain gene and green indicates absence. In the panel shown here, it appears that the most common genes present are KPC-2, KPC-3, NDM-1 and CTX-M-15.\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 1: Go to the ‚ÄúMetadata blocks‚Äù and check the ST box.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 2: Observe the new metadata column next to the tree, with the header ‚ÄúST‚Äù.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? In the tree with added metadata blocks, we can observe a large brown block of isolates belonging to ST258. You will need to check the Legend on the very right of Microreact.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? On a close look, we can observe that ST258 is associated with the presence of the carbapenamase gene KPC-3.\n\n\nMicroreact demo link\nData taken from¬†a series of articles published in 2021 in the journal Clinical Infectious Diseases:\n\nGlobal collection article\n\nPhilippines article\nIndia article\nNigeria article\nColombia article"
  },
  {
    "objectID": "module1/module1.html",
    "href": "module1/module1.html",
    "title": "Module 1",
    "section": "",
    "text": "Module 1: Set up your bioinformatics working environment\n\n\nPart 1: Connect to your Amazon EC2 instance via ssh\nBefore you start, make sure you received the IPv4 address of the virtual machine and your private key to connect to the machine from your course coordinator.\nThe virtual machines provided during this course are intended exclusively for use with the course material.\n\nUsing a Mac\nOpen your Terminal application on your local machine.\nThen type\nchmod 400 /local/path/to/student.pem\nssh -i /local/path/to/key.pem¬† ubuntu@xxx.xxx.xxx.xxx\nReplace the ‚Äúxxx.xxx.xxx.xxx‚Äù with the Public IPv4 of your Amazon instance and the /local/path/to/student.pem with the local path to your student.pem file that was communicated to you previously.\nFor more information on how to connect to your virtual machine, access https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-linux-inst-ssh.html .\n\n\nUsing a Windows machine\nSee https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-linux-inst-from-windows.html\n\n\nHow to connect to your Amazon EC2 instance via your browser (during the course)\nType the Public IPv4 in your browser address bar, eg: https://xxx.xxx.xxx.xxx/\nType in the username ubuntu, and the password provided to you by the course instructor.\nOnce you log in, you should see the welcome screen.\nPlease reject any invitation to update the system.\n\n\nHow to modify your $PATH variable\nOnce you are connected to your Amazon EC2 Ubuntu instance, try to view and edit your configuration files.¬†\nCheck the configuration files:\nls -a ~/.\nEdit the .profile file and add the paths in the PATH variable\nEg:\nvi ~/.profile\nPATH=\"/home/ubuntu/Software/mash-Linux64-v2.3:$PATH\"\n\n\n\nPart 2: Configure your Amazon EC2 Ubuntu instance\nYour Amazon EC2 Ubuntu instance should be ready to go right away. However, in order to go through the next modules of this course, you will need to install the list of tools below.\nFirst, familiarise yourselves with the tools. Understand what is their role in an NGS pipeline, what is the required input and the expected output. Who developed them and when? Do they have dependencies?\nFor each tool that you attempt to install, write down the steps you took to achieve that in the Shared student observations file, which you have received from your course coordinator.\n\nFastQC\nhttps://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n\n\nBactinspector\nhttps://gitlab.com/antunderwood/bactinspector\n\n\nVelvet assembler\nhttps://github.com/dzerbino/velvet\n\n\n\nACT and Artemis\nhttps://www.sanger.ac.uk/tool/artemis-comparison-tool-act/\n\n\n\nUnicycler¬†\nhttps://github.com/rrwick/Unicycler?tab=readme-ov-file#build-and-run-without-installation\n\n\n\nSPADES\nhttps://github.com/ablab/spades\n\n\n\nQuast\nhttps://github.com/ablab/quast\n\n\n\nbrew\nhttps://docs.brew.sh/Homebrew-on-Linux\n\n\n\nmakeblastdb and tblastn\nhttps://www.ncbi.nlm.nih.gov/books/NBK569861/\nprokka\nhttps://github.com/tseemann/prokka\n\n\n\nresfinder\nhttps://github.com/cadms/resfinder\n\nARIBA\nhttps://sanger-pathogens.github.io/ariba/ \nProkka\nhttps://github.com/tseemann/prokka\ndocker pull staphb/prokka:latest\ndocker run staphb/prokka:latest prokka -h"
  }
]